{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ENR6EQ2b8R8",
        "outputId": "bbeb46e6-5a25-4595-fe6a-0799e0d6def9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: datasets 4.0.0\n",
            "Uninstalling datasets-4.0.0:\n",
            "  Successfully uninstalled datasets-4.0.0\n",
            "Collecting datasets==2.18.0\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (18.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets==2.18.0)\n",
            "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (0.70.16)\n",
            "Collecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0)\n",
            "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (3.13.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (6.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0) (1.22.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.4->datasets==2.18.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.4->datasets==2.18.0) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.18.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.18.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.18.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.18.0) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.18.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.18.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.18.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0) (1.17.0)\n",
            "Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: pyarrow-hotfix, fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.18.0 fsspec-2024.2.0 pyarrow-hotfix-0.7\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (0.7)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.13.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y datasets\n",
        "!pip install datasets==2.18.0\n",
        "!pip install evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0_eA7mMhnEH"
      },
      "outputs": [],
      "source": [
        "num = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7JFdShlZwE4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3TR9l4sFBD3"
      },
      "outputs": [],
      "source": [
        "# Finite-Difference JVP — Prune ONLY decoder attention layers (self & cross)\n",
        "# NaN-hardened version\n",
        "\n",
        "import inspect\n",
        "import warnings\n",
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration, T5TokenizerFast,\n",
        "    DataCollatorForSeq2Seq, get_linear_schedule_with_warmup\n",
        ")\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# =========================\n",
        "# 0) Helpers for FD-JVP and kwargs/mask handling\n",
        "# =========================\n",
        "\n",
        "def _filter_kwargs_for_module(module, kwargs):\n",
        "    sig = inspect.signature(module.forward)\n",
        "    allowed = set(sig.parameters.keys()) - {\"self\"}\n",
        "    return {k: v for k, v in kwargs.items() if k in allowed}\n",
        "\n",
        "def _map_args_from_hook(module, args, kwargs):\n",
        "    if kwargs and len(kwargs) > 0:\n",
        "        d = dict(kwargs)\n",
        "    else:\n",
        "        sig = inspect.signature(module.forward)\n",
        "        names = [n for n in sig.parameters.keys() if n != \"self\"]\n",
        "        d = {}\n",
        "        for i, name in enumerate(names):\n",
        "            if i < len(args):\n",
        "                d[name] = args[i]\n",
        "            else:\n",
        "                d[name] = sig.parameters[name].default\n",
        "    d.pop(\"hidden_states\", None)\n",
        "    return d  # filtering happens at call-time\n",
        "\n",
        "def _build_causal_mask(B, q_len, k_len, device, dtype):\n",
        "    # stable large negative instead of -inf to avoid NaNs in softmax\n",
        "    mask = torch.zeros((B, 1, q_len, k_len), dtype=dtype, device=device)\n",
        "    tri = torch.triu(torch.ones((q_len, k_len), dtype=torch.bool, device=device), diagonal=1)\n",
        "    mask[:, :, tri] = -1e9\n",
        "    return mask\n",
        "\n",
        "def _build_zero_mask(B, q_len, k_len, device, dtype):\n",
        "    return torch.zeros((B, 1, q_len, k_len), dtype=dtype, device=device)\n",
        "\n",
        "def _ensure_mask_shape(mask, q_len, k_len, device, dtype, is_self, B_fallback=1):\n",
        "    \"\"\"\n",
        "    Return a (B,1,q_len,k_len) mask; if input mask is missing or wrong-sized,\n",
        "    rebuild a safe default (causal for self, zeros for cross).\n",
        "    \"\"\"\n",
        "    default_fn = _build_causal_mask if is_self else _build_zero_mask\n",
        "\n",
        "    if mask is None:\n",
        "        return default_fn(B_fallback, q_len, k_len, device, dtype)\n",
        "\n",
        "    if mask.dim() == 2:\n",
        "        B = mask.size(0)\n",
        "        return mask[:, None, None, :k_len].expand(B, 1, q_len, k_len).contiguous().to(device=device, dtype=dtype)\n",
        "\n",
        "    if mask.dim() == 4:\n",
        "        B, _, Q, K = mask.shape\n",
        "        if Q >= q_len and K >= k_len:\n",
        "            return mask[:, :, :q_len, :k_len].to(device=device, dtype=dtype)\n",
        "        # cannot slice up to desired size → rebuild\n",
        "        return default_fn(B, q_len, k_len, device, dtype)\n",
        "\n",
        "    # unknown shape → rebuild\n",
        "    return default_fn(B_fallback, q_len, k_len, device, dtype)\n",
        "\n",
        "@torch.no_grad()\n",
        "def _t5sublayer_forward_only(module, X, argdict):\n",
        "    \"\"\"\n",
        "    Deterministic forward for a single T5 sublayer (Self/Cross Attention).\n",
        "    Returns only hidden_states (tensor). fp32, autocast disabled.\n",
        "    Provides safe masks and cache_position/query_length when missing.\n",
        "    \"\"\"\n",
        "    if X is None:\n",
        "        return None\n",
        "    was_training = module.training\n",
        "    module.eval()\n",
        "    Xf = X.float()\n",
        "    try:\n",
        "        raw_kwargs = dict(argdict) if argdict is not None else {}\n",
        "        raw_kwargs.pop(\"position_bias\", None)  # force recompute inside\n",
        "\n",
        "        sig = inspect.signature(module.forward)\n",
        "        params = set(sig.parameters.keys())\n",
        "\n",
        "        # Build kwargs to re-call the sublayer\n",
        "        kwargs = {\"hidden_states\": Xf}\n",
        "\n",
        "        # Cross-attention?\n",
        "        is_cross = \"key_value_states\" in params\n",
        "        kv = None\n",
        "        if is_cross:\n",
        "            kv = raw_kwargs.get(\"key_value_states\", raw_kwargs.get(\"encoder_hidden_states\", None))\n",
        "            if kv is None:\n",
        "                # no encoder states captured → skip this probe (avoid wrong shapes)\n",
        "                return None\n",
        "            kwargs[\"key_value_states\"] = kv.float()\n",
        "\n",
        "        # Determine lengths\n",
        "        q_len = Xf.size(1)\n",
        "        k_len = kv.size(1) if (is_cross and kv is not None) else q_len\n",
        "        B = Xf.size(0)\n",
        "\n",
        "        # Attention mask (name differs across wrappers)\n",
        "        mask_key = \"attention_mask\" if \"attention_mask\" in params else (\"mask\" if \"mask\" in params else None)\n",
        "        if mask_key is not None:\n",
        "            src = raw_kwargs.get(mask_key, raw_kwargs.get(\"mask\", None))\n",
        "            mask = _ensure_mask_shape(src, q_len, k_len, device=Xf.device, dtype=Xf.dtype, is_self=not is_cross, B_fallback=B)\n",
        "            kwargs[mask_key] = mask\n",
        "\n",
        "        # cache_position & query_length fixes\n",
        "        if \"cache_position\" in params:\n",
        "            cp = raw_kwargs.get(\"cache_position\", None)\n",
        "            if cp is None:\n",
        "                cp = torch.arange(q_len, dtype=torch.long, device=Xf.device)\n",
        "            kwargs[\"cache_position\"] = cp\n",
        "        if \"query_length\" in params and \"cache_position\" not in params:\n",
        "            kwargs[\"query_length\"] = q_len\n",
        "\n",
        "        # Deterministic flags if present\n",
        "        if \"use_cache\" in params:\n",
        "            kwargs[\"use_cache\"] = False\n",
        "        if \"output_attentions\" in params:\n",
        "            kwargs[\"output_attentions\"] = False\n",
        "\n",
        "        # Filter to exactly what this module accepts\n",
        "        kwargs = _filter_kwargs_for_module(module, kwargs)\n",
        "\n",
        "        # Disable autocast for stable finite differences\n",
        "        with torch.cuda.amp.autocast(enabled=False):\n",
        "            out = module(**kwargs)\n",
        "\n",
        "        # T5Layer{Self|Cross}Attention returns (hs, present_kv, position_bias)\n",
        "        hs = out[0] if isinstance(out, (tuple, list)) else out\n",
        "        return hs if hs is None or torch.isfinite(hs).all() else None\n",
        "    finally:\n",
        "        module.train(was_training)\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_decoder_attention_jdev(model, self_bufs, cross_bufs, eps=1e-3, k_probes=1):\n",
        "    \"\"\"\n",
        "    For each decoder block sublayer ℓ (self-attn, cross-attn), estimate:\n",
        "      E_v ||(Jℓ - I)v||^2 ≈ E_v || (f(x + eps v) - f(x))/eps - v ||^2\n",
        "    Returns two dicts: self_scores{idx:score}, cross_scores{idx:score}.\n",
        "    NaN-safe: skips any non-finite forward/probe results. Buffers cleared.\n",
        "    \"\"\"\n",
        "    self_scores, cross_scores = {}, {}\n",
        "\n",
        "    # Self-attention J-dev\n",
        "    for idx, buf in self_bufs.items():\n",
        "        X, args = buf[\"X\"], buf[\"args\"]\n",
        "        if X is None:\n",
        "            continue\n",
        "        mod = model.decoder.block[idx].layer[0]  # T5LayerSelfAttention\n",
        "        X = X.float()\n",
        "        y0 = _t5sublayer_forward_only(mod, X, args)\n",
        "        if y0 is None:\n",
        "            buf[\"X\"], buf[\"args\"] = None, None\n",
        "            continue\n",
        "        acc, used = 0.0, 0\n",
        "        for _ in range(max(1, k_probes)):\n",
        "            v = torch.randn_like(X)\n",
        "            y_eps = _t5sublayer_forward_only(mod, X + eps * v, args)\n",
        "            if y_eps is None:\n",
        "                continue\n",
        "            jd_vec = (y_eps - y0) / eps - v\n",
        "            if not torch.isfinite(jd_vec).all():\n",
        "                continue\n",
        "            acc += float(jd_vec.pow(2).mean().item())\n",
        "            used += 1\n",
        "        if used > 0:\n",
        "            self_scores[idx] = acc / used\n",
        "        buf[\"X\"], buf[\"args\"] = None, None\n",
        "\n",
        "    # Cross-attention J-dev\n",
        "    for idx, buf in cross_bufs.items():\n",
        "        X, args = buf[\"X\"], buf[\"args\"]\n",
        "        if X is None:\n",
        "            continue\n",
        "        if len(model.decoder.block[idx].layer) <= 1 or model.decoder.block[idx].layer[1] is None:\n",
        "            continue\n",
        "        mod = model.decoder.block[idx].layer[1]  # T5LayerCrossAttention\n",
        "        X = X.float()\n",
        "        y0 = _t5sublayer_forward_only(mod, X, args)\n",
        "        if y0 is None:\n",
        "            buf[\"X\"], buf[\"args\"] = None, None\n",
        "            continue\n",
        "        acc, used = 0.0, 0\n",
        "        for _ in range(max(1, k_probes)):\n",
        "            v = torch.randn_like(X)\n",
        "            y_eps = _t5sublayer_forward_only(mod, X + eps * v, args)\n",
        "            if y_eps is None:\n",
        "                continue\n",
        "            jd_vec = (y_eps - y0) / eps - v\n",
        "            if not torch.isfinite(jd_vec).all():\n",
        "                continue\n",
        "            acc += float(jd_vec.pow(2).mean().item())\n",
        "            used += 1\n",
        "        if used > 0:\n",
        "            cross_scores[idx] = acc / used\n",
        "        buf[\"X\"], buf[\"args\"] = None, None\n",
        "\n",
        "    return self_scores, cross_scores\n",
        "\n",
        "# =========================\n",
        "# 1) Hook Utilities — capture ONLY decoder attention sublayer inputs/args\n",
        "# =========================\n",
        "\n",
        "def register_decoder_attention_hooks(model):\n",
        "    \"\"\"\n",
        "    Capture inputs/kwargs for decoder self-attention (layer[0]) and cross-attention (layer[1]).\n",
        "    \"\"\"\n",
        "    dec_blocks = model.decoder.block\n",
        "    self_bufs  = {i: {\"X\": None, \"args\": None} for i in range(len(dec_blocks))}\n",
        "    cross_bufs = {i: {\"X\": None, \"args\": None} for i in range(len(dec_blocks))}\n",
        "    hooks = []\n",
        "\n",
        "    # Self-attention pre-hooks\n",
        "    for i, block in enumerate(dec_blocks):\n",
        "        sa = block.layer[0]\n",
        "        def pre_hook_sa(module, args, kwargs, idx=i):\n",
        "            X = kwargs.get(\"hidden_states\", args[0] if len(args) > 0 else None)\n",
        "            self_bufs[idx][\"X\"]    = None if X is None else X.detach()\n",
        "            self_bufs[idx][\"args\"] = _map_args_from_hook(module, args, kwargs)\n",
        "        hooks.append(sa.register_forward_pre_hook(pre_hook_sa, with_kwargs=True))\n",
        "\n",
        "    # Cross-attention pre-hooks\n",
        "    for i, block in enumerate(dec_blocks):\n",
        "        if len(block.layer) > 1 and block.layer[1] is not None:\n",
        "            ca = block.layer[1]\n",
        "            def pre_hook_ca(module, args, kwargs, idx=i):\n",
        "                X = kwargs.get(\"hidden_states\", args[0] if len(args) > 0 else None)\n",
        "                cross_bufs[idx][\"X\"]    = None if X is None else X.detach()\n",
        "                cross_bufs[idx][\"args\"] = _map_args_from_hook(module, args, kwargs)\n",
        "            hooks.append(ca.register_forward_pre_hook(pre_hook_ca, with_kwargs=True))\n",
        "\n",
        "    return hooks, self_bufs, cross_bufs\n",
        "\n",
        "def remove_hooks(hooks):\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "# =========================\n",
        "# 2) Attention-only pruning utilities\n",
        "# =========================\n",
        "\n",
        "class SkipSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Identity replacement for T5LayerSelfAttention.\n",
        "    IMPORTANT: return (hidden_states, None, None) to avoid propagating stale position bias.\n",
        "    \"\"\"\n",
        "    def forward(self, hidden_states, *args, **kwargs):\n",
        "        return (hidden_states, None, None)\n",
        "\n",
        "class SkipCrossAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Identity replacement for T5LayerCrossAttention.\n",
        "    Also return None for position_bias to force recomputation downstream.\n",
        "    \"\"\"\n",
        "    def forward(self, hidden_states, *args, **kwargs):\n",
        "        return (hidden_states, None, None)\n",
        "\n",
        "def prune_decoder_attention(model, self_scores, cross_scores,\n",
        "                            k_self=2, k_cross=2,\n",
        "                            protect_first=True, protect_last=False, verbose=True):\n",
        "    \"\"\"\n",
        "    Prune LOW J-dev attention sublayers (near-identity). FFNs untouched.\n",
        "    \"\"\"\n",
        "    num_layers = len(model.decoder.block)\n",
        "    self_items  = list(self_scores.items())\n",
        "    cross_items = list(cross_scores.items())\n",
        "\n",
        "    if protect_first:\n",
        "        self_items  = [(i,s) for i,s in self_items  if i != 0]\n",
        "        cross_items = [(i,s) for i,s in cross_items if i != 0]\n",
        "    if protect_last:\n",
        "        self_items  = [(i,s) for i,s in self_items  if i != num_layers-1]\n",
        "        cross_items = [(i,s) for i,s in cross_items if i != num_layers-1]\n",
        "\n",
        "    self_items.sort(key=lambda x: x[1])   # lowest first\n",
        "    cross_items.sort(key=lambda x: x[1])  # lowest first\n",
        "\n",
        "    pruned_self, pruned_cross = [], []\n",
        "\n",
        "    for i, _ in self_items[:max(0, k_self)]:\n",
        "        model.decoder.block[i].layer[0] = SkipSelfAttention()\n",
        "        pruned_self.append(i)\n",
        "\n",
        "    for i, _ in cross_items[:max(0, k_cross)]:\n",
        "        if len(model.decoder.block[i].layer) > 1 and model.decoder.block[i].layer[1] is not None:\n",
        "            model.decoder.block[i].layer[1] = SkipCrossAttention()\n",
        "            pruned_cross.append(i)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Pruned decoder SELF-attention at layers:  {pruned_self}\")\n",
        "        print(f\"Pruned decoder CROSS-attention at layers: {pruned_cross}\")\n",
        "\n",
        "    return pruned_self, pruned_cross\n",
        "\n",
        "# =========================\n",
        "# 3) Data & eval helpers (e-SNLI)\n",
        "# =========================\n",
        "\n",
        "def make_t5_nli_prompt(premise, hypothesis):\n",
        "    return f\"nli premise: {premise} hypothesis: {hypothesis}\"\n",
        "\n",
        "def preprocess_function(batch, tokenizer, max_input_length=128, max_target_length=8):\n",
        "    inputs = [make_t5_nli_prompt(p, h) for p, h in zip(batch['premise'], batch['hypothesis'])]\n",
        "    model_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=max_input_length)\n",
        "    label_list = [\"entailment\", \"neutral\", \"contradiction\"]\n",
        "    labels = [label_list[x] if isinstance(x, int) and x < len(label_list) else x for x in batch['label']]\n",
        "    target = tokenizer(labels, padding=\"max_length\", truncation=True, max_length=max_target_length)\n",
        "    model_inputs[\"labels\"] = target[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "def compute_accuracy(preds, refs):\n",
        "    correct = 0\n",
        "    for p, l in zip(preds, refs):\n",
        "        if p == l:\n",
        "            correct += 1\n",
        "    return correct / len(preds) if preds else 0.0\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, dl, tokenizer, device, label_texts):\n",
        "    # Disable cache to avoid KV/pos-bias drift after pruning\n",
        "    model.config.use_cache = False\n",
        "    model.eval()\n",
        "    preds, refs = [], []\n",
        "    for batch in dl:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=2,\n",
        "            use_cache=False   # <— critical for stability after pruning\n",
        "        )\n",
        "        pred_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        label_ids = batch[\"labels\"].clone()\n",
        "        label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "        ref_texts = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "        preds.extend([p.strip().lower() for p in pred_texts])\n",
        "        refs.extend([l.strip().lower() for l in ref_texts])\n",
        "    return compute_accuracy(preds, refs)\n",
        "\n",
        "# =========================\n",
        "# 4) Train + collect attention J-dev, then prune attention-only\n",
        "# =========================\n",
        "\n",
        "def full_finetuning_collect_attn_jdev(train_loader, dev_loader, device, tokenizer, label_texts,\n",
        "                                      jvp_eps=1e-3, jvp_k=1, jvp_every=1, epochs=6):\n",
        "    \"\"\"\n",
        "    Stage 1: Full fine-tuning while periodically collecting FD-JVP J-dev\n",
        "    for decoder self-attention and cross-attention sublayers.\n",
        "    \"\"\"\n",
        "    print(\"=== Stage 1: Full FT & FD-JVP (Decoder Attention only) ===\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n",
        "    model.config.use_cache = False  # <— avoid cache while collecting FD-JVP\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "    scaler = GradScaler()\n",
        "    sched = get_linear_schedule_with_warmup(opt, 0, len(train_loader)*3)\n",
        "\n",
        "    hooks, self_bufs, cross_bufs = register_decoder_attention_hooks(model)\n",
        "    self_sum, self_cnt   = defaultdict(float), defaultdict(int)\n",
        "    cross_sum, cross_cnt = defaultdict(float), defaultdict(int)\n",
        "\n",
        "    step = 0\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            step += 1\n",
        "            opt.zero_grad()\n",
        "            with autocast():\n",
        "                out = model(input_ids=batch[\"input_ids\"].to(device),\n",
        "                            attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                            labels=batch[\"labels\"].to(device))\n",
        "                loss = out.loss\n",
        "                scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "\n",
        "            if step % max(1, jvp_every) == 0:\n",
        "                self_scores, cross_scores = compute_decoder_attention_jdev(\n",
        "                    model, self_bufs, cross_bufs, eps=jvp_eps, k_probes=jvp_k\n",
        "                )\n",
        "                for i, v in self_scores.items():\n",
        "                    if math.isfinite(v):\n",
        "                        self_sum[i]  += v; self_cnt[i]  += 1\n",
        "                for i, v in cross_scores.items():\n",
        "                    if math.isfinite(v):\n",
        "                        cross_sum[i] += v; cross_cnt[i] += 1\n",
        "\n",
        "        epoch_self  = {i: self_sum[i]/self_cnt[i]   for i in self_sum   if self_cnt[i]   > 0}\n",
        "        epoch_cross = {i: cross_sum[i]/cross_cnt[i] for i in cross_sum  if cross_cnt[i]  > 0}\n",
        "        print(f\"[Epoch {epoch+1}] Decoder SELF-attn J-dev:  {epoch_self}\")\n",
        "        print(f\"[Epoch {epoch+1}] Decoder CROSS-attn J-dev: {epoch_cross}\")\n",
        "        acc = evaluate_model(model, dev_loader, tokenizer, device, label_texts)\n",
        "        print(f\"[Epoch {epoch+1}] Dev Acc: {acc:.4f}\")\n",
        "\n",
        "    remove_hooks(hooks)\n",
        "    final_self  = {i: self_sum[i]/self_cnt[i]   for i in self_sum   if self_cnt[i]   > 0}\n",
        "    final_cross = {i: cross_sum[i]/cross_cnt[i] for i in cross_sum  if cross_cnt[i]  > 0}\n",
        "    return model, final_self, final_cross\n",
        "\n",
        "def prune_attention_and_finetune(model, train_loader, dev_loader, device,\n",
        "                                 self_scores, cross_scores, tokenizer, label_texts,\n",
        "                                 k_self=2, k_cross=2, epochs=5):\n",
        "    print(\"=== Stage 2: Prune LOW-Jdev Attention (self/cross) & FT ===\")\n",
        "    pruned_self, pruned_cross = prune_decoder_attention(\n",
        "        model, self_scores, cross_scores, k_self=k_self, k_cross=k_cross,\n",
        "        protect_first=True, protect_last=False, verbose=True\n",
        "    )\n",
        "\n",
        "    model.config.use_cache = False  # <— keep disabled after pruning\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "    sched = get_linear_schedule_with_warmup(opt, 0, len(train_loader)*2)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            opt.zero_grad()\n",
        "            out = model(input_ids=batch[\"input_ids\"].to(device),\n",
        "                        attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                        labels=batch[\"labels\"].to(device))\n",
        "            loss = out.loss\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, tokenizer, device, label_texts)\n",
        "        print(f\"[Prune FT Epoch {epoch+1}] e-SNLI Acc: {acc:.4f}\")\n",
        "    return model, pruned_self, pruned_cross\n",
        "\n",
        "# =========================\n",
        "# 5) Main\n",
        "# =========================\n",
        "\n",
        "def main():\n",
        "    # e-SNLI file paths (adjust if different)\n",
        "    data_files = {\n",
        "        \"train\": \"/content/drive/MyDrive/NLP_datasets/esnli/esnli_train.json\",\n",
        "        \"validation\": \"/content/drive/MyDrive/NLP_datasets/esnli/esnli_valid.json\",\n",
        "        \"test\": \"/content/drive/MyDrive/NLP_datasets/esnli/esnli_test.json\"\n",
        "    }\n",
        "    raw = load_dataset(\"json\", data_files=data_files)\n",
        "    tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
        "    label_texts = [\"entailment\", \"neutral\", \"contradiction\"]\n",
        "\n",
        "    # Smaller subsets for demo; scale up as you wish\n",
        "    train_ds = raw[\"train\"].shuffle(seed=42).select(range(10000))\n",
        "    dev_ds   = raw[\"validation\"].shuffle(seed=42).select(range(2000))\n",
        "\n",
        "    train = train_ds.map(lambda ex: preprocess_function(ex, tokenizer),\n",
        "                         batched=True, remove_columns=train_ds.column_names)\n",
        "    dev   = dev_ds.map(lambda ex: preprocess_function(ex, tokenizer),\n",
        "                       batched=True, remove_columns=dev_ds.column_names)\n",
        "\n",
        "    collator = DataCollatorForSeq2Seq(tokenizer, model=None, padding=\"max_length\", max_length=128)\n",
        "    train_loader = DataLoader(train, batch_size=16, shuffle=True,  collate_fn=collator)\n",
        "    dev_loader   = DataLoader(dev,   batch_size=16, shuffle=False, collate_fn=collator)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Stage 1: Train + collect attention-only J-dev\n",
        "    model, self_scores, cross_scores = full_finetuning_collect_attn_jdev(\n",
        "        train_loader, dev_loader, device, tokenizer, label_texts,\n",
        "        jvp_eps=1e-3, jvp_k=1, jvp_every=1, epochs=6\n",
        "    )\n",
        "\n",
        "    # Stage 2: Prune attention ONLY (no FFN pruning) and FT\n",
        "    # Example: prune 4 lowest self-attn, keep cross-attn intact\n",
        "    model, pruned_self, pruned_cross = prune_attention_and_finetune(\n",
        "        model, train_loader, dev_loader, device,\n",
        "        self_scores, cross_scores, tokenizer, label_texts,\n",
        "        k_self=num, k_cross=0, epochs=5\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wNpT-rTYJBE"
      },
      "outputs": [],
      "source": [
        "# Autograd-JVP pruning — ONLY decoder attention layers (self & cross)\n",
        "# NaN-hardened and mask/bias-safe\n",
        "\n",
        "# ====== Colab Drive (no-op off Colab) ======\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ====== Imports ======\n",
        "import inspect\n",
        "import warnings\n",
        "import math\n",
        "from collections import defaultdict\n",
        "from contextlib import contextmanager\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration, T5TokenizerFast,\n",
        "    DataCollatorForSeq2Seq, get_linear_schedule_with_warmup\n",
        ")\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# =====================================================================================\n",
        "# 0) Helpers for Autograd-JVP and kwargs/mask handling\n",
        "# =====================================================================================\n",
        "\n",
        "def _filter_kwargs_for_module(module, kwargs):\n",
        "    sig = inspect.signature(module.forward)\n",
        "    allowed = set(sig.parameters.keys()) - {\"self\"}\n",
        "    return {k: v for k, v in kwargs.items() if k in allowed}\n",
        "\n",
        "def _map_args_from_hook(module, args, kwargs):\n",
        "    if kwargs and len(kwargs) > 0:\n",
        "        d = dict(kwargs)\n",
        "    else:\n",
        "        sig = inspect.signature(module.forward)\n",
        "        names = [n for n in sig.parameters.keys() if n != \"self\"]\n",
        "        d = {}\n",
        "        for i, name in enumerate(names):\n",
        "            d[name] = args[i] if i < len(args) else sig.parameters[name].default\n",
        "    d.pop(\"hidden_states\", None)\n",
        "    # Never keep position biases captured from some other step/length\n",
        "    d.pop(\"position_bias\", None)\n",
        "    d.pop(\"encoder_decoder_position_bias\", None)\n",
        "    return d  # filtering happens at call-time\n",
        "\n",
        "def _build_causal_mask(B, q_len, k_len, device, dtype):\n",
        "    # stable large negative instead of -inf to avoid NaNs in softmax\n",
        "    mask = torch.zeros((B, 1, q_len, k_len), dtype=dtype, device=device)\n",
        "    tri = torch.triu(torch.ones((q_len, k_len), dtype=torch.bool, device=device), diagonal=1)\n",
        "    mask[:, :, tri] = -1e9\n",
        "    return mask\n",
        "\n",
        "def _build_zero_mask(B, q_len, k_len, device, dtype):\n",
        "    return torch.zeros((B, 1, q_len, k_len), dtype=dtype, device=device)\n",
        "\n",
        "def _ensure_mask_shape(mask, q_len, k_len, device, dtype, is_self, B_fallback=1):\n",
        "    \"\"\"\n",
        "    Return a (B,1,q_len,k_len) mask; if input mask is missing or wrong-sized,\n",
        "    rebuild a safe default (causal for self, zeros for cross).\n",
        "    \"\"\"\n",
        "    default_fn = _build_causal_mask if is_self else _build_zero_mask\n",
        "\n",
        "    if mask is None:\n",
        "        return default_fn(B_fallback, q_len, k_len, device, dtype)\n",
        "\n",
        "    if mask.dim() == 2:\n",
        "        B = mask.size(0)\n",
        "        return mask[:, None, None, :k_len].expand(B, 1, q_len, k_len).contiguous().to(device=device, dtype=dtype)\n",
        "\n",
        "    if mask.dim() == 4:\n",
        "        B, _, Q, K = mask.shape\n",
        "        if Q >= q_len and K >= k_len:\n",
        "            return mask[:, :, :q_len, :k_len].to(device=device, dtype=dtype)\n",
        "        # cannot slice up to desired size → rebuild\n",
        "        return default_fn(B, q_len, k_len, device, dtype)\n",
        "\n",
        "    # unknown shape → rebuild\n",
        "    return default_fn(B_fallback, q_len, k_len, device, dtype)\n",
        "\n",
        "@contextmanager\n",
        "def force_math_sdp():\n",
        "    \"\"\"\n",
        "    Force 'math' scaled-dot-product attention kernels so higher-order grads exist.\n",
        "    Falls back to a no-op context if backend is unavailable (e.g., CPU).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with torch.backends.cuda.sdp_kernel(enable_flash=False,\n",
        "                                            enable_mem_efficient=False,\n",
        "                                            enable_math=True):\n",
        "            yield\n",
        "    except Exception:\n",
        "        yield\n",
        "\n",
        "def _make_attn_callable(module, argdict):\n",
        "    \"\"\"\n",
        "    Build a pure function f(x) -> hidden_states for a single T5 sublayer (self/cross).\n",
        "    We (re)construct masks and never reuse captured biases.\n",
        "    \"\"\"\n",
        "    raw = dict(argdict) if argdict is not None else {}\n",
        "\n",
        "    sig = inspect.signature(module.forward)\n",
        "    params = set(sig.parameters.keys())\n",
        "\n",
        "    is_cross = \"key_value_states\" in params\n",
        "\n",
        "    # Pack constant kwargs up-front (except hidden_states which will be the input)\n",
        "    const_kwargs = {}\n",
        "\n",
        "    # KV states (for cross attention)\n",
        "    if is_cross:\n",
        "        kv = raw.get(\"key_value_states\", raw.get(\"encoder_hidden_states\", None))\n",
        "        const_kwargs[\"key_value_states\"] = None if kv is None else kv.float()\n",
        "\n",
        "    # Deterministic flags\n",
        "    if \"use_cache\" in params:\n",
        "        const_kwargs[\"use_cache\"] = False\n",
        "    if \"output_attentions\" in params:\n",
        "        const_kwargs[\"output_attentions\"] = False\n",
        "    if \"return_dict\" in params:\n",
        "        const_kwargs[\"return_dict\"] = False\n",
        "\n",
        "    # We'll rebuild attention mask at call time (depends on x length)\n",
        "    attn_mask_key = \"attention_mask\" if \"attention_mask\" in params else (\"mask\" if \"mask\" in params else None)\n",
        "    captured_mask = raw.get(attn_mask_key, raw.get(\"mask\", None)) if attn_mask_key is not None else None\n",
        "    captured_kv   = const_kwargs.get(\"key_value_states\", None)\n",
        "\n",
        "    def _f(x):\n",
        "        x = x.float()\n",
        "        q_len = x.size(1)\n",
        "        k_len = captured_kv.size(1) if (is_cross and captured_kv is not None) else q_len\n",
        "        B     = x.size(0)\n",
        "\n",
        "        kwargs = {\"hidden_states\": x}\n",
        "        if is_cross:\n",
        "            if captured_kv is None:\n",
        "                # no encoder states available -> identity\n",
        "                return x\n",
        "            kwargs[\"key_value_states\"] = captured_kv\n",
        "\n",
        "        if attn_mask_key is not None:\n",
        "            kwargs[attn_mask_key] = _ensure_mask_shape(\n",
        "                captured_mask, q_len, k_len, x.device, x.dtype, is_self=(not is_cross), B_fallback=B\n",
        "            )\n",
        "\n",
        "        # cache_position / query_length\n",
        "        if \"cache_position\" in params:\n",
        "            kwargs[\"cache_position\"] = torch.arange(q_len, dtype=torch.long, device=x.device)\n",
        "        if \"query_length\" in params and \"cache_position\" not in params:\n",
        "            kwargs[\"query_length\"] = q_len\n",
        "\n",
        "        # Never pass position biases from capture; force recompute\n",
        "        kwargs.update(const_kwargs)\n",
        "        kwargs = _filter_kwargs_for_module(module, kwargs)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=False):\n",
        "            out = module(**kwargs)\n",
        "\n",
        "        hs = out[0] if isinstance(out, (tuple, list)) else out\n",
        "        return hs\n",
        "\n",
        "    return _f\n",
        "\n",
        "def compute_decoder_attention_jdev_autograd(model, self_bufs, cross_bufs, k_probes=1, rademacher=True):\n",
        "    \"\"\"\n",
        "    For each decoder attention sublayer ℓ, estimate:\n",
        "      E_v ||(Jℓ - I)v||^2 using autograd JVP.\n",
        "    Non-finite probes are skipped. Buffers are cleared.\n",
        "    \"\"\"\n",
        "    from torch.autograd.functional import jvp as autograd_jvp\n",
        "\n",
        "    self_scores, cross_scores = {}, {}\n",
        "\n",
        "    # Self-attention J-dev\n",
        "    for idx, buf in self_bufs.items():\n",
        "        X, args = buf[\"X\"], buf[\"args\"]\n",
        "        buf[\"X\"], buf[\"args\"] = None, None  # clear ASAP\n",
        "        if X is None:\n",
        "            continue\n",
        "        mod = model.decoder.block[idx].layer[0]  # T5LayerSelfAttention\n",
        "\n",
        "        was_training = mod.training\n",
        "        mod.eval()\n",
        "        try:\n",
        "            fn = _make_attn_callable(mod, args)\n",
        "            x0 = X.detach().requires_grad_(True).float()\n",
        "\n",
        "            # Sanity forward\n",
        "            try:\n",
        "                with force_math_sdp():\n",
        "                    _ = fn(x0)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            acc, used = 0.0, 0\n",
        "            with force_math_sdp():\n",
        "                for _ in range(max(1, k_probes)):\n",
        "                    if rademacher:\n",
        "                        v = torch.empty_like(x0).bernoulli_(0.5).mul_(2).sub_(1)\n",
        "                    else:\n",
        "                        v = torch.randn_like(x0)\n",
        "                    try:\n",
        "                        _, Jv = autograd_jvp(fn, (x0,), (v,), create_graph=False, strict=True)\n",
        "                    except Exception:\n",
        "                        continue\n",
        "                    if Jv is None or not torch.isfinite(Jv).all():\n",
        "                        continue\n",
        "                    jd_vec = Jv - v\n",
        "                    if not torch.isfinite(jd_vec).all():\n",
        "                        continue\n",
        "                    acc += float(jd_vec.pow(2).mean().item())\n",
        "                    used += 1\n",
        "            if used > 0:\n",
        "                self_scores[idx] = acc / used\n",
        "        finally:\n",
        "            mod.train(was_training)\n",
        "\n",
        "    # Cross-attention J-dev\n",
        "    for idx, buf in cross_bufs.items():\n",
        "        X, args = buf[\"X\"], buf[\"args\"]\n",
        "        buf[\"X\"], buf[\"args\"] = None, None\n",
        "        if X is None:\n",
        "            continue\n",
        "        if len(model.decoder.block[idx].layer) <= 1 or model.decoder.block[idx].layer[1] is None:\n",
        "            continue\n",
        "        mod = model.decoder.block[idx].layer[1]  # T5LayerCrossAttention\n",
        "\n",
        "        was_training = mod.training\n",
        "        mod.eval()\n",
        "        try:\n",
        "            fn = _make_attn_callable(mod, args)\n",
        "            x0 = X.detach().requires_grad_(True).float()\n",
        "\n",
        "            try:\n",
        "                with force_math_sdp():\n",
        "                    _ = fn(x0)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            acc, used = 0.0, 0\n",
        "            with force_math_sdp():\n",
        "                for _ in range(max(1, k_probes)):\n",
        "                    if rademacher:\n",
        "                        v = torch.empty_like(x0).bernoulli_(0.5).mul_(2).sub_(1)\n",
        "                    else:\n",
        "                        v = torch.randn_like(x0)\n",
        "                    try:\n",
        "                        _, Jv = autograd_jvp(fn, (x0,), (v,), create_graph=False, strict=True)\n",
        "                    except Exception:\n",
        "                        continue\n",
        "                    if Jv is None or not torch.isfinite(Jv).all():\n",
        "                        continue\n",
        "                    jd_vec = Jv - v\n",
        "                    if not torch.isfinite(jd_vec).all():\n",
        "                        continue\n",
        "                    acc += float(jd_vec.pow(2).mean().item())\n",
        "                    used += 1\n",
        "            if used > 0:\n",
        "                cross_scores[idx] = acc / used\n",
        "        finally:\n",
        "            mod.train(was_training)\n",
        "\n",
        "    return self_scores, cross_scores\n",
        "\n",
        "# =====================================================================================\n",
        "# 1) Hook Utilities — capture ONLY decoder attention sublayer inputs/args\n",
        "# =====================================================================================\n",
        "\n",
        "def register_decoder_attention_hooks(model):\n",
        "    \"\"\"\n",
        "    Capture inputs/kwargs for decoder self-attention (layer[0]) and cross-attention (layer[1]).\n",
        "    \"\"\"\n",
        "    dec_blocks = model.decoder.block\n",
        "    self_bufs  = {i: {\"X\": None, \"args\": None} for i in range(len(dec_blocks))}\n",
        "    cross_bufs = {i: {\"X\": None, \"args\": None} for i in range(len(dec_blocks))}\n",
        "    hooks = []\n",
        "\n",
        "    # Self-attention pre-hooks\n",
        "    for i, block in enumerate(dec_blocks):\n",
        "        sa = block.layer[0]\n",
        "        def pre_hook_sa(module, args, kwargs, idx=i):\n",
        "            X = kwargs.get(\"hidden_states\", args[0] if len(args) > 0 else None)\n",
        "            self_bufs[idx][\"X\"]    = None if X is None else X.detach()\n",
        "            self_bufs[idx][\"args\"] = _map_args_from_hook(module, args, kwargs)\n",
        "        hooks.append(sa.register_forward_pre_hook(pre_hook_sa, with_kwargs=True))\n",
        "\n",
        "    # Cross-attention pre-hooks\n",
        "    for i, block in enumerate(dec_blocks):\n",
        "        if len(block.layer) > 1 and block.layer[1] is not None:\n",
        "            ca = block.layer[1]\n",
        "            def pre_hook_ca(module, args, kwargs, idx=i):\n",
        "                X = kwargs.get(\"hidden_states\", args[0] if len(args) > 0 else None)\n",
        "                cross_bufs[idx][\"X\"]    = None if X is None else X.detach()\n",
        "                cross_bufs[idx][\"args\"] = _map_args_from_hook(module, args, kwargs)\n",
        "            hooks.append(ca.register_forward_pre_hook(pre_hook_ca, with_kwargs=True))\n",
        "\n",
        "    return hooks, self_bufs, cross_bufs\n",
        "\n",
        "def remove_hooks(hooks):\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "# =====================================================================================\n",
        "# 2) Attention-only pruning utilities\n",
        "# =====================================================================================\n",
        "\n",
        "class SkipSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Identity replacement for T5LayerSelfAttention.\n",
        "    IMPORTANT: return (hidden_states, None, None) to avoid propagating stale position bias.\n",
        "    \"\"\"\n",
        "    def forward(self, hidden_states, *args, **kwargs):\n",
        "        return (hidden_states, None, None)\n",
        "\n",
        "class SkipCrossAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Identity replacement for T5LayerCrossAttention.\n",
        "    Also return None for position_bias to force recomputation downstream.\n",
        "    \"\"\"\n",
        "    def forward(self, hidden_states, *args, **kwargs):\n",
        "        return (hidden_states, None, None)\n",
        "\n",
        "def prune_decoder_attention(model, self_scores, cross_scores,\n",
        "                            k_self=2, k_cross=2,\n",
        "                            protect_first=True, protect_last=False, verbose=True):\n",
        "    \"\"\"\n",
        "    Prune LOW J-dev attention sublayers (near-identity). FFNs untouched.\n",
        "    \"\"\"\n",
        "    num_layers = len(model.decoder.block)\n",
        "    self_items  = list(self_scores.items())\n",
        "    cross_items = list(cross_scores.items())\n",
        "\n",
        "    if protect_first:\n",
        "        self_items  = [(i,s) for i,s in self_items  if i != 0]\n",
        "        cross_items = [(i,s) for i,s in cross_items if i != 0]\n",
        "    if protect_last:\n",
        "        self_items  = [(i,s) for i,s in self_items  if i != num_layers-1]\n",
        "        cross_items = [(i,s) for i,s in cross_items if i != num_layers-1]\n",
        "\n",
        "    self_items.sort(key=lambda x: x[1])   # lowest first\n",
        "    cross_items.sort(key=lambda x: x[1])  # lowest first\n",
        "\n",
        "    pruned_self, pruned_cross = [], []\n",
        "\n",
        "    for i, _ in self_items[:max(0, k_self)]:\n",
        "        model.decoder.block[i].layer[0] = SkipSelfAttention()\n",
        "        pruned_self.append(i)\n",
        "\n",
        "    for i, _ in cross_items[:max(0, k_cross)]:\n",
        "        if len(model.decoder.block[i].layer) > 1 and model.decoder.block[i].layer[1] is not None:\n",
        "            model.decoder.block[i].layer[1] = SkipCrossAttention()\n",
        "            pruned_cross.append(i)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Pruned decoder SELF-attention at layers:  {pruned_self}\")\n",
        "        print(f\"Pruned decoder CROSS-attention at layers: {pruned_cross}\")\n",
        "\n",
        "    return pruned_self, pruned_cross\n",
        "\n",
        "# =====================================================================================\n",
        "# 3) Data & eval helpers (e-SNLI)\n",
        "# =====================================================================================\n",
        "\n",
        "def make_t5_nli_prompt(premise, hypothesis):\n",
        "    return f\"nli premise: {premise} hypothesis: {hypothesis}\"\n",
        "\n",
        "def preprocess_function(batch, tokenizer, max_input_length=128, max_target_length=8):\n",
        "    inputs = [make_t5_nli_prompt(p, h) for p, h in zip(batch['premise'], batch['hypothesis'])]\n",
        "    model_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=max_input_length)\n",
        "    label_list = [\"entailment\", \"neutral\", \"contradiction\"]\n",
        "    labels = [label_list[x] if isinstance(x, int) and x < len(label_list) else x for x in batch['label']]\n",
        "    target = tokenizer(labels, padding=\"max_length\", truncation=True, max_length=max_target_length)\n",
        "    model_inputs[\"labels\"] = target[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "def compute_accuracy(preds, refs):\n",
        "    correct = 0\n",
        "    for p, l in zip(preds, refs):\n",
        "        if p == l:\n",
        "            correct += 1\n",
        "    return correct / len(preds) if preds else 0.0\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, dl, tokenizer, device, label_texts):\n",
        "    # Disable cache to avoid KV/pos-bias drift after pruning\n",
        "    model.config.use_cache = False\n",
        "    model.eval()\n",
        "    preds, refs = [], []\n",
        "    for batch in dl:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=2,\n",
        "            use_cache=False\n",
        "        )\n",
        "        pred_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        label_ids = batch[\"labels\"].clone()\n",
        "        label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "        ref_texts = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "        preds.extend([p.strip().lower() for p in pred_texts])\n",
        "        refs.extend([l.strip().lower() for l in ref_texts])\n",
        "    return compute_accuracy(preds, refs)\n",
        "\n",
        "# =====================================================================================\n",
        "# 4) Train + collect attention J-dev (Autograd JVP), then prune attention-only\n",
        "# =====================================================================================\n",
        "\n",
        "def full_finetuning_collect_attn_jdev(train_loader, dev_loader, device, tokenizer, label_texts,\n",
        "                                      jvp_k=1, jvp_every=1, epochs=6, rademacher=True):\n",
        "    \"\"\"\n",
        "    Stage 1: Full fine-tuning while periodically collecting Autograd-JVP J-dev\n",
        "    for decoder self-attention and cross-attention sublayers.\n",
        "    \"\"\"\n",
        "    print(\"=== Stage 1: Full FT & Autograd-JVP (Decoder Attention only) ===\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n",
        "    model.config.use_cache = False  # avoid KV cache during JVP collection\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "    scaler = GradScaler()\n",
        "    sched = get_linear_schedule_with_warmup(opt, 0, len(train_loader)*3)\n",
        "\n",
        "    hooks, self_bufs, cross_bufs = register_decoder_attention_hooks(model)\n",
        "    self_sum, self_cnt   = defaultdict(float), defaultdict(int)\n",
        "    cross_sum, cross_cnt = defaultdict(float), defaultdict(int)\n",
        "\n",
        "    step = 0\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            step += 1\n",
        "            opt.zero_grad()\n",
        "            with autocast():\n",
        "                out = model(input_ids=batch[\"input_ids\"].to(device),\n",
        "                            attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                            labels=batch[\"labels\"].to(device))\n",
        "                loss = out.loss\n",
        "                scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "\n",
        "            if step % max(1, jvp_every) == 0:\n",
        "                self_scores, cross_scores = compute_decoder_attention_jdev_autograd(\n",
        "                    model, self_bufs, cross_bufs, k_probes=jvp_k, rademacher=rademacher\n",
        "                )\n",
        "                for i, v in self_scores.items():\n",
        "                    if math.isfinite(v):\n",
        "                        self_sum[i]  += v; self_cnt[i]  += 1\n",
        "                for i, v in cross_scores.items():\n",
        "                    if math.isfinite(v):\n",
        "                        cross_sum[i] += v; cross_cnt[i] += 1\n",
        "\n",
        "        epoch_self  = {i: self_sum[i]/self_cnt[i]   for i in self_sum   if self_cnt[i]   > 0}\n",
        "        epoch_cross = {i: cross_sum[i]/cross_cnt[i] for i in cross_sum  if cross_cnt[i]  > 0}\n",
        "        print(f\"[Epoch {epoch+1}] Decoder SELF-attn J-dev:  {epoch_self}\")\n",
        "        print(f\"[Epoch {epoch+1}] Decoder CROSS-attn J-dev: {epoch_cross}\")\n",
        "        acc = evaluate_model(model, dev_loader, tokenizer, device, label_texts)\n",
        "        print(f\"[Epoch {epoch+1}] Dev Acc: {acc:.4f}\")\n",
        "\n",
        "    remove_hooks(hooks)\n",
        "    final_self  = {i: self_sum[i]/self_cnt[i]   for i in self_sum   if self_cnt[i]   > 0}\n",
        "    final_cross = {i: cross_sum[i]/cross_cnt[i] for i in cross_sum  if cross_cnt[i]  > 0}\n",
        "    return model, final_self, final_cross\n",
        "\n",
        "def prune_attention_and_finetune(model, train_loader, dev_loader, device,\n",
        "                                 self_scores, cross_scores, tokenizer, label_texts,\n",
        "                                 k_self=2, k_cross=2, epochs=5):\n",
        "    print(\"=== Stage 2: Prune LOW-Jdev Attention (self/cross) & FT ===\")\n",
        "    pruned_self, pruned_cross = prune_decoder_attention(\n",
        "        model, self_scores, cross_scores, k_self=k_self, k_cross=k_cross,\n",
        "        protect_first=True, protect_last=False, verbose=True\n",
        "    )\n",
        "\n",
        "    model.config.use_cache = False  # keep disabled after pruning\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "    sched = get_linear_schedule_with_warmup(opt, 0, len(train_loader)*2)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            opt.zero_grad()\n",
        "            out = model(input_ids=batch[\"input_ids\"].to(device),\n",
        "                        attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                        labels=batch[\"labels\"].to(device))\n",
        "            loss = out.loss\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, tokenizer, device, label_texts)\n",
        "        print(f\"[Prune FT Epoch {epoch+1}] e-SNLI Acc: {acc:.4f}\")\n",
        "    return model, pruned_self, pruned_cross\n",
        "\n",
        "# =====================================================================================\n",
        "# 5) Main\n",
        "# =====================================================================================\n",
        "\n",
        "def main():\n",
        "    # e-SNLI file paths (adjust if different)\n",
        "    data_files = {\n",
        "        \"train\": \"/content/drive/MyDrive/NLP_datasets/esnli/esnli_train.json\",\n",
        "        \"validation\": \"/content/drive/MyDrive/NLP_datasets/esnli/esnli_valid.json\",\n",
        "        \"test\": \"/content/drive/MyDrive/NLP_datasets/esnli/esnli_test.json\"\n",
        "    }\n",
        "    raw = load_dataset(\"json\", data_files=data_files)\n",
        "    tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
        "    label_texts = [\"entailment\", \"neutral\", \"contradiction\"]\n",
        "\n",
        "    # Smaller subsets for demo; scale up as you wish\n",
        "    train_ds = raw[\"train\"].shuffle(seed=42).select(range(10000))\n",
        "    dev_ds   = raw[\"validation\"].shuffle(seed=42).select(range(2000))\n",
        "\n",
        "    train = train_ds.map(lambda ex: preprocess_function(ex, tokenizer),\n",
        "                         batched=True, remove_columns=train_ds.column_names)\n",
        "    dev   = dev_ds.map(lambda ex: preprocess_function(ex, tokenizer),\n",
        "                       batched=True, remove_columns=dev_ds.column_names)\n",
        "\n",
        "    collator = DataCollatorForSeq2Seq(tokenizer, model=None, padding=\"max_length\", max_length=128)\n",
        "    train_loader = DataLoader(train, batch_size=16, shuffle=True,  collate_fn=collator)\n",
        "    dev_loader   = DataLoader(dev,   batch_size=16, shuffle=False, collate_fn=collator)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Stage 1: Train + collect attention-only J-dev (Autograd JVP)\n",
        "    model, self_scores, cross_scores = full_finetuning_collect_attn_jdev(\n",
        "        train_loader, dev_loader, device, tokenizer, label_texts,\n",
        "        jvp_k=2, jvp_every=1, epochs=6, rademacher=True\n",
        "    )\n",
        "\n",
        "    # Stage 2: Prune attention ONLY (no FFN pruning) and FT\n",
        "    # Example: prune 4 lowest self-attn, keep cross-attn intact\n",
        "    model, pruned_self, pruned_cross = prune_attention_and_finetune(\n",
        "        model, train_loader, dev_loader, device,\n",
        "        self_scores, cross_scores, tokenizer, label_texts,\n",
        "        k_self=num, k_cross=0, epochs=5\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw7_IaqIYJdu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrZO22V9jpvG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3URfaocE60Y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LnbaRLQ-HOc"
      },
      "outputs": [],
      "source": [
        "# Prune the decoder (ATTENTION-ONLY) with Finite-Difference JVP — CQA\n",
        "\n",
        "# ====== Colab Drive (no-op off Colab) ======\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ====== Imports ======\n",
        "import inspect\n",
        "import warnings\n",
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration, T5TokenizerFast,\n",
        "    DataCollatorForSeq2Seq, get_linear_schedule_with_warmup\n",
        ")\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# =========================\n",
        "# 0) CQA data\n",
        "# =========================\n",
        "data_files = {\n",
        "    \"train\": \"/content/drive/MyDrive/NLP_datasets/cqa/cqa_train.json\",\n",
        "    \"test\":  \"/content/drive/MyDrive/NLP_datasets/cqa/cqa_test.json\"\n",
        "}\n",
        "dataset = load_dataset(\"json\", data_files=data_files)\n",
        "\n",
        "def preprocess_cqa(batch, tokenizer, max_input_length=128, max_target_length=8, use_cot=False):\n",
        "    if use_cot and 'abstractive_explanation' in batch:\n",
        "        inputs = [\n",
        "            f\"question: {q} choices: {', '.join(choices)} rationale: {exp}\"\n",
        "            for q, choices, exp in zip(batch['question'], batch['choices'], batch['abstractive_explanation'])\n",
        "        ]\n",
        "    else:\n",
        "        inputs = [\n",
        "            f\"question: {q} choices: {', '.join(choices)}\"\n",
        "            for q, choices in zip(batch['question'], batch['choices'])\n",
        "        ]\n",
        "    targets = [str(ans).strip() for ans in batch['answer']]\n",
        "    model_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=max_input_length)\n",
        "    target = tokenizer(targets, padding=\"max_length\", truncation=True, max_length=max_target_length)\n",
        "    model_inputs[\"labels\"] = target[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
        "USE_COT = False\n",
        "\n",
        "train = dataset[\"train\"].map(lambda ex: preprocess_cqa(ex, tokenizer, use_cot=USE_COT),\n",
        "                             batched=True, remove_columns=dataset[\"train\"].column_names)\n",
        "dev   = dataset[\"test\"].map(lambda ex: preprocess_cqa(ex, tokenizer, use_cot=False),\n",
        "                             batched=True, remove_columns=dataset[\"test\"].column_names)\n",
        "\n",
        "collator = DataCollatorForSeq2Seq(tokenizer, model=None, padding=\"max_length\", max_length=128)\n",
        "train_loader = DataLoader(train, batch_size=32, shuffle=True,  collate_fn=collator)\n",
        "dev_loader   = DataLoader(dev,   batch_size=32, shuffle=False, collate_fn=collator)\n",
        "\n",
        "# =========================\n",
        "# 1) Eval helpers\n",
        "# =========================\n",
        "def compute_accuracy(preds, refs):\n",
        "    correct = 0\n",
        "    for p, l in zip(preds, refs):\n",
        "        if p == l:\n",
        "            correct += 1\n",
        "    return correct / len(preds) if len(preds) > 0 else 0.0\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, dl, tokenizer, device):\n",
        "    # disable cache to avoid KV/pos-bias drift after pruning\n",
        "    model.config.use_cache = False\n",
        "    model.eval()\n",
        "    preds, refs = [], []\n",
        "    for batch in dl:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=4,\n",
        "            use_cache=False,          # critical after pruning\n",
        "        )\n",
        "        pred_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        label_ids = batch[\"labels\"].clone()\n",
        "        label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "        ref_texts = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "        preds.extend([p.strip().lower() for p in pred_texts])\n",
        "        refs.extend([l.strip().lower() for l in ref_texts])\n",
        "    return compute_accuracy(preds, refs)\n",
        "\n",
        "# =========================\n",
        "# 2) FD-JVP helpers (robust)\n",
        "# =========================\n",
        "def _filter_kwargs_for_module(module, kwargs):\n",
        "    sig = inspect.signature(module.forward)\n",
        "    allowed = set(sig.parameters.keys()) - {\"self\"}\n",
        "    return {k: v for k, v in kwargs.items() if k in allowed}\n",
        "\n",
        "def _map_args_from_hook(module, args, kwargs):\n",
        "    if kwargs and len(kwargs) > 0:\n",
        "        d = dict(kwargs)\n",
        "    else:\n",
        "        sig = inspect.signature(module.forward)\n",
        "        names = [n for n in sig.parameters.keys() if n != \"self\"]\n",
        "        d = {}\n",
        "        for i, name in enumerate(names):\n",
        "            if i < len(args):\n",
        "                d[name] = args[i]\n",
        "            else:\n",
        "                d[name] = sig.parameters[name].default\n",
        "    d.pop(\"hidden_states\", None)\n",
        "    return d  # filter at call time\n",
        "\n",
        "def _build_causal_mask(B, q_len, k_len, device, dtype):\n",
        "    # use stable large negative (not -inf) to avoid NaNs in softmax\n",
        "    mask = torch.zeros((B, 1, q_len, k_len), dtype=dtype, device=device)\n",
        "    tri = torch.triu(torch.ones((q_len, k_len), dtype=torch.bool, device=device), diagonal=1)\n",
        "    mask[:, :, tri] = -1e9\n",
        "    return mask\n",
        "\n",
        "def _ensure_mask_shape(mask, q_len, k_len, device, dtype, is_self):\n",
        "    \"\"\"\n",
        "    Return a (B,1,q_len,k_len) mask; if input mask is missing or wrong-sized,\n",
        "    rebuild a safe default (causal for self, zeros for cross).\n",
        "    \"\"\"\n",
        "    if mask is None:\n",
        "        B = 1\n",
        "        return _build_causal_mask(B, q_len, k_len, device, dtype) if is_self \\\n",
        "               else torch.zeros((B,1,q_len,k_len), dtype=dtype, device=device)\n",
        "    if mask.dim() == 2:\n",
        "        B = mask.size(0)\n",
        "        return mask[:, None, None, :k_len].expand(B,1,q_len,k_len).contiguous().to(device=device, dtype=dtype)\n",
        "    if mask.dim() == 4:\n",
        "        B, _, Q, K = mask.shape\n",
        "        if Q >= q_len and K >= k_len:\n",
        "            return mask[:, :, :q_len, :k_len].to(device=device, dtype=dtype)\n",
        "        return _build_causal_mask(B, q_len, k_len, device, dtype) if is_self \\\n",
        "               else torch.zeros((B,1,q_len,k_len), dtype=dtype, device=device)\n",
        "    B = 1\n",
        "    return _build_causal_mask(B, q_len, k_len, device, dtype) if is_self \\\n",
        "           else torch.zeros((B,1,q_len,k_len), dtype=dtype, device=device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def _t5sublayer_forward_only(module, X, argdict):\n",
        "    \"\"\"\n",
        "    Deterministic forward for a single T5 sublayer (Self/Cross Attention).\n",
        "    Returns hidden_states (tensor). fp32, autocast disabled. Provides safe masks\n",
        "    and cache_position/query_length when missing.\n",
        "    \"\"\"\n",
        "    if X is None:\n",
        "        return None\n",
        "    was_training = module.training\n",
        "    module.eval()\n",
        "    Xf = X.float()\n",
        "    try:\n",
        "        raw = dict(argdict) if argdict is not None else {}\n",
        "        raw.pop(\"position_bias\", None)  # force recompute\n",
        "\n",
        "        sig = inspect.signature(module.forward)\n",
        "        params = set(sig.parameters.keys())\n",
        "\n",
        "        kwargs = {\"hidden_states\": Xf}\n",
        "\n",
        "        # Cross-attention?\n",
        "        is_cross = \"key_value_states\" in params\n",
        "        kv = None\n",
        "        if is_cross:\n",
        "            kv = raw.get(\"key_value_states\", raw.get(\"encoder_hidden_states\", None))\n",
        "            if kv is None:\n",
        "                return None\n",
        "            kwargs[\"key_value_states\"] = kv.float()\n",
        "\n",
        "        q_len = Xf.size(1)\n",
        "        k_len = kv.size(1) if (is_cross and kv is not None) else q_len\n",
        "\n",
        "        mask_key = \"attention_mask\" if \"attention_mask\" in params else (\"mask\" if \"mask\" in params else None)\n",
        "        src_mask = raw.get(mask_key, raw.get(\"mask\", None)) if mask_key is not None else None\n",
        "        kwargs[mask_key] = _ensure_mask_shape(src_mask, q_len, k_len, Xf.device, Xf.dtype, is_self=not is_cross)\n",
        "\n",
        "        # cache_position / query_length\n",
        "        if \"cache_position\" in params:\n",
        "            cp = raw.get(\"cache_position\", None)\n",
        "            if cp is None:\n",
        "                cp = torch.arange(q_len, dtype=torch.long, device=Xf.device)\n",
        "            kwargs[\"cache_position\"] = cp\n",
        "        if \"query_length\" in params and \"cache_position\" not in params:\n",
        "            kwargs[\"query_length\"] = q_len\n",
        "\n",
        "        if \"use_cache\" in params:\n",
        "            kwargs[\"use_cache\"] = False\n",
        "        if \"output_attentions\" in params:\n",
        "            kwargs[\"output_attentions\"] = False\n",
        "\n",
        "        kwargs = _filter_kwargs_for_module(module, kwargs)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=False):\n",
        "            out = module(**kwargs)\n",
        "\n",
        "        hs = out[0] if isinstance(out, (tuple, list)) else out\n",
        "        if hs is None or not torch.isfinite(hs).all():\n",
        "            return None\n",
        "        return hs\n",
        "    finally:\n",
        "        module.train(was_training)\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_decoder_attention_jdev(model, self_bufs, cross_bufs, eps=1e-3, k_probes=1):\n",
        "    \"\"\"\n",
        "    For each decoder attention sublayer ℓ, estimate:\n",
        "      E_v ||(Jℓ - I)v||^2 ≈ E_v || (f(x + eps v) - f(x))/eps - v ||^2\n",
        "    Non-finite probes are skipped.\n",
        "    \"\"\"\n",
        "    self_scores, cross_scores = {}, {}\n",
        "\n",
        "    # Self-attention J-dev\n",
        "    for idx, buf in self_bufs.items():\n",
        "        X, args = buf[\"X\"], buf[\"args\"]\n",
        "        if X is None:\n",
        "            continue\n",
        "        mod = model.decoder.block[idx].layer[0]\n",
        "        X = X.float()\n",
        "        y0 = _t5sublayer_forward_only(mod, X, args)\n",
        "        if y0 is None:\n",
        "            buf[\"X\"], buf[\"args\"] = None, None\n",
        "            continue\n",
        "        acc, used = 0.0, 0\n",
        "        for _ in range(max(1, k_probes)):\n",
        "            v = torch.randn_like(X)\n",
        "            y_eps = _t5sublayer_forward_only(mod, X + eps * v, args)\n",
        "            if y_eps is None:\n",
        "                continue\n",
        "            jd_vec = (y_eps - y0) / eps - v\n",
        "            if not torch.isfinite(jd_vec).all():\n",
        "                continue\n",
        "            acc += float(jd_vec.pow(2).mean().item())\n",
        "            used += 1\n",
        "        if used > 0:\n",
        "            self_scores[idx] = acc / used\n",
        "        buf[\"X\"], buf[\"args\"] = None, None\n",
        "\n",
        "    # Cross-attention J-dev\n",
        "    for idx, buf in cross_bufs.items():\n",
        "        X, args = buf[\"X\"], buf[\"args\"]\n",
        "        if X is None:\n",
        "            continue\n",
        "        if len(model.decoder.block[idx].layer) <= 1 or model.decoder.block[idx].layer[1] is None:\n",
        "            continue\n",
        "        mod = model.decoder.block[idx].layer[1]\n",
        "        X = X.float()\n",
        "        y0 = _t5sublayer_forward_only(mod, X, args)\n",
        "        if y0 is None:\n",
        "            buf[\"X\"], buf[\"args\"] = None, None\n",
        "            continue\n",
        "        acc, used = 0.0, 0\n",
        "        for _ in range(max(1, k_probes)):\n",
        "            v = torch.randn_like(X)\n",
        "            y_eps = _t5sublayer_forward_only(mod, X + eps * v, args)\n",
        "            if y_eps is None:\n",
        "                continue\n",
        "            jd_vec = (y_eps - y0) / eps - v\n",
        "            if not torch.isfinite(jd_vec).all():\n",
        "                continue\n",
        "            acc += float(jd_vec.pow(2).mean().item())\n",
        "            used += 1\n",
        "        if used > 0:\n",
        "            cross_scores[idx] = acc / used\n",
        "        buf[\"X\"], buf[\"args\"] = None, None\n",
        "\n",
        "    return self_scores, cross_scores\n",
        "\n",
        "# =========================\n",
        "# 3) Hooks to capture ONLY decoder attention inputs\n",
        "# =========================\n",
        "def register_decoder_attention_hooks(model):\n",
        "    dec_blocks = model.decoder.block\n",
        "    self_bufs  = {i: {\"X\": None, \"args\": None} for i in range(len(dec_blocks))}\n",
        "    cross_bufs = {i: {\"X\": None, \"args\": None} for i in range(len(dec_blocks))}\n",
        "    hooks = []\n",
        "\n",
        "    for i, block in enumerate(dec_blocks):\n",
        "        sa = block.layer[0]\n",
        "        def pre_hook_sa(module, args, kwargs, idx=i):\n",
        "            X = kwargs.get(\"hidden_states\", args[0] if len(args) > 0 else None)\n",
        "            self_bufs[idx][\"X\"]    = None if X is None else X.detach()\n",
        "            self_bufs[idx][\"args\"] = _map_args_from_hook(module, args, kwargs)\n",
        "        hooks.append(sa.register_forward_pre_hook(pre_hook_sa, with_kwargs=True))\n",
        "\n",
        "    for i, block in enumerate(dec_blocks):\n",
        "        if len(block.layer) > 1 and block.layer[1] is not None:\n",
        "            ca = block.layer[1]\n",
        "            def pre_hook_ca(module, args, kwargs, idx=i):\n",
        "                X = kwargs.get(\"hidden_states\", args[0] if len(args) > 0 else None)\n",
        "                cross_bufs[idx][\"X\"]    = None if X is None else X.detach()\n",
        "                cross_bufs[idx][\"args\"] = _map_args_from_hook(module, args, kwargs)\n",
        "            hooks.append(ca.register_forward_pre_hook(pre_hook_ca, with_kwargs=True))\n",
        "\n",
        "    return hooks, self_bufs, cross_bufs\n",
        "\n",
        "def remove_hooks(hooks):\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "# =========================\n",
        "# 4) Attention-only pruning utilities\n",
        "# =========================\n",
        "class SkipSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Identity replacement for T5LayerSelfAttention.\n",
        "    IMPORTANT: return (hidden_states, None, None) so no stale position_bias is reused.\n",
        "    \"\"\"\n",
        "    def forward(self, hidden_states, *args, **kwargs):\n",
        "        return (hidden_states, None, None)\n",
        "\n",
        "class SkipCrossAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Identity replacement for T5LayerCrossAttention.\n",
        "    Also return None for position_bias to force recomputation downstream.\n",
        "    \"\"\"\n",
        "    def forward(self, hidden_states, *args, **kwargs):\n",
        "        return (hidden_states, None, None)\n",
        "\n",
        "def prune_decoder_attention(model, self_scores, cross_scores,\n",
        "                            k_self=2, k_cross=2,\n",
        "                            protect_first=True, protect_last=False, verbose=True):\n",
        "    num_layers = len(model.decoder.block)\n",
        "    self_items  = list(self_scores.items())\n",
        "    cross_items = list(cross_scores.items())\n",
        "\n",
        "    if protect_first:\n",
        "        self_items  = [(i,s) for i,s in self_items  if i != 0]\n",
        "        cross_items = [(i,s) for i,s in cross_items if i != 0]\n",
        "    if protect_last:\n",
        "        self_items  = [(i,s) for i,s in self_items  if i != num_layers-1]\n",
        "        cross_items = [(i,s) for i,s in cross_items if i != num_layers-1]\n",
        "\n",
        "    self_items.sort(key=lambda x: x[1])   # lowest J-dev first\n",
        "    cross_items.sort(key=lambda x: x[1])\n",
        "\n",
        "    pruned_self, pruned_cross = [], []\n",
        "\n",
        "    for i, _ in self_items[:max(0, k_self)]:\n",
        "        model.decoder.block[i].layer[0] = SkipSelfAttention()\n",
        "        pruned_self.append(i)\n",
        "\n",
        "    for i, _ in cross_items[:max(0, k_cross)]:\n",
        "        if len(model.decoder.block[i].layer) > 1 and model.decoder.block[i].layer[1] is not None:\n",
        "            model.decoder.block[i].layer[1] = SkipCrossAttention()\n",
        "            pruned_cross.append(i)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Pruned decoder SELF-attention at layers:  {pruned_self}\")\n",
        "        print(f\"Pruned decoder CROSS-attention at layers: {pruned_cross}\")\n",
        "\n",
        "    return pruned_self, pruned_cross\n",
        "\n",
        "# =========================\n",
        "# 5) Train → collect J-dev → prune attention → FT\n",
        "# =========================\n",
        "def full_finetuning_collect_attn_jdev(train_loader, dev_loader, device, tokenizer,\n",
        "                                      jvp_eps=1e-3, jvp_k=1, jvp_every=1, epochs=6):\n",
        "    print(\"=== Stage 1: Full FT & FD-JVP (Decoder Attention only) ===\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n",
        "    model.config.use_cache = False  # keep cache off while we collect FD-JVP\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "    scaler = GradScaler()\n",
        "    sched = get_linear_schedule_with_warmup(opt, 0, len(train_loader)*3)\n",
        "\n",
        "    hooks, self_bufs, cross_bufs = register_decoder_attention_hooks(model)\n",
        "    self_sum, self_cnt   = defaultdict(float), defaultdict(int)\n",
        "    cross_sum, cross_cnt = defaultdict(float), defaultdict(int)\n",
        "\n",
        "    step = 0\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            step += 1\n",
        "            opt.zero_grad()\n",
        "            with autocast():\n",
        "                out = model(input_ids=batch[\"input_ids\"].to(device),\n",
        "                            attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                            labels=batch[\"labels\"].to(device))\n",
        "                loss = out.loss\n",
        "                scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "\n",
        "            if step % max(1, jvp_every) == 0:\n",
        "                self_scores, cross_scores = compute_decoder_attention_jdev(\n",
        "                    model, self_bufs, cross_bufs, eps=jvp_eps, k_probes=jvp_k\n",
        "                )\n",
        "                for i, v in self_scores.items():\n",
        "                    if math.isfinite(v):\n",
        "                        self_sum[i]  += v; self_cnt[i]  += 1\n",
        "                for i, v in cross_scores.items():\n",
        "                    if math.isfinite(v):\n",
        "                        cross_sum[i] += v; cross_cnt[i] += 1\n",
        "\n",
        "        epoch_self  = {i: self_sum[i]/self_cnt[i]   for i in self_sum   if self_cnt[i]   > 0}\n",
        "        epoch_cross = {i: cross_sum[i]/cross_cnt[i] for i in cross_sum  if cross_cnt[i]  > 0}\n",
        "        print(f\"[Epoch {epoch+1}] Decoder SELF-attn J-dev:  {epoch_self}\")\n",
        "        print(f\"[Epoch {epoch+1}] Decoder CROSS-attn J-dev: {epoch_cross}\")\n",
        "        acc = evaluate_model(model, dev_loader, tokenizer, device)\n",
        "        print(f\"[Epoch {epoch+1}] CQA Dev Acc: {acc:.4f}\")\n",
        "\n",
        "    remove_hooks(hooks)\n",
        "    final_self  = {i: self_sum[i]/self_cnt[i]   for i in self_sum   if self_cnt[i]   > 0}\n",
        "    final_cross = {i: cross_sum[i]/cross_cnt[i] for i in cross_sum  if cross_cnt[i]  > 0}\n",
        "    return model, final_self, final_cross\n",
        "\n",
        "def prune_attention_and_finetune(model, train_loader, dev_loader, device,\n",
        "                                 self_scores, cross_scores, tokenizer,\n",
        "                                 k_self=2, k_cross=2, epochs=5):\n",
        "    print(\"=== Stage 2: Prune LOW-Jdev Attention (self/cross) & Fine-tune ===\")\n",
        "    pruned_self, pruned_cross = prune_decoder_attention(\n",
        "        model, self_scores, cross_scores,\n",
        "        k_self=k_self, k_cross=k_cross,\n",
        "        protect_first=True, protect_last=False, verbose=True\n",
        "    )\n",
        "\n",
        "    # keep cache disabled after structural changes\n",
        "    model.config.use_cache = False\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "    sched = get_linear_schedule_with_warmup(opt, 0, len(train_loader)*2)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            opt.zero_grad()\n",
        "            out = model(input_ids=batch[\"input_ids\"].to(device),\n",
        "                        attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                        labels=batch[\"labels\"].to(device))\n",
        "            loss = out.loss\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, tokenizer, device)\n",
        "        print(f\"[Prune FT Epoch {epoch+1}] CQA Acc: {acc:.4f}\")\n",
        "    return model, pruned_self, pruned_cross\n",
        "\n",
        "# =========================\n",
        "# 6) Main\n",
        "# =========================\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Stage 1: Train + collect attention-only J-dev\n",
        "    model, self_scores, cross_scores = full_finetuning_collect_attn_jdev(\n",
        "        train_loader, dev_loader, device, tokenizer,\n",
        "        jvp_eps=1e-3, jvp_k=1, jvp_every=1, epochs=6\n",
        "    )\n",
        "\n",
        "    # Stage 2: Prune attention ONLY and fine-tune\n",
        "    # Tip: set k_cross=0 if you want to keep all cross-attention.\n",
        "    model, pruned_self, pruned_cross = prune_attention_and_finetune(\n",
        "        model, train_loader, dev_loader, device,\n",
        "        self_scores, cross_scores, tokenizer,\n",
        "        k_self=num, k_cross=0, epochs=5\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2brmCbkwauiw"
      },
      "outputs": [],
      "source": [
        "# Autograd-JVP pruning — ONLY decoder attention layers (self & cross) on CQA\n",
        "# NaN-hardened and mask/bias-safe\n",
        "\n",
        "# ====== Colab Drive (no-op off Colab) ======\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ====== Imports ======\n",
        "import inspect\n",
        "import warnings\n",
        "import math\n",
        "from collections import defaultdict\n",
        "from contextlib import contextmanager\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration, T5TokenizerFast,\n",
        "    DataCollatorForSeq2Seq, get_linear_schedule_with_warmup\n",
        ")\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# =========================\n",
        "# 0) CQA data\n",
        "# =========================\n",
        "data_files = {\n",
        "    \"train\": \"/content/drive/MyDrive/NLP_datasets/cqa/cqa_train.json\",\n",
        "    \"test\":  \"/content/drive/MyDrive/NLP_datasets/cqa/cqa_test.json\"\n",
        "}\n",
        "dataset = load_dataset(\"json\", data_files=data_files)\n",
        "\n",
        "def preprocess_cqa(batch, tokenizer, max_input_length=128, max_target_length=8, use_cot=False):\n",
        "    if use_cot and 'abstractive_explanation' in batch:\n",
        "        inputs = [\n",
        "            f\"question: {q} choices: {', '.join(choices)} rationale: {exp}\"\n",
        "            for q, choices, exp in zip(batch['question'], batch['choices'], batch['abstractive_explanation'])\n",
        "        ]\n",
        "    else:\n",
        "        inputs = [\n",
        "            f\"question: {q} choices: {', '.join(choices)}\"\n",
        "            for q, choices in zip(batch['question'], batch['choices'])\n",
        "        ]\n",
        "    targets = [str(ans).strip() for ans in batch['answer']]\n",
        "    model_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=max_input_length)\n",
        "    target = tokenizer(targets, padding=\"max_length\", truncation=True, max_length=max_target_length)\n",
        "    model_inputs[\"labels\"] = target[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
        "USE_COT = False\n",
        "\n",
        "train = dataset[\"train\"].map(lambda ex: preprocess_cqa(ex, tokenizer, use_cot=USE_COT),\n",
        "                             batched=True, remove_columns=dataset[\"train\"].column_names)\n",
        "dev   = dataset[\"test\"].map(lambda ex: preprocess_cqa(ex, tokenizer, use_cot=False),\n",
        "                             batched=True, remove_columns=dataset[\"test\"].column_names)\n",
        "\n",
        "# Use label_pad_token_id=-100 so CE ignores padding\n",
        "collator = DataCollatorForSeq2Seq(tokenizer, model=None, padding=\"max_length\", max_length=128, label_pad_token_id=-100)\n",
        "train_loader = DataLoader(train, batch_size=32, shuffle=True,  collate_fn=collator)\n",
        "dev_loader   = DataLoader(dev,   batch_size=32, shuffle=False, collate_fn=collator)\n",
        "\n",
        "# =========================\n",
        "# 1) Eval helpers\n",
        "# =========================\n",
        "def compute_accuracy(preds, refs):\n",
        "    correct = 0\n",
        "    for p, l in zip(preds, refs):\n",
        "        if p == l:\n",
        "            correct += 1\n",
        "    return correct / len(preds) if len(preds) > 0 else 0.0\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, dl, tokenizer, device):\n",
        "    # disable cache to avoid KV/pos-bias drift after pruning\n",
        "    model.config.use_cache = False\n",
        "    model.eval()\n",
        "    preds, refs = [], []\n",
        "    for batch in dl:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=4,\n",
        "            use_cache=False,          # critical after pruning\n",
        "        )\n",
        "        pred_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        label_ids = batch[\"labels\"].clone()\n",
        "        label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "        ref_texts = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "        preds.extend([p.strip().lower() for p in pred_texts])\n",
        "        refs.extend([l.strip().lower() for l in ref_texts])\n",
        "    return compute_accuracy(preds, refs)\n",
        "\n",
        "# =========================\n",
        "# 2) Autograd-JVP helpers (robust) + mask/bias utilities\n",
        "# =========================\n",
        "def _filter_kwargs_for_module(module, kwargs):\n",
        "    sig = inspect.signature(module.forward)\n",
        "    allowed = set(sig.parameters.keys()) - {\"self\"}\n",
        "    return {k: v for k, v in kwargs.items() if k in allowed}\n",
        "\n",
        "def _map_args_from_hook(module, args, kwargs):\n",
        "    if kwargs and len(kwargs) > 0:\n",
        "        d = dict(kwargs)\n",
        "    else:\n",
        "        sig = inspect.signature(module.forward)\n",
        "        names = [n for n in sig.parameters.keys() if n != \"self\"]\n",
        "        d = {}\n",
        "        for i, name in enumerate(names):\n",
        "            if i < len(args):\n",
        "                d[name] = args[i]\n",
        "            else:\n",
        "                d[name] = sig.parameters[name].default\n",
        "    # we will set hidden_states; never reuse cached biases\n",
        "    d.pop(\"hidden_states\", None)\n",
        "    d.pop(\"position_bias\", None)\n",
        "    d.pop(\"encoder_decoder_position_bias\", None)\n",
        "    return d\n",
        "\n",
        "def _build_causal_mask(B, q_len, k_len, device, dtype):\n",
        "    # stable large negative (not -inf) to avoid NaNs in softmax\n",
        "    mask = torch.zeros((B, 1, q_len, k_len), dtype=dtype, device=device)\n",
        "    tri = torch.triu(torch.ones((q_len, k_len), dtype=torch.bool, device=device), diagonal=1)\n",
        "    mask[:, :, tri] = -1e9\n",
        "    return mask\n",
        "\n",
        "def _ensure_mask_shape(mask, q_len, k_len, device, dtype, is_self):\n",
        "    \"\"\"\n",
        "    Return a (B,1,q_len,k_len) mask; if input mask is missing or wrong-sized,\n",
        "    rebuild a safe default (causal for self, zeros for cross).\n",
        "    \"\"\"\n",
        "    if mask is None:\n",
        "        B = 1\n",
        "        return _build_causal_mask(B, q_len, k_len, device, dtype) if is_self \\\n",
        "               else torch.zeros((B,1,q_len,k_len), dtype=dtype, device=device)\n",
        "    if mask.dim() == 2:  # (B, K)\n",
        "        B = mask.size(0)\n",
        "        return mask[:, None, None, :k_len].expand(B,1,q_len,k_len).contiguous().to(device=device, dtype=dtype)\n",
        "    if mask.dim() == 4:\n",
        "        B, _, Q, K = mask.shape\n",
        "        if Q >= q_len and K >= k_len:\n",
        "            return mask[:, :, :q_len, :k_len].to(device=device, dtype=dtype)\n",
        "        return _build_causal_mask(B, q_len, k_len, device, dtype) if is_self \\\n",
        "               else torch.zeros((B,1,q_len,k_len), dtype=dtype, device=device)\n",
        "    B = 1\n",
        "    return _build_causal_mask(B, q_len, k_len, device, dtype) if is_self \\\n",
        "           else torch.zeros((B,1,q_len,k_len), dtype=dtype, device=device)\n",
        "\n",
        "@contextmanager\n",
        "def force_math_sdp():\n",
        "    \"\"\"\n",
        "    Force 'math' scaled-dot-product attention kernels so higher-order grads exist.\n",
        "    Falls back to a no-op context if backend is unavailable (e.g., CPU).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with torch.backends.cuda.sdp_kernel(enable_flash=False,\n",
        "                                            enable_mem_efficient=False,\n",
        "                                            enable_math=True):\n",
        "            yield\n",
        "    except Exception:\n",
        "        yield\n",
        "\n",
        "def _make_attn_callable(module, argdict):\n",
        "    \"\"\"\n",
        "    Build pure fn f(x)->hs for a single T5 sublayer (self/cross).\n",
        "    Rebuild masks and never reuse captured position biases.\n",
        "    \"\"\"\n",
        "    raw = dict(argdict) if argdict is not None else {}\n",
        "\n",
        "    sig = inspect.signature(module.forward)\n",
        "    params = set(sig.parameters.keys())\n",
        "    is_cross = \"key_value_states\" in params\n",
        "\n",
        "    const_kwargs = {}\n",
        "\n",
        "    # KV states for cross attention\n",
        "    if is_cross:\n",
        "        kv = raw.get(\"key_value_states\", raw.get(\"encoder_hidden_states\", None))\n",
        "        const_kwargs[\"key_value_states\"] = None if kv is None else kv.float()\n",
        "\n",
        "    # deterministic flags\n",
        "    if \"use_cache\" in params:\n",
        "        const_kwargs[\"use_cache\"] = False\n",
        "    if \"output_attentions\" in params:\n",
        "        const_kwargs[\"output_attentions\"] = False\n",
        "    if \"return_dict\" in params:\n",
        "        const_kwargs[\"return_dict\"] = False\n",
        "\n",
        "    # mask source (we'll reshape at call time)\n",
        "    mask_key = \"attention_mask\" if \"attention_mask\" in params else (\"mask\" if \"mask\" in params else None)\n",
        "    captured_mask = raw.get(mask_key, raw.get(\"mask\", None)) if mask_key is not None else None\n",
        "    captured_kv = const_kwargs.get(\"key_value_states\", None)\n",
        "\n",
        "    def _f(x):\n",
        "        x = x.float()\n",
        "        q_len = x.size(1)\n",
        "        k_len = captured_kv.size(1) if (is_cross and captured_kv is not None) else q_len\n",
        "        B     = x.size(0)\n",
        "\n",
        "        kwargs = {\"hidden_states\": x}\n",
        "        if is_cross:\n",
        "            if captured_kv is None:\n",
        "                return x  # identity if no encoder states present\n",
        "            kwargs[\"key_value_states\"] = captured_kv\n",
        "\n",
        "        if mask_key is not None:\n",
        "            kwargs[mask_key] = _ensure_mask_shape(\n",
        "                captured_mask, q_len, k_len, x.device, x.dtype, is_self=(not is_cross)\n",
        "            )\n",
        "\n",
        "        # cache_position / query_length\n",
        "        if \"cache_position\" in params:\n",
        "            kwargs[\"cache_position\"] = torch.arange(q_len, dtype=torch.long, device=x.device)\n",
        "        if \"query_length\" in params and \"cache_position\" not in params:\n",
        "            kwargs[\"query_length\"] = q_len\n",
        "\n",
        "        # do NOT pass any position_bias\n",
        "        kwargs.update(const_kwargs)\n",
        "        kwargs = _filter_kwargs_for_module(module, kwargs)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=False):\n",
        "            out = module(**kwargs)\n",
        "\n",
        "        hs = out[0] if isinstance(out, (tuple, list)) else out\n",
        "        return hs\n",
        "\n",
        "    return _f\n",
        "\n",
        "def compute_decoder_attention_jdev_autograd(model, self_bufs, cross_bufs, k_probes=1, rademacher=True):\n",
        "    \"\"\"\n",
        "    For each decoder attention sublayer ℓ, estimate:\n",
        "      E_v ||(Jℓ - I)v||^2 using autograd JVP.\n",
        "    Non-finite probes are skipped. Buffers are cleared.\n",
        "    \"\"\"\n",
        "    from torch.autograd.functional import jvp as autograd_jvp\n",
        "\n",
        "    self_scores, cross_scores = {}, {}\n",
        "\n",
        "    # Self-attention J-dev\n",
        "    for idx, buf in self_bufs.items():\n",
        "        X, args = buf[\"X\"], buf[\"args\"]\n",
        "        buf[\"X\"], buf[\"args\"] = None, None  # clear ASAP\n",
        "        if X is None:\n",
        "            continue\n",
        "        mod = model.decoder.block[idx].layer[0]  # T5LayerSelfAttention\n",
        "\n",
        "        was_training = mod.training\n",
        "        mod.eval()\n",
        "        try:\n",
        "            fn = _make_attn_callable(mod, args)\n",
        "            x0 = X.detach().requires_grad_(True).float()\n",
        "\n",
        "            # Sanity forward\n",
        "            try:\n",
        "                with force_math_sdp():\n",
        "                    _ = fn(x0)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            acc, used = 0.0, 0\n",
        "            with force_math_sdp():\n",
        "                for _ in range(max(1, k_probes)):\n",
        "                    if rademacher:\n",
        "                        v = torch.empty_like(x0).bernoulli_(0.5).mul_(2).sub_(1)\n",
        "                    else:\n",
        "                        v = torch.randn_like(x0)\n",
        "                    try:\n",
        "                        _, Jv = autograd_jvp(fn, (x0,), (v,), create_graph=False, strict=True)\n",
        "                    except Exception:\n",
        "                        continue\n",
        "                    if Jv is None or not torch.isfinite(Jv).all():\n",
        "                        continue\n",
        "                    jd_vec = Jv - v\n",
        "                    if not torch.isfinite(jd_vec).all():\n",
        "                        continue\n",
        "                    acc += float(jd_vec.pow(2).mean().item())\n",
        "                    used += 1\n",
        "            if used > 0:\n",
        "                self_scores[idx] = acc / used\n",
        "        finally:\n",
        "            mod.train(was_training)\n",
        "\n",
        "    # Cross-attention J-dev\n",
        "    for idx, buf in cross_bufs.items():\n",
        "        X, args = buf[\"X\"], buf[\"args\"]\n",
        "        buf[\"X\"], buf[\"args\"] = None, None\n",
        "        if X is None:\n",
        "            continue\n",
        "        if len(model.decoder.block[idx].layer) <= 1 or model.decoder.block[idx].layer[1] is None:\n",
        "            continue\n",
        "        mod = model.decoder.block[idx].layer[1]  # T5LayerCrossAttention\n",
        "\n",
        "        was_training = mod.training\n",
        "        mod.eval()\n",
        "        try:\n",
        "            fn = _make_attn_callable(mod, args)\n",
        "            x0 = X.detach().requires_grad_(True).float()\n",
        "\n",
        "            try:\n",
        "                with force_math_sdp():\n",
        "                    _ = fn(x0)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            acc, used = 0.0, 0\n",
        "            with force_math_sdp():\n",
        "                for _ in range(max(1, k_probes)):\n",
        "                    if rademacher:\n",
        "                        v = torch.empty_like(x0).bernoulli_(0.5).mul_(2).sub_(1)\n",
        "                    else:\n",
        "                        v = torch.randn_like(x0)\n",
        "                    try:\n",
        "                        _, Jv = autograd_jvp(fn, (x0,), (v,), create_graph=False, strict=True)\n",
        "                    except Exception:\n",
        "                        continue\n",
        "                    if Jv is None or not torch.isfinite(Jv).all():\n",
        "                        continue\n",
        "                    jd_vec = Jv - v\n",
        "                    if not torch.isfinite(jd_vec).all():\n",
        "                        continue\n",
        "                    acc += float(jd_vec.pow(2).mean().item())\n",
        "                    used += 1\n",
        "            if used > 0:\n",
        "                cross_scores[idx] = acc / used\n",
        "        finally:\n",
        "            mod.train(was_training)\n",
        "\n",
        "    return self_scores, cross_scores\n",
        "\n",
        "# =========================\n",
        "# 3) Hooks to capture ONLY decoder attention inputs\n",
        "# =========================\n",
        "def register_decoder_attention_hooks(model):\n",
        "    dec_blocks = model.decoder.block\n",
        "    self_bufs  = {i: {\"X\": None, \"args\": None} for i in range(len(dec_blocks))}\n",
        "    cross_bufs = {i: {\"X\": None, \"args\": None} for i in range(len(dec_blocks))}\n",
        "    hooks = []\n",
        "\n",
        "    for i, block in enumerate(dec_blocks):\n",
        "        sa = block.layer[0]\n",
        "        def pre_hook_sa(module, args, kwargs, idx=i):\n",
        "            X = kwargs.get(\"hidden_states\", args[0] if len(args) > 0 else None)\n",
        "            self_bufs[idx][\"X\"]    = None if X is None else X.detach()\n",
        "            self_bufs[idx][\"args\"] = _map_args_from_hook(module, args, kwargs)\n",
        "        hooks.append(sa.register_forward_pre_hook(pre_hook_sa, with_kwargs=True))\n",
        "\n",
        "    for i, block in enumerate(dec_blocks):\n",
        "        if len(block.layer) > 1 and block.layer[1] is not None:\n",
        "            ca = block.layer[1]\n",
        "            def pre_hook_ca(module, args, kwargs, idx=i):\n",
        "                X = kwargs.get(\"hidden_states\", args[0] if len(args) > 0 else None)\n",
        "                cross_bufs[idx][\"X\"]    = None if X is None else X.detach()\n",
        "                cross_bufs[idx][\"args\"] = _map_args_from_hook(module, args, kwargs)\n",
        "            hooks.append(ca.register_forward_pre_hook(pre_hook_ca, with_kwargs=True))\n",
        "\n",
        "    return hooks, self_bufs, cross_bufs\n",
        "\n",
        "def remove_hooks(hooks):\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "# =========================\n",
        "# 4) Attention-only pruning utilities\n",
        "# =========================\n",
        "class SkipSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Identity replacement for T5LayerSelfAttention.\n",
        "    Return (hidden_states, None, None) so no stale position_bias is reused.\n",
        "    \"\"\"\n",
        "    def forward(self, hidden_states, *args, **kwargs):\n",
        "        return (hidden_states, None, None)\n",
        "\n",
        "class SkipCrossAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Identity replacement for T5LayerCrossAttention.\n",
        "    Also return None for position_bias to force recomputation downstream.\n",
        "    \"\"\"\n",
        "    def forward(self, hidden_states, *args, **kwargs):\n",
        "        return (hidden_states, None, None)\n",
        "\n",
        "def prune_decoder_attention(model, self_scores, cross_scores,\n",
        "                            k_self=2, k_cross=2,\n",
        "                            protect_first=True, protect_last=False, verbose=True):\n",
        "    num_layers = len(model.decoder.block)\n",
        "    self_items  = list(self_scores.items())\n",
        "    cross_items = list(cross_scores.items())\n",
        "\n",
        "    if protect_first:\n",
        "        self_items  = [(i,s) for i,s in self_items  if i != 0]\n",
        "        cross_items = [(i,s) for i,s in cross_items if i != 0]\n",
        "    if protect_last:\n",
        "        self_items  = [(i,s) for i,s in self_items  if i != num_layers-1]\n",
        "        cross_items = [(i,s) for i,s in cross_items if i != num_layers-1]\n",
        "\n",
        "    self_items.sort(key=lambda x: x[1])   # lowest J-dev first\n",
        "    cross_items.sort(key=lambda x: x[1])\n",
        "\n",
        "    pruned_self, pruned_cross = [], []\n",
        "\n",
        "    for i, _ in self_items[:max(0, k_self)]:\n",
        "        model.decoder.block[i].layer[0] = SkipSelfAttention()\n",
        "        pruned_self.append(i)\n",
        "\n",
        "    for i, _ in cross_items[:max(0, k_cross)]:\n",
        "        if len(model.decoder.block[i].layer) > 1 and model.decoder.block[i].layer[1] is not None:\n",
        "            model.decoder.block[i].layer[1] = SkipCrossAttention()\n",
        "            pruned_cross.append(i)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Pruned decoder SELF-attention at layers:  {pruned_self}\")\n",
        "        print(f\"Pruned decoder CROSS-attention at layers: {pruned_cross}\")\n",
        "\n",
        "    return pruned_self, pruned_cross\n",
        "\n",
        "# =========================\n",
        "# 5) Train → collect Autograd-JVP → prune attention → FT\n",
        "# =========================\n",
        "def full_finetuning_collect_attn_jdev(train_loader, dev_loader, device, tokenizer,\n",
        "                                      jvp_k=1, jvp_every=1, epochs=6):\n",
        "    print(\"=== Stage 1: Full FT & Autograd-JVP (Decoder Attention only) ===\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n",
        "    model.config.use_cache = False  # keep cache off while we collect JVP\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "    scaler = GradScaler()\n",
        "    sched = get_linear_schedule_with_warmup(opt, 0, len(train_loader)*3)\n",
        "\n",
        "    hooks, self_bufs, cross_bufs = register_decoder_attention_hooks(model)\n",
        "    self_sum, self_cnt   = defaultdict(float), defaultdict(int)\n",
        "    cross_sum, cross_cnt = defaultdict(float), defaultdict(int)\n",
        "\n",
        "    step = 0\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            step += 1\n",
        "            opt.zero_grad()\n",
        "            with autocast():\n",
        "                out = model(input_ids=batch[\"input_ids\"].to(device),\n",
        "                            attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                            labels=batch[\"labels\"].to(device))\n",
        "                loss = out.loss\n",
        "                scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "\n",
        "            if step % max(1, jvp_every) == 0:\n",
        "                self_scores, cross_scores = compute_decoder_attention_jdev_autograd(\n",
        "                    model, self_bufs, cross_bufs, k_probes=jvp_k\n",
        "                )\n",
        "                for i, v in self_scores.items():\n",
        "                    if math.isfinite(v):\n",
        "                        self_sum[i]  += v; self_cnt[i]  += 1\n",
        "                for i, v in cross_scores.items():\n",
        "                    if math.isfinite(v):\n",
        "                        cross_sum[i] += v; cross_cnt[i] += 1\n",
        "\n",
        "        epoch_self  = {i: self_sum[i]/self_cnt[i]   for i in self_sum   if self_cnt[i]   > 0}\n",
        "        epoch_cross = {i: cross_sum[i]/cross_cnt[i] for i in cross_sum  if cross_cnt[i]  > 0}\n",
        "        print(f\"[Epoch {epoch+1}] Decoder SELF-attn J-dev:  {epoch_self}\")\n",
        "        print(f\"[Epoch {epoch+1}] Decoder CROSS-attn J-dev: {epoch_cross}\")\n",
        "        acc = evaluate_model(model, dev_loader, tokenizer, device)\n",
        "        print(f\"[Epoch {epoch+1}] CQA Dev Acc: {acc:.4f}\")\n",
        "\n",
        "    remove_hooks(hooks)\n",
        "    final_self  = {i: self_sum[i]/self_cnt[i]   for i in self_sum   if self_cnt[i]   > 0}\n",
        "    final_cross = {i: cross_sum[i]/cross_cnt[i] for i in cross_sum  if cross_cnt[i]  > 0}\n",
        "    return model, final_self, final_cross\n",
        "\n",
        "def prune_attention_and_finetune(model, train_loader, dev_loader, device,\n",
        "                                 self_scores, cross_scores, tokenizer,\n",
        "                                 k_self=2, k_cross=2, epochs=5):\n",
        "    print(\"=== Stage 2: Prune LOW-Jdev Attention (self/cross) & Fine-tune ===\")\n",
        "    pruned_self, pruned_cross = prune_decoder_attention(\n",
        "        model, self_scores, cross_scores,\n",
        "        k_self=k_self, k_cross=k_cross,\n",
        "        protect_first=True, protect_last=False, verbose=True\n",
        "    )\n",
        "\n",
        "    # keep cache disabled after structural changes\n",
        "    model.config.use_cache = False\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "    sched = get_linear_schedule_with_warmup(opt, 0, len(train_loader)*2)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            opt.zero_grad()\n",
        "            out = model(input_ids=batch[\"input_ids\"].to(device),\n",
        "                        attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                        labels=batch[\"labels\"].to(device))\n",
        "            loss = out.loss\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, tokenizer, device)\n",
        "        print(f\"[Prune FT Epoch {epoch+1}] CQA Acc: {acc:.4f}\")\n",
        "    return model, pruned_self, pruned_cross\n",
        "\n",
        "# =========================\n",
        "# 6) Main\n",
        "# =========================\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Stage 1: Train + collect attention-only J-dev (Autograd JVP)\n",
        "    model, self_scores, cross_scores = full_finetuning_collect_attn_jdev(\n",
        "        train_loader, dev_loader, device, tokenizer,\n",
        "        jvp_k=1, jvp_every=1, epochs=6\n",
        "    )\n",
        "\n",
        "    # Stage 2: Prune attention ONLY and fine-tune\n",
        "    # Tip: set k_cross=0 if you want to keep all cross-attention.\n",
        "    model, pruned_self, pruned_cross = prune_attention_and_finetune(\n",
        "        model, train_loader, dev_loader, device,\n",
        "        self_scores, cross_scores, tokenizer,\n",
        "        k_self=num, k_cross=0, epochs=5\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKfC_ACtkh-h"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iOHlkluavHz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n32l-LfB9UD6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpqHkWIcENvu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSq5sd24EOIs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deThNrqN7ZfY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyzRczTHJXF2"
      },
      "outputs": [],
      "source": [
        "# Only prune decoder — FD-JVP (NaN-safe, bias-safe)\n",
        "\n",
        "# --- Mount Google Drive if using Colab ---\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# --- Standard Imports ---\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration, T5TokenizerFast,\n",
        "    DataCollatorForSeq2Seq, Adafactor\n",
        ")\n",
        "from torch.cuda.amp import autocast\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import inspect\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# ------------- Repro -------------\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(1234)\n",
        "\n",
        "# --- 1. Load ANLI1 Dataset ---\n",
        "data_files = {\n",
        "    \"train\":      \"/content/drive/MyDrive/NLP_datasets/anli1/anli1_train.json\",\n",
        "    \"validation\": \"/content/drive/MyDrive/NLP_datasets/anli1/anli1_valid.json\",\n",
        "    \"test\":       \"/content/drive/MyDrive/NLP_datasets/anli1/anli1_test.json\"\n",
        "}\n",
        "dataset = load_dataset(\"json\", data_files=data_files)\n",
        "\n",
        "# --- 2. Preprocessing Function ---\n",
        "def make_t5_nli_prompt(premise, hypothesis):\n",
        "    return f\"nli premise: {premise} hypothesis: {hypothesis}\"\n",
        "\n",
        "def preprocess_anli(batch, tokenizer, max_input_length=128, max_target_length=8):\n",
        "    inputs = [make_t5_nli_prompt(p, h) for p, h in zip(batch['premise'], batch['hypothesis'])]\n",
        "    label_list = [\"entailment\", \"neutral\", \"contradiction\"]\n",
        "\n",
        "    labels_str = []\n",
        "    for x in batch['label']:\n",
        "        sx = str(x).strip().lower()\n",
        "        if sx.isdigit() and int(sx) < 3:\n",
        "            labels_str.append(label_list[int(sx)])\n",
        "        else:\n",
        "            labels_str.append(sx)\n",
        "\n",
        "    model_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=max_input_length)\n",
        "    target = tokenizer(text_target=labels_str, padding=\"max_length\", truncation=True, max_length=max_target_length)\n",
        "    model_inputs[\"labels\"] = target[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
        "\n",
        "# Map datasets\n",
        "train = dataset[\"train\"].map(\n",
        "    lambda ex: preprocess_anli(ex, tokenizer),\n",
        "    batched=True, remove_columns=dataset[\"train\"].column_names\n",
        ")\n",
        "dev = dataset[\"validation\"].map(\n",
        "    lambda ex: preprocess_anli(ex, tokenizer),\n",
        "    batched=True, remove_columns=dataset[\"validation\"].column_names\n",
        ")\n",
        "\n",
        "# --- Load model before creating the collator (so collator can mask label pads -> -100) ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n",
        "# Avoid kv-cache so biases are always recomputed (also matches SkipBlock behavior)\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Collator that converts pad tokens in labels to -100\n",
        "collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=-100)\n",
        "train_loader = DataLoader(train, batch_size=16, shuffle=True, collate_fn=collator)\n",
        "dev_loader   = DataLoader(dev,   batch_size=16, shuffle=False, collate_fn=collator)\n",
        "\n",
        "# =========================\n",
        "# 3) FD-JVP helpers (NaN-safe) — capture + deterministic replay for T5Block\n",
        "# =========================\n",
        "\n",
        "def _filter_kwargs_for_module(module, kwargs):\n",
        "    sig = inspect.signature(module.forward)\n",
        "    allowed = set(sig.parameters.keys()) - {\"self\"}\n",
        "    return {k: v for k, v in kwargs.items() if k in allowed}\n",
        "\n",
        "def _map_args_from_hook(module, args, kwargs):\n",
        "    \"\"\"Normalize to kwargs for re-calling module.forward.\"\"\"\n",
        "    if kwargs and len(kwargs) > 0:\n",
        "        d = dict(kwargs)\n",
        "    else:\n",
        "        sig = inspect.signature(module.forward)\n",
        "        names = [n for n in sig.parameters.keys() if n != \"self\"]\n",
        "        d = {}\n",
        "        for i, name in enumerate(names):\n",
        "            d[name] = args[i] if i < len(args) else sig.parameters[name].default\n",
        "    d.pop(\"hidden_states\", None)  # we will set it explicitly\n",
        "    return d\n",
        "\n",
        "def _build_causal_mask(B, q_len, k_len, device, dtype):\n",
        "    # stable large negative (not -inf) to avoid NaNs in softmax\n",
        "    mask = torch.zeros((B, 1, q_len, k_len), dtype=dtype, device=device)\n",
        "    tri = torch.triu(torch.ones((q_len, k_len), dtype=torch.bool, device=device), diagonal=1)\n",
        "    mask[:, :, tri] = -1e9\n",
        "    return mask\n",
        "\n",
        "def _build_zero_mask(B, q_len, k_len, device, dtype):\n",
        "    return torch.zeros((B, 1, q_len, k_len), dtype=dtype, device=device)\n",
        "\n",
        "def _ensure_mask_shape(mask, q_len, k_len, device, dtype, is_self, B_fallback=1):\n",
        "    \"\"\"\n",
        "    Produce a valid (B,1,q_len,k_len) mask; rebuild default if missing/invalid.\n",
        "    Self-attn -> causal mask; Cross-attn -> zero mask.\n",
        "    \"\"\"\n",
        "    default_fn = _build_causal_mask if is_self else _build_zero_mask\n",
        "\n",
        "    if mask is None:\n",
        "        return default_fn(B_fallback, q_len, k_len, device, dtype)\n",
        "\n",
        "    if mask.dim() == 2:  # (B, K)\n",
        "        B = mask.size(0)\n",
        "        return mask[:, None, None, :k_len].expand(B, 1, q_len, k_len).contiguous().to(device=device, dtype=dtype)\n",
        "\n",
        "    if mask.dim() == 4:\n",
        "        B, _, Q, K = mask.shape\n",
        "        if Q >= q_len and K >= k_len:\n",
        "            return mask[:, :, :q_len, :k_len].to(device=device, dtype=dtype)\n",
        "        return default_fn(B, q_len, k_len, device, dtype)\n",
        "\n",
        "    return default_fn(B_fallback, q_len, k_len, device, dtype)\n",
        "\n",
        "@torch.no_grad()\n",
        "def _t5block_forward_only(block, X, argdict):\n",
        "    \"\"\"\n",
        "    Deterministic single T5Block forward in fp32 (autocast disabled).\n",
        "    Provides safe masks & cache_position/query_length when missing.\n",
        "    Returns only hidden_states tensor.\n",
        "    \"\"\"\n",
        "    if X is None:\n",
        "        return None\n",
        "    was_training = block.training\n",
        "    block.eval()\n",
        "    Xf = X.float()\n",
        "    try:\n",
        "        raw = dict(argdict) if argdict is not None else {}\n",
        "\n",
        "        # Never reuse captured biases (length-mismatch risk)\n",
        "        raw.pop(\"position_bias\", None)\n",
        "        raw.pop(\"encoder_decoder_position_bias\", None)\n",
        "\n",
        "        kwargs = {\"hidden_states\": Xf}\n",
        "\n",
        "        q_len = Xf.size(1)\n",
        "        B = Xf.size(0)\n",
        "\n",
        "        # Self-attention mask (called \"attention_mask\" at block level)\n",
        "        attn_mask = raw.get(\"attention_mask\", raw.get(\"mask\", None))\n",
        "        kwargs[\"attention_mask\"] = _ensure_mask_shape(\n",
        "            attn_mask, q_len, q_len, Xf.device, Xf.dtype, is_self=True, B_fallback=B\n",
        "        )\n",
        "\n",
        "        # Cross-attention (if encoder states available)\n",
        "        enc_states = raw.get(\"encoder_hidden_states\", None)\n",
        "        if enc_states is not None:\n",
        "            enc_states = enc_states.float()\n",
        "            kwargs[\"encoder_hidden_states\"] = enc_states\n",
        "            k_len = enc_states.size(1)\n",
        "            enc_mask = raw.get(\"encoder_attention_mask\", None)\n",
        "            kwargs[\"encoder_attention_mask\"] = _ensure_mask_shape(\n",
        "                enc_mask, q_len, k_len, Xf.device, Xf.dtype, is_self=False, B_fallback=B\n",
        "            )\n",
        "\n",
        "        # cache_position / query_length if present in this HF version\n",
        "        params = set(inspect.signature(block.forward).parameters.keys())\n",
        "        if \"cache_position\" in params:\n",
        "            cp = raw.get(\"cache_position\", None)\n",
        "            if cp is None:\n",
        "                cp = torch.arange(q_len, dtype=torch.long, device=Xf.device)\n",
        "            kwargs[\"cache_position\"] = cp\n",
        "        if \"query_length\" in params:\n",
        "            kwargs[\"query_length\"] = q_len\n",
        "\n",
        "        # Deterministic flags\n",
        "        if \"use_cache\" in params:\n",
        "            kwargs[\"use_cache\"] = False\n",
        "        if \"output_attentions\" in params:\n",
        "            kwargs[\"output_attentions\"] = False\n",
        "        if \"return_dict\" in params:\n",
        "            kwargs[\"return_dict\"] = False\n",
        "\n",
        "        kwargs = _filter_kwargs_for_module(block, kwargs)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=False):\n",
        "            out = block(**kwargs)\n",
        "\n",
        "        hs = out[0] if isinstance(out, (tuple, list)) else out\n",
        "        return hs if hs is None or torch.isfinite(hs).all() else None\n",
        "    finally:\n",
        "        block.train(was_training)\n",
        "\n",
        "def register_decoder_block_jvp_hooks(model):\n",
        "    \"\"\"\n",
        "    Capture per-decoder-block incoming hidden_states + kwargs with forward_pre_hook.\n",
        "    \"\"\"\n",
        "    dec_blocks = model.decoder.block\n",
        "    dec_bufs = {i: {\"X\": None, \"args\": None} for i in range(len(dec_blocks))}\n",
        "    dec_hooks = []\n",
        "    for i, block in enumerate(dec_blocks):\n",
        "        def pre_hook(module, args, kwargs, idx=i):\n",
        "            X = kwargs.get(\"hidden_states\", args[0] if len(args) > 0 else None)\n",
        "            dec_bufs[idx][\"X\"]    = None if X is None else X.detach()\n",
        "            dec_bufs[idx][\"args\"] = _map_args_from_hook(module, args, kwargs)\n",
        "        dec_hooks.append(block.register_forward_pre_hook(pre_hook, with_kwargs=True))\n",
        "    return dec_hooks, dec_bufs\n",
        "\n",
        "def remove_hooks(hooks):\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_decoder_block_jdev(model, dec_bufs, eps=1e-3, k_probes=1):\n",
        "    \"\"\"\n",
        "    For each decoder block ℓ, estimate E_v ||(Jℓ - I)v||^2 via finite differences:\n",
        "      (J - I)v ≈ (f(x + eps v) - f(x))/eps - v\n",
        "    NaN-safe: skips any non-finite forward/probe results. Clears buffers.\n",
        "    \"\"\"\n",
        "    scores = {}\n",
        "    for idx, buf in dec_bufs.items():\n",
        "        X, args = buf[\"X\"], buf[\"args\"]\n",
        "        if X is None:\n",
        "            continue\n",
        "        block = model.decoder.block[idx]\n",
        "        X = X.float()\n",
        "\n",
        "        y0 = _t5block_forward_only(block, X, args)\n",
        "        if y0 is None:\n",
        "            buf[\"X\"], buf[\"args\"] = None, None\n",
        "            continue\n",
        "\n",
        "        acc, used = 0.0, 0\n",
        "        for _ in range(max(1, k_probes)):\n",
        "            v = torch.randn_like(X)\n",
        "            y_eps = _t5block_forward_only(block, X + eps * v, args)\n",
        "            if y_eps is None:\n",
        "                continue\n",
        "            jd_vec = (y_eps - y0) / eps - v\n",
        "            if not torch.isfinite(jd_vec).all():\n",
        "                continue\n",
        "            acc += float(jd_vec.pow(2).mean().item())\n",
        "            used += 1\n",
        "\n",
        "        if used > 0:\n",
        "            scores[idx] = acc / used\n",
        "\n",
        "        buf[\"X\"], buf[\"args\"] = None, None\n",
        "    return scores\n",
        "\n",
        "# =========================\n",
        "# 4) Pruning Utilities (decoder blocks)\n",
        "# =========================\n",
        "\n",
        "class SkipBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Identity replacement for a T5 decoder block.\n",
        "    IMPORTANT: return None for biases so later layers recompute them\n",
        "               with the correct (Q,K) sizes.\n",
        "    Keeps tuple shape:\n",
        "    (hidden_states, present_key_value, self_attn_weights, cross_attn_weights,\n",
        "     position_bias, encoder_decoder_position_bias)\n",
        "    \"\"\"\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        position_bias=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        encoder_decoder_position_bias=None,\n",
        "        layer_head_mask=None,\n",
        "        cross_attn_layer_head_mask=None,\n",
        "        past_key_value=None,\n",
        "        use_cache=False,\n",
        "        output_attentions=False,\n",
        "        return_dict=False,\n",
        "        cache_position=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        return (hidden_states, None, None, None, None, None)\n",
        "\n",
        "def prune_decoder_blocks_by_jdev(blocks, jdev_scores, num_prune=4, protect_first=True, protect_last=False, verbose=True):\n",
        "    \"\"\"\n",
        "    Prune LOW J-dev (near-identity) decoder blocks.\n",
        "    \"\"\"\n",
        "    if not jdev_scores:\n",
        "        if verbose:\n",
        "            print(\"No J-dev scores available; skipping pruning.\")\n",
        "        return []\n",
        "    items = list(jdev_scores.items())\n",
        "    if protect_first:\n",
        "        items = [(i, s) for i, s in items if i != 0]\n",
        "    if protect_last:\n",
        "        items = [(i, s) for i, s in items if i != len(blocks) - 1]\n",
        "    items.sort(key=lambda x: x[1])  # lowest first\n",
        "\n",
        "    prune_idxs = [i for i, _ in items[:max(0, num_prune)]]\n",
        "    for i in prune_idxs:\n",
        "        blocks[i] = SkipBlock()\n",
        "    if verbose:\n",
        "        print(f\"Pruned decoder blocks (lowest J-dev): {prune_idxs}\")\n",
        "    return prune_idxs\n",
        "\n",
        "# =========================\n",
        "# 5) Training / Eval\n",
        "# =========================\n",
        "\n",
        "CANON = {\n",
        "    \"entailment\": \"entailment\",\n",
        "    \"entailed\": \"entailment\",\n",
        "    \"neutral\": \"neutral\",\n",
        "    \"contradiction\": \"contradiction\",\n",
        "    \"contradict\": \"contradiction\",\n",
        "    \"contradictory\": \"contradiction\",\n",
        "    \"contradicted\": \"contradiction\",\n",
        "}\n",
        "\n",
        "def canonicalize_label(s: str):\n",
        "    s = (s or \"\").strip().lower()\n",
        "    first = s.split()[0] if s else s\n",
        "    return CANON.get(first, first)\n",
        "\n",
        "def compute_accuracy(preds, refs):\n",
        "    correct = 0\n",
        "    for p, l in zip(preds, refs):\n",
        "        if canonicalize_label(p) == canonicalize_label(l):\n",
        "            correct += 1\n",
        "    return correct / len(preds) if len(preds) > 0 else 0.0\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, dl, tokenizer, device):\n",
        "    model.eval()\n",
        "    preds, refs = [], []\n",
        "    for batch in dl:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        # decode gold labels to text\n",
        "        label_ids = batch[\"labels\"].clone()\n",
        "        label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "        ref_texts = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "        # generate predictions; keep cache off for consistency (biases recompute each step)\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=4,\n",
        "            use_cache=False,\n",
        "        )\n",
        "        pred_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "        preds.extend([p.strip().lower() for p in pred_texts])\n",
        "        refs.extend([l.strip().lower() for l in ref_texts])\n",
        "\n",
        "    return compute_accuracy(preds, refs)\n",
        "\n",
        "def build_optimizer(model):\n",
        "    # Stable default for T5\n",
        "    return Adafactor(model.parameters(), relative_step=True, scale_parameter=True, warmup_init=True)\n",
        "\n",
        "def full_finetuning_with_jdev(model, train_loader, dev_loader, device, tokenizer,\n",
        "                              jvp_eps=1e-3, jvp_k=1, jvp_every=1, epochs=6):\n",
        "    \"\"\"\n",
        "    Stage 1: Full FT while periodically collecting FD-JVP J-dev for decoder blocks.\n",
        "    \"\"\"\n",
        "    print(\"=== Stage 1: Full FT + FD-JVP (Decoder Blocks) ===\")\n",
        "    opt = build_optimizer(model)\n",
        "    dec_hooks, dec_bufs = register_decoder_block_jvp_hooks(model)\n",
        "\n",
        "    dec_sum, dec_cnt = defaultdict(float), defaultdict(int)\n",
        "    step = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            step += 1\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Disable AMP for numerical stability\n",
        "            with autocast(enabled=False):\n",
        "                out = model(\n",
        "                    input_ids=batch[\"input_ids\"].to(device),\n",
        "                    attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                    labels=batch[\"labels\"].to(device),\n",
        "                )\n",
        "                loss = out.loss\n",
        "\n",
        "            if not torch.isfinite(loss):\n",
        "                print(\"Loss is NaN/Inf — skipping batch.\")\n",
        "                continue\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # <-- fixed typo\n",
        "            opt.step()\n",
        "\n",
        "            # Collect FD-JVP every N steps\n",
        "            if step % max(1, jvp_every) == 0:\n",
        "                dec_scores = compute_decoder_block_jdev(model, dec_bufs, eps=jvp_eps, k_probes=jvp_k)\n",
        "                for i, v in dec_scores.items():\n",
        "                    if math.isfinite(v):\n",
        "                        dec_sum[i] += v\n",
        "                        dec_cnt[i] += 1\n",
        "\n",
        "        epoch_dec = {i: dec_sum[i] / dec_cnt[i] for i in dec_sum if dec_cnt[i] > 0}\n",
        "        print(f\"[Epoch {epoch+1}] Decoder Block J-dev (mean): {epoch_dec}\")\n",
        "        acc = evaluate_model(model, dev_loader, tokenizer, device)\n",
        "        print(f\"[Epoch {epoch+1}] Dev Acc: {acc:.4f}\")\n",
        "\n",
        "    remove_hooks(dec_hooks)\n",
        "    final_dec = {i: dec_sum[i] / dec_cnt[i] for i in dec_sum if dec_cnt[i] > 0}\n",
        "    return model, final_dec\n",
        "\n",
        "def prune_and_finetune(model, train_loader, dev_loader, device, tokenizer,\n",
        "                       dec_jdev_scores, num_prune=num, epochs=5):\n",
        "    print(\"=== Stage 2: Prune LOW-Jdev Decoder Blocks & Fine-tuning ===\")\n",
        "    _ = prune_decoder_blocks_by_jdev(\n",
        "        model.decoder.block, dec_jdev_scores, num_prune=num_prune,\n",
        "        protect_first=True, protect_last=False, verbose=True\n",
        "    )\n",
        "\n",
        "    # Keep cache off post-pruning to always recompute biases freshly\n",
        "    model.config.use_cache = False\n",
        "\n",
        "    opt = build_optimizer(model)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            opt.zero_grad()\n",
        "            with autocast(enabled=False):\n",
        "                out = model(\n",
        "                    input_ids=batch[\"input_ids\"].to(device),\n",
        "                    attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                    labels=batch[\"labels\"].to(device),\n",
        "                )\n",
        "                loss = out.loss\n",
        "            if not torch.isfinite(loss):\n",
        "                print(\"Loss is NaN/Inf — skipping batch.\")\n",
        "                continue\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # <-- fixed typo\n",
        "            opt.step()\n",
        "\n",
        "        acc = evaluate_model(model, dev_loader, tokenizer, device)\n",
        "        print(f\"[Prune FT Epoch {epoch+1}] ANLI1 Acc: {acc:.4f}\")\n",
        "    return model\n",
        "\n",
        "# --- 6. Entrypoint ---\n",
        "def main():\n",
        "    global model  # reuse the earlier-loaded model instance\n",
        "    model, dec_jdev_scores = full_finetuning_with_jdev(\n",
        "        model, train_loader, dev_loader, device, tokenizer,\n",
        "        jvp_eps=1e-3, jvp_k=1, jvp_every=1, epochs=6\n",
        "    )\n",
        "    model = prune_and_finetune(\n",
        "        model, train_loader, dev_loader, device, tokenizer,\n",
        "        dec_jdev_scores, num_prune=num, epochs=5\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwzouSkYJYLo"
      },
      "outputs": [],
      "source": [
        "# Only prune decoder — Autograd JVP (NaN-safe, bias-safe)\n",
        "# ATTENTION-ONLY pruning (self & cross) for T5 on ANLI1\n",
        "\n",
        "# --- Mount Google Drive if using Colab ---\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# --- Standard Imports ---\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration, T5TokenizerFast,\n",
        "    DataCollatorForSeq2Seq, Adafactor\n",
        ")\n",
        "from collections import defaultdict\n",
        "import warnings, math, random, numpy as np, inspect\n",
        "from contextlib import contextmanager\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# ------------- Repro -------------\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "set_seed(1234)\n",
        "\n",
        "# --- 1. Load ANLI1 Dataset ---\n",
        "data_files = {\n",
        "    \"train\":      \"/content/drive/MyDrive/NLP_datasets/anli1/anli1_train.json\",\n",
        "    \"validation\": \"/content/drive/MyDrive/NLP_datasets/anli1/anli1_valid.json\",\n",
        "    \"test\":       \"/content/drive/MyDrive/NLP_datasets/anli1/anli1_test.json\"\n",
        "}\n",
        "dataset = load_dataset(\"json\", data_files=data_files)\n",
        "\n",
        "# --- 2. Preprocessing Function ---\n",
        "def make_t5_nli_prompt(premise, hypothesis):\n",
        "    return f\"nli premise: {premise} hypothesis: {hypothesis}\"\n",
        "\n",
        "def preprocess_anli(batch, tokenizer, max_input_length=128, max_target_length=8):\n",
        "    inputs = [make_t5_nli_prompt(p, h) for p, h in zip(batch['premise'], batch['hypothesis'])]\n",
        "    label_list = [\"entailment\", \"neutral\", \"contradiction\"]\n",
        "    labels_str = []\n",
        "    for x in batch['label']:\n",
        "        sx = str(x).strip().lower()\n",
        "        if sx.isdigit() and int(sx) < 3: labels_str.append(label_list[int(sx)])\n",
        "        else: labels_str.append(sx)\n",
        "    model_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=max_input_length)\n",
        "    target = tokenizer(text_target=labels_str, padding=\"max_length\", truncation=True, max_length=max_target_length)\n",
        "    model_inputs[\"labels\"] = target[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
        "\n",
        "# Map datasets\n",
        "train = dataset[\"train\"].map(\n",
        "    lambda ex: preprocess_anli(ex, tokenizer),\n",
        "    batched=True, remove_columns=dataset[\"train\"].column_names\n",
        ")\n",
        "dev = dataset[\"validation\"].map(\n",
        "    lambda ex: preprocess_anli(ex, tokenizer),\n",
        "    batched=True, remove_columns=dataset[\"validation\"].column_names\n",
        ")\n",
        "\n",
        "# --- Load model before creating the collator (so collator can mask label pads -> -100) ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n",
        "# Avoid kv-cache so biases are always recomputed\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Collator that converts pad tokens in labels to -100\n",
        "collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=-100, padding=\"max_length\", max_length=128)\n",
        "train_loader = DataLoader(train, batch_size=16, shuffle=True,  collate_fn=collator)\n",
        "dev_loader   = DataLoader(dev,   batch_size=16, shuffle=False, collate_fn=collator)\n",
        "\n",
        "# =========================\n",
        "# 3) Autograd-JVP helpers (NaN-safe, bias-safe) — attention sublayers only\n",
        "# =========================\n",
        "\n",
        "def _filter_kwargs_for_module(module, kwargs):\n",
        "    sig = inspect.signature(module.forward)\n",
        "    allowed = set(sig.parameters.keys()) - {\"self\"}\n",
        "    return {k: v for k, v in kwargs.items() if k in allowed}\n",
        "\n",
        "def _map_args_from_hook(module, args, kwargs):\n",
        "    if kwargs and len(kwargs) > 0:\n",
        "        d = dict(kwargs)\n",
        "    else:\n",
        "        sig = inspect.signature(module.forward)\n",
        "        names = [n for n in sig.parameters.keys() if n != \"self\"]\n",
        "        d = {name: (args[i] if i < len(args) else sig.parameters[name].default) for i, name in enumerate(names)}\n",
        "    # We will set hidden_states; never reuse cached biases\n",
        "    d.pop(\"hidden_states\", None)\n",
        "    d.pop(\"position_bias\", None)\n",
        "    d.pop(\"encoder_decoder_position_bias\", None)\n",
        "    return d\n",
        "\n",
        "def _build_causal_mask(B, q_len, k_len, device, dtype):\n",
        "    mask = torch.zeros((B, 1, q_len, k_len), dtype=dtype, device=device)\n",
        "    tri = torch.triu(torch.ones((q_len, k_len), dtype=torch.bool, device=device), diagonal=1)\n",
        "    mask[:, :, tri] = -1e9\n",
        "    return mask\n",
        "\n",
        "def _ensure_mask_shape(mask, q_len, k_len, device, dtype, is_self):\n",
        "    if mask is None:\n",
        "        B = 1\n",
        "        return _build_causal_mask(B, q_len, k_len, device, dtype) if is_self \\\n",
        "               else torch.zeros((B,1,q_len,k_len), dtype=dtype, device=device)\n",
        "    if mask.dim() == 2:\n",
        "        B = mask.size(0)\n",
        "        return mask[:, None, None, :k_len].expand(B,1,q_len,k_len).contiguous().to(device=device, dtype=dtype)\n",
        "    if mask.dim() == 4:\n",
        "        B, _, Q, K = mask.shape\n",
        "        if Q >= q_len and K >= k_len:\n",
        "            return mask[:, :, :q_len, :k_len].to(device=device, dtype=dtype)\n",
        "        return _build_causal_mask(B, q_len, k_len, device, dtype) if is_self \\\n",
        "               else torch.zeros((B,1,q_len,k_len), dtype=dtype, device=device)\n",
        "    B = 1\n",
        "    return _build_causal_mask(B, q_len, k_len, device, dtype) if is_self \\\n",
        "           else torch.zeros((B,1,q_len,k_len), dtype=dtype, device=device)\n",
        "\n",
        "@contextmanager\n",
        "def force_math_sdp():\n",
        "    \"\"\"Force math SDPA kernels so higher-order grads exist; no-op if unavailable.\"\"\"\n",
        "    try:\n",
        "        with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True):\n",
        "            yield\n",
        "    except Exception:\n",
        "        yield\n",
        "\n",
        "def _make_attn_callable(module, argdict):\n",
        "    \"\"\"Build pure fn f(x)->hidden_states for a single T5 sublayer (self/cross).\"\"\"\n",
        "    raw = dict(argdict) if argdict is not None else {}\n",
        "    sig = inspect.signature(module.forward)\n",
        "    params = set(sig.parameters.keys())\n",
        "    is_cross = \"key_value_states\" in params\n",
        "\n",
        "    const_kwargs = {}\n",
        "    if is_cross:\n",
        "        kv = raw.get(\"key_value_states\", raw.get(\"encoder_hidden_states\", None))\n",
        "        const_kwargs[\"key_value_states\"] = None if kv is None else kv.float()\n",
        "\n",
        "    if \"use_cache\" in params:          const_kwargs[\"use_cache\"] = False\n",
        "    if \"output_attentions\" in params:  const_kwargs[\"output_attentions\"] = False\n",
        "    if \"return_dict\" in params:        const_kwargs[\"return_dict\"] = False\n",
        "\n",
        "    mask_key = \"attention_mask\" if \"attention_mask\" in params else (\"mask\" if \"mask\" in params else None)\n",
        "    captured_mask = raw.get(mask_key, raw.get(\"mask\", None)) if mask_key is not None else None\n",
        "    captured_kv   = const_kwargs.get(\"key_value_states\", None)\n",
        "\n",
        "    def _f(x):\n",
        "        x = x.float()\n",
        "        q_len = x.size(1)\n",
        "        k_len = captured_kv.size(1) if (is_cross and captured_kv is not None) else q_len\n",
        "        kwargs = {\"hidden_states\": x}\n",
        "        if is_cross:\n",
        "            if captured_kv is None:  # identity if no encoder states present\n",
        "                return x\n",
        "            kwargs[\"key_value_states\"] = captured_kv\n",
        "        if mask_key is not None:\n",
        "            kwargs[mask_key] = _ensure_mask_shape(captured_mask, q_len, k_len, x.device, x.dtype, is_self=not is_cross)\n",
        "        if \"cache_position\" in params:\n",
        "            kwargs[\"cache_position\"] = torch.arange(q_len, dtype=torch.long, device=x.device)\n",
        "        if \"query_length\" in params and \"cache_position\" not in params:\n",
        "            kwargs[\"query_length\"] = q_len\n",
        "\n",
        "        kwargs.update(const_kwargs)\n",
        "        kwargs = _filter_kwargs_for_module(module, kwargs)\n",
        "        with torch.cuda.amp.autocast(enabled=False):\n",
        "            out = module(**kwargs)\n",
        "        return out[0] if isinstance(out, (tuple, list)) else out\n",
        "    return _f\n",
        "\n",
        "def compute_decoder_attention_jdev_autograd(model, self_bufs, cross_bufs, k_probes=1, rademacher=True):\n",
        "    \"\"\"\n",
        "    For each decoder attention sublayer ℓ, estimate:\n",
        "      E_v ||(Jℓ - I)v||^2 using autograd JVP.\n",
        "    Non-finite probes are skipped. Buffers are cleared.\n",
        "    \"\"\"\n",
        "    from torch.autograd.functional import jvp as autograd_jvp\n",
        "\n",
        "    self_scores, cross_scores = {}, {}\n",
        "\n",
        "    # Self-attention J-dev\n",
        "    for idx, buf in self_bufs.items():\n",
        "        X, args = buf[\"X\"], buf[\"args\"]\n",
        "        buf[\"X\"], buf[\"args\"] = None, None  # clear ASAP\n",
        "        if X is None: continue\n",
        "        mod = model.decoder.block[idx].layer[0]  # T5LayerSelfAttention\n",
        "\n",
        "        was_training = mod.training\n",
        "        mod.eval()\n",
        "        try:\n",
        "            fn  = _make_attn_callable(mod, args)\n",
        "            x0  = X.detach().requires_grad_(True).float()\n",
        "            try:\n",
        "                with force_math_sdp():\n",
        "                    _ = fn(x0)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            acc, used = 0.0, 0\n",
        "            with force_math_sdp():\n",
        "                for _ in range(max(1, k_probes)):\n",
        "                    if rademacher:\n",
        "                        v = torch.empty_like(x0).bernoulli_(0.5).mul_(2).sub_(1)\n",
        "                    else:\n",
        "                        v = torch.randn_like(x0)\n",
        "                    try:\n",
        "                        _, Jv = autograd_jvp(fn, (x0,), (v,), create_graph=False, strict=True)\n",
        "                    except Exception:\n",
        "                        continue\n",
        "                    if Jv is None or not torch.isfinite(Jv).all():\n",
        "                        continue\n",
        "                    jd_vec = Jv - v\n",
        "                    if not torch.isfinite(jd_vec).all():\n",
        "                        continue\n",
        "                    acc += float(jd_vec.pow(2).mean().item()); used += 1\n",
        "            if used > 0: self_scores[idx] = acc / used\n",
        "        finally:\n",
        "            mod.train(was_training)\n",
        "\n",
        "    # Cross-attention J-dev\n",
        "    for idx, buf in cross_bufs.items():\n",
        "        X, args = buf[\"X\"], buf[\"args\"]\n",
        "        buf[\"X\"], buf[\"args\"] = None, None\n",
        "        if X is None: continue\n",
        "        if len(model.decoder.block[idx].layer) <= 1 or model.decoder.block[idx].layer[1] is None:\n",
        "            continue\n",
        "        mod = model.decoder.block[idx].layer[1]  # T5LayerCrossAttention\n",
        "\n",
        "        was_training = mod.training\n",
        "        mod.eval()\n",
        "        try:\n",
        "            fn  = _make_attn_callable(mod, args)\n",
        "            x0  = X.detach().requires_grad_(True).float()\n",
        "            try:\n",
        "                with force_math_sdp():\n",
        "                    _ = fn(x0)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            acc, used = 0.0, 0\n",
        "            with force_math_sdp():\n",
        "                for _ in range(max(1, k_probes)):\n",
        "                    if rademacher:\n",
        "                        v = torch.empty_like(x0).bernoulli_(0.5).mul_(2).sub_(1)\n",
        "                    else:\n",
        "                        v = torch.randn_like(x0)\n",
        "                    try:\n",
        "                        _, Jv = autograd_jvp(fn, (x0,), (v,), create_graph=False, strict=True)\n",
        "                    except Exception:\n",
        "                        continue\n",
        "                    if Jv is None or not torch.isfinite(Jv).all():\n",
        "                        continue\n",
        "                    jd_vec = Jv - v\n",
        "                    if not torch.isfinite(jd_vec).all():\n",
        "                        continue\n",
        "                    acc += float(jd_vec.pow(2).mean().item()); used += 1\n",
        "            if used > 0: cross_scores[idx] = acc / used\n",
        "        finally:\n",
        "            mod.train(was_training)\n",
        "\n",
        "    return self_scores, cross_scores\n",
        "\n",
        "# =========================\n",
        "# 4) Hooks to capture ONLY decoder attention inputs\n",
        "# =========================\n",
        "def register_decoder_attention_hooks(model):\n",
        "    dec_blocks = model.decoder.block\n",
        "    self_bufs  = {i: {\"X\": None, \"args\": None} for i in range(len(dec_blocks))}\n",
        "    cross_bufs = {i: {\"X\": None, \"args\": None} for i in range(len(dec_blocks))}\n",
        "    hooks = []\n",
        "\n",
        "    for i, block in enumerate(dec_blocks):\n",
        "        sa = block.layer[0]\n",
        "        def pre_hook_sa(module, args, kwargs, idx=i):\n",
        "            X = kwargs.get(\"hidden_states\", args[0] if len(args) > 0 else None)\n",
        "            self_bufs[idx][\"X\"]    = None if X is None else X.detach()\n",
        "            self_bufs[idx][\"args\"] = _map_args_from_hook(module, args, kwargs)\n",
        "        hooks.append(sa.register_forward_pre_hook(pre_hook_sa, with_kwargs=True))\n",
        "\n",
        "    for i, block in enumerate(dec_blocks):\n",
        "        if len(block.layer) > 1 and block.layer[1] is not None:\n",
        "            ca = block.layer[1]\n",
        "            def pre_hook_ca(module, args, kwargs, idx=i):\n",
        "                X = kwargs.get(\"hidden_states\", args[0] if len(args) > 0 else None)\n",
        "                cross_bufs[idx][\"X\"]    = None if X is None else X.detach()\n",
        "                cross_bufs[idx][\"args\"] = _map_args_from_hook(module, args, kwargs)\n",
        "            hooks.append(ca.register_forward_pre_hook(pre_hook_ca, with_kwargs=True))\n",
        "\n",
        "    return hooks, self_bufs, cross_bufs\n",
        "\n",
        "def remove_hooks(hooks):\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "# =========================\n",
        "# 5) Attention-only pruning utilities\n",
        "# =========================\n",
        "class SkipSelfAttention(nn.Module):\n",
        "    \"\"\"Identity replacement for T5LayerSelfAttention (also drop position_bias).\"\"\"\n",
        "    def forward(self, hidden_states, *args, **kwargs):\n",
        "        return (hidden_states, None, None)\n",
        "\n",
        "class SkipCrossAttention(nn.Module):\n",
        "    \"\"\"Identity replacement for T5LayerCrossAttention (also drop position_bias).\"\"\"\n",
        "    def forward(self, hidden_states, *args, **kwargs):\n",
        "        return (hidden_states, None, None)\n",
        "\n",
        "def prune_decoder_attention(model, self_scores, cross_scores,\n",
        "                            k_self=2, k_cross=2,\n",
        "                            protect_first=True, protect_last=False, verbose=True):\n",
        "    num_layers = len(model.decoder.block)\n",
        "    self_items  = list(self_scores.items())\n",
        "    cross_items = list(cross_scores.items())\n",
        "\n",
        "    if protect_first:\n",
        "        self_items  = [(i,s) for i,s in self_items  if i != 0]\n",
        "        cross_items = [(i,s) for i,s in cross_items if i != 0]\n",
        "    if protect_last:\n",
        "        self_items  = [(i,s) for i,s in self_items  if i != num_layers-1]\n",
        "        cross_items = [(i,s) for i,s in cross_items if i != num_layers-1]\n",
        "\n",
        "    self_items.sort(key=lambda x: x[1])   # lowest J-dev first\n",
        "    cross_items.sort(key=lambda x: x[1])\n",
        "\n",
        "    pruned_self, pruned_cross = [], []\n",
        "\n",
        "    for i, _ in self_items[:max(0, k_self)]:\n",
        "        model.decoder.block[i].layer[0] = SkipSelfAttention()\n",
        "        pruned_self.append(i)\n",
        "\n",
        "    for i, _ in cross_items[:max(0, k_cross)]:\n",
        "        if len(model.decoder.block[i].layer) > 1 and model.decoder.block[i].layer[1] is not None:\n",
        "            model.decoder.block[i].layer[1] = SkipCrossAttention()\n",
        "            pruned_cross.append(i)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Pruned decoder SELF-attention at layers:  {pruned_self}\")\n",
        "        print(f\"Pruned decoder CROSS-attention at layers: {pruned_cross}\")\n",
        "\n",
        "    return pruned_self, pruned_cross\n",
        "\n",
        "# =========================\n",
        "# 6) Training / Eval helpers\n",
        "# =========================\n",
        "CANON = {\n",
        "    \"entailment\": \"entailment\",\n",
        "    \"entailed\": \"entailment\",\n",
        "    \"neutral\": \"neutral\",\n",
        "    \"contradiction\": \"contradiction\",\n",
        "    \"contradict\": \"contradiction\",\n",
        "    \"contradictory\": \"contradiction\",\n",
        "    \"contradicted\": \"contradiction\",\n",
        "}\n",
        "def canonicalize_label(s: str):\n",
        "    s = (s or \"\").strip().lower()\n",
        "    first = s.split()[0] if s else s\n",
        "    return CANON.get(first, first)\n",
        "\n",
        "def compute_accuracy(preds, refs):\n",
        "    correct = 0\n",
        "    for p, l in zip(preds, refs):\n",
        "        if canonicalize_label(p) == canonicalize_label(l):\n",
        "            correct += 1\n",
        "    return correct / len(preds) if len(preds) > 0 else 0.0\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, dl, tokenizer, device):\n",
        "    model.config.use_cache = False\n",
        "    model.eval()\n",
        "    preds, refs = [], []\n",
        "    for batch in dl:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        # decode gold labels to text\n",
        "        label_ids = batch[\"labels\"].clone()\n",
        "        label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "        ref_texts = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=4,\n",
        "            use_cache=False,\n",
        "        )\n",
        "        pred_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        preds.extend([p.strip().lower() for p in pred_texts])\n",
        "        refs.extend([l.strip().lower() for l in ref_texts])\n",
        "    return compute_accuracy(preds, refs)\n",
        "\n",
        "def build_optimizer(m):\n",
        "    return Adafactor(m.parameters(), relative_step=True, scale_parameter=True, warmup_init=True)\n",
        "\n",
        "# =========================\n",
        "# 7) Train → collect Autograd-JVP (attention) → prune → FT\n",
        "# =========================\n",
        "def full_finetuning_collect_attn_jdev(model, train_loader, dev_loader, device, tokenizer,\n",
        "                                      jvp_k=1, jvp_every=1, epochs=4):\n",
        "    print(\"=== Stage 1: Full FT & Autograd-JVP (Decoder Attention only) ===\")\n",
        "    model.config.use_cache = False\n",
        "    opt = build_optimizer(model)\n",
        "\n",
        "    hooks, self_bufs, cross_bufs = register_decoder_attention_hooks(model)\n",
        "    self_sum, self_cnt   = defaultdict(float), defaultdict(int)\n",
        "    cross_sum, cross_cnt = defaultdict(float), defaultdict(int)\n",
        "\n",
        "    step = 0\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            step += 1\n",
        "            opt.zero_grad()\n",
        "\n",
        "            out = model(\n",
        "                input_ids=batch[\"input_ids\"].to(device),\n",
        "                attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                labels=batch[\"labels\"].to(device),\n",
        "            )\n",
        "            loss = out.loss\n",
        "            if not torch.isfinite(loss):\n",
        "                print(\"Loss is NaN/Inf — skipping batch.\"); continue\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "\n",
        "            if step % max(1, jvp_every) == 0:\n",
        "                self_scores, cross_scores = compute_decoder_attention_jdev_autograd(\n",
        "                    model, self_bufs, cross_bufs, k_probes=jvp_k, rademacher=True\n",
        "                )\n",
        "                for i, v in self_scores.items():\n",
        "                    if math.isfinite(v): self_sum[i]  += v; self_cnt[i]  += 1\n",
        "                for i, v in cross_scores.items():\n",
        "                    if math.isfinite(v): cross_sum[i] += v; cross_cnt[i] += 1\n",
        "\n",
        "        epoch_self  = {i: self_sum[i]/self_cnt[i]   for i in self_sum  if self_cnt[i]  > 0}\n",
        "        epoch_cross = {i: cross_sum[i]/cross_cnt[i] for i in cross_sum if cross_cnt[i] > 0}\n",
        "        print(f\"[Epoch {epoch+1}] Decoder SELF-attn J-dev:  {epoch_self}\")\n",
        "        print(f\"[Epoch {epoch+1}] Decoder CROSS-attn J-dev: {epoch_cross}\")\n",
        "        acc = evaluate_model(model, dev_loader, tokenizer, device)\n",
        "        print(f\"[Epoch {epoch+1}] Dev Acc: {acc:.4f}\")\n",
        "\n",
        "    remove_hooks(hooks)\n",
        "    final_self  = {i: self_sum[i]/self_cnt[i]   for i in self_sum   if self_cnt[i]   > 0}\n",
        "    final_cross = {i: cross_sum[i]/cross_cnt[i] for i in cross_sum  if cross_cnt[i]  > 0}\n",
        "    return model, final_self, final_cross\n",
        "\n",
        "def prune_attention_and_finetune(model, train_loader, dev_loader, device, tokenizer,\n",
        "                                 self_scores, cross_scores, k_self=num, k_cross=0, epochs=5):\n",
        "    print(\"=== Stage 2: Prune LOW-Jdev Attention (self/cross) & Fine-tuning ===\")\n",
        "    _self, _cross = prune_decoder_attention(\n",
        "        model, self_scores, cross_scores,\n",
        "        k_self=k_self, k_cross=k_cross, protect_first=True, protect_last=False, verbose=True\n",
        "    )\n",
        "\n",
        "    model.config.use_cache = False\n",
        "    opt = build_optimizer(model)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            opt.zero_grad()\n",
        "            out = model(\n",
        "                input_ids=batch[\"input_ids\"].to(device),\n",
        "                attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                labels=batch[\"labels\"].to(device),\n",
        "            )\n",
        "            loss = out.loss\n",
        "            if not torch.isfinite(loss):\n",
        "                print(\"Loss is NaN/Inf — skipping batch.\"); continue\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "\n",
        "        acc = evaluate_model(model, dev_loader, tokenizer, device)\n",
        "        print(f\"[Prune FT Epoch {epoch+1}] ANLI1 Acc: {acc:.4f}\")\n",
        "    return model, _self, _cross\n",
        "\n",
        "# --- 8. Entrypoint ---\n",
        "def main():\n",
        "    global model\n",
        "    model, self_scores, cross_scores = full_finetuning_collect_attn_jdev(\n",
        "        model, train_loader, dev_loader, device, tokenizer,\n",
        "        jvp_k=1, jvp_every=1, epochs=6\n",
        "    )\n",
        "    model, pruned_self, pruned_cross = prune_attention_and_finetune(\n",
        "        model, train_loader, dev_loader, device, tokenizer,\n",
        "        self_scores, cross_scores, k_self=num, k_cross=0, epochs=5\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc5B9-XdmCSo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XALVC5NdcGfg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSIg78iPcHF0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gX0GHph222Ka"
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# 0. Google Drive Mount\n",
        "# ===========================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ===========================\n",
        "# 1. Imports and Setup\n",
        "# ===========================\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration, T5TokenizerFast,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "from collections import defaultdict\n",
        "import warnings, math, inspect, random, numpy as np\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Repro\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "set_seed(1234)\n",
        "\n",
        "# ===========================\n",
        "# 2. Load SVAMP Dataset\n",
        "# ===========================\n",
        "data_files = {\n",
        "    \"train\": \"/content/drive/MyDrive/NLP_datasets/svamp/svamp_train.json\",\n",
        "    \"test\":  \"/content/drive/MyDrive/NLP_datasets/svamp/svamp_test.json\"\n",
        "}\n",
        "dataset = load_dataset(\"json\", data_files=data_files)\n",
        "\n",
        "# ===========================\n",
        "# 3. Preprocessing\n",
        "# ===========================\n",
        "def preprocess_svamp(batch, tokenizer, max_input_length=128, max_target_length=8):\n",
        "    model_inputs = tokenizer(\n",
        "        batch[\"input\"], padding=\"max_length\", truncation=True, max_length=max_input_length\n",
        "    )\n",
        "    targets = [str(x) for x in batch[\"label\"]]\n",
        "    target_encodings = tokenizer(\n",
        "        targets, padding=\"max_length\", truncation=True, max_length=max_target_length\n",
        "    )\n",
        "    model_inputs[\"labels\"] = target_encodings[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
        "train = dataset[\"train\"].map(lambda ex: preprocess_svamp(ex, tokenizer), batched=True, remove_columns=dataset[\"train\"].column_names)\n",
        "dev   = dataset[\"test\"].map(lambda ex: preprocess_svamp(ex, tokenizer),  batched=True, remove_columns=dataset[\"test\"].column_names)\n",
        "\n",
        "# Collator (pad labels to -100 automatically if model is passed; safe with default too)\n",
        "collator = DataCollatorForSeq2Seq(tokenizer, model=None, padding=\"max_length\", max_length=128)\n",
        "train_loader = DataLoader(train, batch_size=8, shuffle=True,  collate_fn=collator)\n",
        "dev_loader   = DataLoader(dev,   batch_size=8, shuffle=False, collate_fn=collator)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ===========================\n",
        "# 4) FD-JVP helpers (NaN-safe) — capture + deterministic replay for T5Block\n",
        "# ===========================\n",
        "\n",
        "def _filter_kwargs_for_module(module, kwargs):\n",
        "    sig = inspect.signature(module.forward)\n",
        "    allowed = set(sig.parameters.keys()) - {\"self\"}\n",
        "    return {k: v for k, v in kwargs.items() if k in allowed}\n",
        "\n",
        "def _map_args_from_hook(module, args, kwargs):\n",
        "    \"\"\"Normalize to kwargs for re-calling module.forward.\"\"\"\n",
        "    if kwargs and len(kwargs) > 0:\n",
        "        d = dict(kwargs)\n",
        "    else:\n",
        "        sig = inspect.signature(module.forward)\n",
        "        names = [n for n in sig.parameters.keys() if n != \"self\"]\n",
        "        d = {}\n",
        "        for i, name in enumerate(names):\n",
        "            d[name] = args[i] if i < len(args) else sig.parameters[name].default\n",
        "    d.pop(\"hidden_states\", None)  # we set explicitly\n",
        "    return d\n",
        "\n",
        "def _build_causal_mask(B, q_len, k_len, device, dtype):\n",
        "    # 0 for allowed, large negative for disallowed (stable vs -inf)\n",
        "    mask = torch.zeros((B, 1, q_len, k_len), dtype=dtype, device=device)\n",
        "    tri = torch.triu(torch.ones((q_len, k_len), dtype=torch.bool, device=device), diagonal=1)\n",
        "    mask[:, :, tri] = -1e9\n",
        "    return mask\n",
        "\n",
        "def _build_zero_mask(B, q_len, k_len, device, dtype):\n",
        "    return torch.zeros((B, 1, q_len, k_len), dtype=dtype, device=device)\n",
        "\n",
        "def _ensure_mask_shape(mask, q_len, k_len, device, dtype, is_self, B_fallback=1):\n",
        "    \"\"\"\n",
        "    Produce a valid (B,1,q_len,k_len) mask; rebuild default if missing/invalid.\n",
        "    Self-attn -> causal mask; Cross-attn -> zero mask.\n",
        "    \"\"\"\n",
        "    default_fn = _build_causal_mask if is_self else _build_zero_mask\n",
        "\n",
        "    if mask is None:\n",
        "        return default_fn(B_fallback, q_len, k_len, device, dtype)\n",
        "\n",
        "    if mask.dim() == 2:  # (B, K)\n",
        "        B = mask.size(0)\n",
        "        return mask[:, None, None, :k_len].expand(B, 1, q_len, k_len).contiguous().to(device=device, dtype=dtype)\n",
        "\n",
        "    if mask.dim() == 4:\n",
        "        B, _, Q, K = mask.shape\n",
        "        if Q >= q_len and K >= k_len:\n",
        "            return mask[:, :, :q_len, :k_len].to(device=device, dtype=dtype)\n",
        "        return default_fn(B, q_len, k_len, device, dtype)\n",
        "\n",
        "    return default_fn(B_fallback, q_len, k_len, device, dtype)\n",
        "\n",
        "@torch.no_grad()\n",
        "def _t5block_forward_only(block, X, argdict):\n",
        "    \"\"\"\n",
        "    Deterministic single T5Block forward in fp32 (autocast disabled).\n",
        "    Provides safe masks & cache_position/query_length when missing.\n",
        "    Returns only hidden_states tensor (or None if non-finite).\n",
        "    \"\"\"\n",
        "    if X is None:\n",
        "        return None\n",
        "    was_training = block.training\n",
        "    block.eval()\n",
        "    Xf = X.float()\n",
        "    try:\n",
        "        raw = dict(argdict) if argdict is not None else {}\n",
        "        kwargs = {\"hidden_states\": Xf}\n",
        "\n",
        "        q_len = Xf.size(1)\n",
        "        B = Xf.size(0)\n",
        "\n",
        "        # Self-attention mask at block level\n",
        "        attn_mask = raw.get(\"attention_mask\", raw.get(\"mask\", None))\n",
        "        kwargs[\"attention_mask\"] = _ensure_mask_shape(\n",
        "            attn_mask, q_len, q_len, Xf.device, Xf.dtype, is_self=True, B_fallback=B\n",
        "        )\n",
        "\n",
        "        # Cross-attention (if encoder states present)\n",
        "        enc_states = raw.get(\"encoder_hidden_states\", None)\n",
        "        if enc_states is not None:\n",
        "            enc_states = enc_states.float()\n",
        "            kwargs[\"encoder_hidden_states\"] = enc_states\n",
        "            k_len = enc_states.size(1)\n",
        "            enc_mask = raw.get(\"encoder_attention_mask\", None)\n",
        "            kwargs[\"encoder_attention_mask\"] = _ensure_mask_shape(\n",
        "                enc_mask, q_len, k_len, Xf.device, Xf.dtype, is_self=False, B_fallback=B\n",
        "            )\n",
        "\n",
        "        # Optional signature args (version-dependent)\n",
        "        params = set(inspect.signature(block.forward).parameters.keys())\n",
        "        if \"cache_position\" in params:\n",
        "            cp = raw.get(\"cache_position\", None)\n",
        "            if cp is None:\n",
        "                cp = torch.arange(q_len, dtype=torch.long, device=Xf.device)\n",
        "            kwargs[\"cache_position\"] = cp\n",
        "        if \"query_length\" in params:\n",
        "            kwargs[\"query_length\"] = q_len\n",
        "        if \"use_cache\" in params:\n",
        "            kwargs[\"use_cache\"] = False\n",
        "        if \"output_attentions\" in params:\n",
        "            kwargs[\"output_attentions\"] = False\n",
        "        if \"return_dict\" in params:\n",
        "            kwargs[\"return_dict\"] = False\n",
        "\n",
        "        kwargs = _filter_kwargs_for_module(block, kwargs)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=False):\n",
        "            out = block(**kwargs)\n",
        "\n",
        "        hs = out[0] if isinstance(out, (tuple, list)) else out\n",
        "        return hs if hs is None or torch.isfinite(hs).all() else None\n",
        "    finally:\n",
        "        block.train(was_training)\n",
        "\n",
        "def register_decoder_block_jvp_hooks(model):\n",
        "    \"\"\"\n",
        "    Capture per-decoder-block incoming hidden_states + kwargs with forward_pre_hook.\n",
        "    \"\"\"\n",
        "    dec_blocks = model.decoder.block\n",
        "    dec_bufs = {i: {\"X\": None, \"args\": None} for i in range(len(dec_blocks))}\n",
        "    dec_hooks = []\n",
        "    for i, block in enumerate(dec_blocks):\n",
        "        def pre_hook(module, args, kwargs, idx=i):\n",
        "            X = kwargs.get(\"hidden_states\", args[0] if len(args) > 0 else None)\n",
        "            dec_bufs[idx][\"X\"]    = None if X is None else X.detach()\n",
        "            dec_bufs[idx][\"args\"] = _map_args_from_hook(module, args, kwargs)\n",
        "        dec_hooks.append(block.register_forward_pre_hook(pre_hook, with_kwargs=True))\n",
        "    return dec_hooks, dec_bufs\n",
        "\n",
        "def remove_hooks(hooks):\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_decoder_block_jdev(model, dec_bufs, eps=1e-3, k_probes=1):\n",
        "    \"\"\"\n",
        "    For each decoder block ℓ, estimate E_v ||(Jℓ - I)v||^2 via finite differences:\n",
        "      (J - I)v ≈ (f(x + eps v) - f(x))/eps - v\n",
        "    NaN-safe: skips any non-finite forward/probe results. Clears buffers.\n",
        "    \"\"\"\n",
        "    scores = {}\n",
        "    for idx, buf in dec_bufs.items():\n",
        "        X, args = buf[\"X\"], buf[\"args\"]\n",
        "        if X is None:\n",
        "            continue\n",
        "        block = model.decoder.block[idx]\n",
        "        X = X.float()\n",
        "\n",
        "        y0 = _t5block_forward_only(block, X, args)\n",
        "        if y0 is None:\n",
        "            buf[\"X\"], buf[\"args\"] = None, None\n",
        "            continue\n",
        "\n",
        "        acc, used = 0.0, 0\n",
        "        for _ in range(max(1, k_probes)):\n",
        "            v = torch.randn_like(X)\n",
        "            y_eps = _t5block_forward_only(block, X + eps * v, args)\n",
        "            if y_eps is None:\n",
        "                continue\n",
        "            jd_vec = (y_eps - y0) / eps - v\n",
        "            if not torch.isfinite(jd_vec).all():\n",
        "                continue\n",
        "            acc += float(jd_vec.pow(2).mean().item())\n",
        "            used += 1\n",
        "\n",
        "        if used > 0:\n",
        "            scores[idx] = acc / used\n",
        "\n",
        "        buf[\"X\"], buf[\"args\"] = None, None\n",
        "    return scores\n",
        "\n",
        "# ===========================\n",
        "# 5) Pruning Utilities (decoder blocks)\n",
        "# ===========================\n",
        "class SkipBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Identity replacement for a T5 decoder block. Keeps tuple shape:\n",
        "    (hidden_states, present_key_value, self_attn_weights, cross_attn_weights,\n",
        "     position_bias, encoder_decoder_position_bias)\n",
        "    \"\"\"\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        position_bias=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        encoder_decoder_position_bias=None,\n",
        "        layer_head_mask=None,\n",
        "        cross_attn_layer_head_mask=None,\n",
        "        past_key_value=None,\n",
        "        use_cache=False,\n",
        "        output_attentions=False,\n",
        "        return_dict=False,\n",
        "        cache_position=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        return (hidden_states, None, None, None, position_bias, encoder_decoder_position_bias)\n",
        "\n",
        "def prune_decoder_blocks_by_jdev(blocks, jdev_scores, num_prune=4, protect_first=True, protect_last=False, verbose=True):\n",
        "    \"\"\"\n",
        "    Prune LOW J-dev (near-identity) decoder blocks.\n",
        "    \"\"\"\n",
        "    if not jdev_scores:\n",
        "        if verbose: print(\"No J-dev scores available; skipping pruning.\")\n",
        "        return []\n",
        "    items = list(jdev_scores.items())\n",
        "    if protect_first:\n",
        "        items = [(i, s) for i, s in items if i != 0]\n",
        "    if protect_last:\n",
        "        items = [(i, s) for i, s in items if i != len(blocks) - 1]\n",
        "    items.sort(key=lambda x: x[1])  # lowest first\n",
        "    prune_idxs = [i for i, _ in items[:max(0, num_prune)]]\n",
        "    for i in prune_idxs:\n",
        "        blocks[i] = SkipBlock()\n",
        "    if verbose:\n",
        "        print(f\"Pruned decoder blocks (lowest J-dev): {prune_idxs}\")\n",
        "    return prune_idxs\n",
        "\n",
        "# ===========================\n",
        "# 6) Eval Helper\n",
        "# ===========================\n",
        "def compute_accuracy(preds, refs):\n",
        "    correct = 0\n",
        "    for p, l in zip(preds, refs):\n",
        "        if str(p).strip().lower() == str(l).strip().lower():\n",
        "            correct += 1\n",
        "    return correct / len(preds) if len(preds) > 0 else 0.0\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, dl, tokenizer, device):\n",
        "    model.eval()\n",
        "    preds, refs = [], []\n",
        "    for batch in dl:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        # decode gold labels to text\n",
        "        label_ids = batch[\"labels\"].clone()\n",
        "        label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "        ref_texts = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=8,\n",
        "            use_cache=False  # stay consistent with SkipBlock semantics\n",
        "        )\n",
        "        pred_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "        preds.extend([p.strip().lower() for p in pred_texts])\n",
        "        refs.extend([l.strip().lower() for l in ref_texts])\n",
        "    return compute_accuracy(preds, refs)\n",
        "\n",
        "# ===========================\n",
        "# 7) Training + FD-JVP + Pruning\n",
        "# ===========================\n",
        "def full_finetuning_with_jdev(train_loader, dev_loader, device, tokenizer,\n",
        "                              jvp_eps=1e-3, jvp_k=1, jvp_every=1, epochs=6):\n",
        "    \"\"\"\n",
        "    Stage 1: full FT while periodically collecting FD-JVP J-dev for decoder blocks.\n",
        "    \"\"\"\n",
        "    print(\"=== Stage 1: Full FT + FD-JVP (Decoder Blocks) ===\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n",
        "    # Avoid kv-cache complexity in replay and in SkipBlock\n",
        "    model.config.use_cache = False\n",
        "\n",
        "    # Simple, stable optimizer (AdamW with small LR also fine)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "    dec_hooks, dec_bufs = register_decoder_block_jvp_hooks(model)\n",
        "    dec_sum, dec_cnt = defaultdict(float), defaultdict(int)\n",
        "    step = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            step += 1\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Disable AMP for stability throughout FD-JVP collection phase\n",
        "            out = model(\n",
        "                input_ids=batch[\"input_ids\"].to(device),\n",
        "                attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                labels=batch[\"labels\"].to(device),\n",
        "            )\n",
        "            loss = out.loss\n",
        "            if not torch.isfinite(loss):\n",
        "                print(\"Loss is NaN/Inf — skipping batch.\")\n",
        "                continue\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "\n",
        "            # Collect FD-JVP every N steps\n",
        "            if step % max(1, jvp_every) == 0:\n",
        "                dec_scores = compute_decoder_block_jdev(model, dec_bufs, eps=jvp_eps, k_probes=jvp_k)\n",
        "                for i, v in dec_scores.items():\n",
        "                    if math.isfinite(v):\n",
        "                        dec_sum[i] += v\n",
        "                        dec_cnt[i] += 1\n",
        "\n",
        "        epoch_dec = {i: dec_sum[i] / dec_cnt[i] for i in dec_sum if dec_cnt[i] > 0}\n",
        "        print(f\"[Epoch {epoch+1}] Decoder Block J-dev (mean): {epoch_dec}\")\n",
        "        acc = evaluate_model(model, dev_loader, tokenizer, device)\n",
        "        print(f\"[Epoch {epoch+1}] Dev Acc: {acc:.4f}\")\n",
        "\n",
        "    remove_hooks(dec_hooks)\n",
        "    final_dec = {i: dec_sum[i] / dec_cnt[i] for i in dec_sum if dec_cnt[i] > 0}\n",
        "    return model, final_dec\n",
        "\n",
        "def prune_and_finetuning(model, train_loader, dev_loader, device, tokenizer,\n",
        "                         dec_jdev_scores, num_prune=num, epochs=5):\n",
        "    print(\"=== Stage 2: Prune LOW-Jdev Decoder Blocks & Fine-tuning ===\")\n",
        "    _ = prune_decoder_blocks_by_jdev(\n",
        "        model.decoder.block, dec_jdev_scores, num_prune=num_prune,\n",
        "        protect_first=True, protect_last=False, verbose=True\n",
        "    )\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            opt.zero_grad()\n",
        "            out = model(\n",
        "                input_ids=batch[\"input_ids\"].to(device),\n",
        "                attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                labels=batch[\"labels\"].to(device),\n",
        "            )\n",
        "            loss = out.loss\n",
        "            if not torch.isfinite(loss):\n",
        "                print(\"Loss is NaN/Inf — skipping batch.\")\n",
        "                continue\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "\n",
        "        acc = evaluate_model(model, dev_loader, tokenizer, device)\n",
        "        print(f\"[Prune FT Epoch {epoch+1}] SVAMP Acc: {acc:.4f}\")\n",
        "    return model\n",
        "\n",
        "# ===========================\n",
        "# 8. Main Entrypoint\n",
        "# ===========================\n",
        "def main():\n",
        "    model, dec_jdev_scores = full_finetuning_with_jdev(\n",
        "        train_loader, dev_loader, device, tokenizer,\n",
        "        jvp_eps=1e-3, jvp_k=1, jvp_every=1, epochs=6\n",
        "    )\n",
        "    model = prune_and_finetuning(\n",
        "        model, train_loader, dev_loader, device, tokenizer,\n",
        "        dec_jdev_scores, num_prune=num, epochs=5\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGXGpf-0nyNa"
      },
      "outputs": [],
      "source": [
        "# Autograd JVP, SVAMP\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# 0) (Optional) Google Drive Mount (no-op off Colab)\n",
        "# ===========================\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ===========================\n",
        "# 1) Imports and Setup\n",
        "# ===========================\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration, T5TokenizerFast,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "from collections import defaultdict\n",
        "import warnings, math, inspect, random, numpy as np\n",
        "from contextlib import nullcontext\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Repro\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "set_seed(1234)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ===========================\n",
        "# 2) Load SVAMP Dataset\n",
        "# ===========================\n",
        "data_files = {\n",
        "    \"train\": \"/content/drive/MyDrive/NLP_datasets/svamp/svamp_train.json\",\n",
        "    \"test\":  \"/content/drive/MyDrive/NLP_datasets/svamp/svamp_test.json\"\n",
        "}\n",
        "dataset = load_dataset(\"json\", data_files=data_files)\n",
        "\n",
        "# ===========================\n",
        "# 3) Preprocessing\n",
        "# ===========================\n",
        "def preprocess_svamp(batch, tokenizer, max_input_length=128, max_target_length=8):\n",
        "    model_inputs = tokenizer(\n",
        "        batch[\"input\"], padding=\"max_length\", truncation=True, max_length=max_input_length\n",
        "    )\n",
        "    targets = [str(x) for x in batch[\"label\"]]\n",
        "    target_encodings = tokenizer(\n",
        "        targets, padding=\"max_length\", truncation=True, max_length=max_target_length\n",
        "    )\n",
        "    model_inputs[\"labels\"] = target_encodings[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
        "train = dataset[\"train\"].map(lambda ex: preprocess_svamp(ex, tokenizer),\n",
        "                             batched=True, remove_columns=dataset[\"train\"].column_names)\n",
        "dev   = dataset[\"test\"].map(lambda ex: preprocess_svamp(ex, tokenizer),\n",
        "                             batched=True, remove_columns=dataset[\"test\"].column_names)\n",
        "\n",
        "# Collator (mask label pads to -100 even without passing model)\n",
        "collator = DataCollatorForSeq2Seq(tokenizer, model=None, label_pad_token_id=-100,\n",
        "                                  padding=\"max_length\", max_length=128)\n",
        "train_loader = DataLoader(train, batch_size=8, shuffle=True,  collate_fn=collator)\n",
        "dev_loader   = DataLoader(dev,   batch_size=8, shuffle=False, collate_fn=collator)\n",
        "\n",
        "# ===========================\n",
        "# 4) Autograd-JVP helpers — capture + deterministic replay for T5Block\n",
        "# ===========================\n",
        "\n",
        "def _filter_kwargs_for_module(module, kwargs):\n",
        "    sig = inspect.signature(module.forward)\n",
        "    allowed = set(sig.parameters.keys()) - {\"self\"}\n",
        "    return {k: v for k, v in kwargs.items() if k in allowed}\n",
        "\n",
        "def _map_args_from_hook(module, args, kwargs):\n",
        "    \"\"\"Normalize to kwargs for re-calling module.forward; drop cached biases.\"\"\"\n",
        "    if kwargs and len(kwargs) > 0:\n",
        "        d = dict(kwargs)\n",
        "    else:\n",
        "        sig = inspect.signature(module.forward)\n",
        "        names = [n for n in sig.parameters.keys() if n != \"self\"]\n",
        "        d = {}\n",
        "        for i, name in enumerate(names):\n",
        "            d[name] = args[i] if i < len(args) else sig.parameters[name].default\n",
        "    d.pop(\"hidden_states\", None)                  # we set explicitly\n",
        "    d.pop(\"position_bias\", None)                  # never reuse cached biases\n",
        "    d.pop(\"encoder_decoder_position_bias\", None)  # never reuse cached biases\n",
        "    return d\n",
        "\n",
        "def _build_causal_mask(B, q_len, k_len, device, dtype):\n",
        "    # stable large negative (not -inf) to avoid NaNs in softmax\n",
        "    mask = torch.zeros((B, 1, q_len, k_len), dtype=dtype, device=device)\n",
        "    tri = torch.triu(torch.ones((q_len, k_len), dtype=torch.bool, device=device), diagonal=1)\n",
        "    mask[:, :, tri] = -1e9\n",
        "    return mask\n",
        "\n",
        "def _build_zero_mask(B, q_len, k_len, device, dtype):\n",
        "    return torch.zeros((B, 1, q_len, k_len), dtype=dtype, device=device)\n",
        "\n",
        "def _ensure_mask_shape(mask, q_len, k_len, device, dtype, is_self, B_fallback=1):\n",
        "    \"\"\"\n",
        "    Produce a valid (B,1,q_len,k_len) mask; rebuild default if missing/invalid.\n",
        "    Self-attn -> causal mask; Cross-attn -> zero mask.\n",
        "    \"\"\"\n",
        "    default_fn = _build_causal_mask if is_self else _build_zero_mask\n",
        "\n",
        "    if mask is None:\n",
        "        return default_fn(B_fallback, q_len, k_len, device, dtype)\n",
        "\n",
        "    if mask.dim() == 2:  # (B, K)\n",
        "        B = mask.size(0)\n",
        "        return mask[:, None, None, :k_len].expand(B, 1, q_len, k_len).contiguous().to(device=device, dtype=dtype)\n",
        "\n",
        "    if mask.dim() == 4:\n",
        "        B, _, Q, K = mask.shape\n",
        "        if Q >= q_len and K >= k_len:\n",
        "            return mask[:, :, :q_len, :k_len].to(device=device, dtype=dtype)\n",
        "        return default_fn(B, q_len, k_len, device, dtype)\n",
        "\n",
        "    return default_fn(B_fallback, q_len, k_len, device, dtype)\n",
        "\n",
        "def _sdp_math_ctx():\n",
        "    \"\"\"\n",
        "    Prefer math (non-Flash/MemEfficient) SDP kernels so higher-order grads exist.\n",
        "    Falls back to nullcontext if backend is unavailable (e.g., CPU).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return torch.backends.cuda.sdp_kernel(enable_flash=False,\n",
        "                                              enable_mem_efficient=False,\n",
        "                                              enable_math=True)\n",
        "    except Exception:\n",
        "        return nullcontext()\n",
        "\n",
        "def _make_block_callable(block, argdict):\n",
        "    \"\"\"\n",
        "    Build pure fn f(x)->hs for a single T5 decoder block.\n",
        "    Rebuild masks; never reuse captured position biases.\n",
        "    \"\"\"\n",
        "    raw = dict(argdict) if argdict is not None else {}\n",
        "\n",
        "    # never reuse cached biases (shape-mismatch risk during generation)\n",
        "    raw.pop(\"position_bias\", None)\n",
        "    raw.pop(\"encoder_decoder_position_bias\", None)\n",
        "\n",
        "    params = set(inspect.signature(block.forward).parameters.keys())\n",
        "\n",
        "    # captured sources for masks & encoder states\n",
        "    attn_mask_src = raw.get(\"attention_mask\", raw.get(\"mask\", None))\n",
        "    enc_states    = raw.get(\"encoder_hidden_states\", None)\n",
        "    enc_mask_src  = raw.get(\"encoder_attention_mask\", None)\n",
        "\n",
        "    const_kwargs = {}\n",
        "    if enc_states is not None:\n",
        "        const_kwargs[\"encoder_hidden_states\"] = enc_states.float()\n",
        "\n",
        "    # deterministic flags\n",
        "    if \"use_cache\" in params:\n",
        "        const_kwargs[\"use_cache\"] = False\n",
        "    if \"output_attentions\" in params:\n",
        "        const_kwargs[\"output_attentions\"] = False\n",
        "    if \"return_dict\" in params:\n",
        "        const_kwargs[\"return_dict\"] = False\n",
        "\n",
        "    def _f(x):\n",
        "        x = x.float()\n",
        "        q_len = x.size(1)\n",
        "        B     = x.size(0)\n",
        "        kwargs = {\"hidden_states\": x}\n",
        "\n",
        "        # self mask\n",
        "        kwargs[\"attention_mask\"] = _ensure_mask_shape(\n",
        "            attn_mask_src, q_len, q_len, x.device, x.dtype, is_self=True, B_fallback=B\n",
        "        )\n",
        "\n",
        "        # cross (optional)\n",
        "        if \"encoder_hidden_states\" in const_kwargs:\n",
        "            kv = const_kwargs[\"encoder_hidden_states\"]\n",
        "            k_len = kv.size(1)\n",
        "            kwargs[\"encoder_hidden_states\"] = kv\n",
        "            kwargs[\"encoder_attention_mask\"] = _ensure_mask_shape(\n",
        "                enc_mask_src, q_len, k_len, x.device, x.dtype, is_self=False, B_fallback=B\n",
        "            )\n",
        "\n",
        "        # positional args (version-dependent)\n",
        "        if \"cache_position\" in params:\n",
        "            kwargs[\"cache_position\"] = torch.arange(q_len, dtype=torch.long, device=x.device)\n",
        "        if \"query_length\" in params and \"cache_position\" not in params:\n",
        "            kwargs[\"query_length\"] = q_len\n",
        "\n",
        "        kwargs.update(const_kwargs)\n",
        "        kwargs = _filter_kwargs_for_module(block, kwargs)\n",
        "\n",
        "        # Use math SDP to keep higher-order derivatives available\n",
        "        with _sdp_math_ctx():\n",
        "            out = block(**kwargs)\n",
        "\n",
        "        hs = out[0] if isinstance(out, (tuple, list)) else out\n",
        "        return hs\n",
        "\n",
        "    return _f\n",
        "\n",
        "def register_decoder_block_jvp_hooks(model):\n",
        "    \"\"\"\n",
        "    Capture per-decoder-block incoming hidden_states + kwargs with forward_pre_hook.\n",
        "    \"\"\"\n",
        "    dec_blocks = model.decoder.block\n",
        "    dec_bufs = {i: {\"X\": None, \"args\": None} for i in range(len(dec_blocks))}\n",
        "    dec_hooks = []\n",
        "    for i, block in enumerate(dec_blocks):\n",
        "        def pre_hook(module, args, kwargs, idx=i):\n",
        "            X = kwargs.get(\"hidden_states\", args[0] if len(args) > 0 else None)\n",
        "            dec_bufs[idx][\"X\"]    = None if X is None else X.detach()\n",
        "            dec_bufs[idx][\"args\"] = _map_args_from_hook(module, args, kwargs)\n",
        "        dec_hooks.append(block.register_forward_pre_hook(pre_hook, with_kwargs=True))\n",
        "    return dec_hooks, dec_bufs\n",
        "\n",
        "def remove_hooks(hooks):\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "def compute_decoder_block_jdev_autograd(model, dec_bufs, k_probes=2, rademacher=True):\n",
        "    \"\"\"\n",
        "    Autograd/func JVP estimator of E_v ||(J - I)v||^2 for each decoder block.\n",
        "    - rademacher=True uses ±1 probes; else Gaussian.\n",
        "    - Returns dict {layer_idx: score}.\n",
        "    \"\"\"\n",
        "    # Prefer torch.func.jvp in PyTorch 2+, else fall back to autograd.functional.jvp\n",
        "    try:\n",
        "        from torch.func import jvp as func_jvp\n",
        "        use_func = True\n",
        "    except Exception:\n",
        "        from torch.autograd.functional import jvp as autograd_jvp\n",
        "        use_func = False\n",
        "\n",
        "    scores = {}\n",
        "\n",
        "    for idx, buf in dec_bufs.items():\n",
        "        X, args = buf[\"X\"], buf[\"args\"]\n",
        "        # clear buffers early to limit memory\n",
        "        buf[\"X\"], buf[\"args\"] = None, None\n",
        "\n",
        "        if X is None:\n",
        "            continue\n",
        "\n",
        "        block = model.decoder.block[idx]\n",
        "        was_training = block.training\n",
        "        block.eval()  # no dropout for determinism\n",
        "\n",
        "        try:\n",
        "            f = _make_block_callable(block, args)\n",
        "            x0 = X.detach().requires_grad_(True).float()\n",
        "\n",
        "            # quick health check\n",
        "            try:\n",
        "                with _sdp_math_ctx():\n",
        "                    y0 = f(x0)\n",
        "            except Exception:\n",
        "                continue\n",
        "            if y0 is None or not torch.isfinite(y0).all():\n",
        "                continue\n",
        "\n",
        "            acc, used = 0.0, 0\n",
        "            for _ in range(max(1, k_probes)):\n",
        "                if rademacher:\n",
        "                    v = torch.empty_like(x0).bernoulli_(0.5).mul_(2).sub_(1)\n",
        "                else:\n",
        "                    v = torch.randn_like(x0)\n",
        "\n",
        "                try:\n",
        "                    if use_func:\n",
        "                        _, Jv = func_jvp(f, (x0,), (v,))\n",
        "                    else:\n",
        "                        _, Jv = autograd_jvp(f, (x0,), (v,), create_graph=False, strict=False)\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "                if Jv is None or not torch.isfinite(Jv).all():\n",
        "                    continue\n",
        "\n",
        "                jd = Jv - v\n",
        "                if not torch.isfinite(jd).all():\n",
        "                    continue\n",
        "\n",
        "                acc += float(jd.pow(2).mean().item())\n",
        "                used += 1\n",
        "\n",
        "            if used > 0:\n",
        "                scores[idx] = acc / used\n",
        "        finally:\n",
        "            block.train(was_training)\n",
        "\n",
        "    return scores\n",
        "\n",
        "# ===========================\n",
        "# 5) Pruning Utilities (decoder blocks)\n",
        "# ===========================\n",
        "class SkipBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Identity replacement for a T5 decoder block.\n",
        "    Return None for biases so later layers recompute them with correct (Q,K) sizes.\n",
        "    Keeps tuple shape:\n",
        "    (hidden_states, present_key_value, self_attn_weights, cross_attn_weights,\n",
        "     position_bias, encoder_decoder_position_bias)\n",
        "    \"\"\"\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        position_bias=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        encoder_decoder_position_bias=None,\n",
        "        layer_head_mask=None,\n",
        "        cross_attn_layer_head_mask=None,\n",
        "        past_key_value=None,\n",
        "        use_cache=False,\n",
        "        output_attentions=False,\n",
        "        return_dict=False,\n",
        "        cache_position=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        return (hidden_states, None, None, None, None, None)\n",
        "\n",
        "def prune_decoder_blocks_by_jdev(blocks, jdev_scores, num_prune=4, protect_first=True, protect_last=False, verbose=True):\n",
        "    \"\"\"\n",
        "    Prune LOW J-dev (near-identity) decoder blocks.\n",
        "    \"\"\"\n",
        "    if not jdev_scores:\n",
        "        if verbose: print(\"No J-dev scores available; skipping pruning.\")\n",
        "        return []\n",
        "    items = list(jdev_scores.items())\n",
        "    if protect_first:\n",
        "        items = [(i, s) for i, s in items if i != 0]\n",
        "    if protect_last:\n",
        "        items = [(i, s) for i, s in items if i != len(blocks) - 1]\n",
        "    items.sort(key=lambda x: x[1])  # lowest first\n",
        "    prune_idxs = [i for i, _ in items[:max(0, num_prune)]]\n",
        "    for i in prune_idxs:\n",
        "        blocks[i] = SkipBlock()\n",
        "    if verbose:\n",
        "        print(f\"Pruned decoder blocks (lowest J-dev): {prune_idxs}\")\n",
        "    return prune_idxs\n",
        "\n",
        "# ===========================\n",
        "# 6) Eval Helper\n",
        "# ===========================\n",
        "def compute_accuracy(preds, refs):\n",
        "    correct = 0\n",
        "    for p, l in zip(preds, refs):\n",
        "        if str(p).strip().lower() == str(l).strip().lower():\n",
        "            correct += 1\n",
        "    return correct / len(preds) if len(preds) > 0 else 0.0\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, dl, tokenizer, device):\n",
        "    model.config.use_cache = False  # recompute biases fresh\n",
        "    model.eval()\n",
        "    preds, refs = [], []\n",
        "    for batch in dl:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        # decode gold labels to text\n",
        "        label_ids = batch[\"labels\"].clone()\n",
        "        label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "        ref_texts = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=8,\n",
        "            use_cache=False  # consistent with SkipBlock\n",
        "        )\n",
        "        pred_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "        preds.extend([p.strip().lower() for p in pred_texts])\n",
        "        refs.extend([l.strip().lower() for l in ref_texts])\n",
        "    return compute_accuracy(preds, refs)\n",
        "\n",
        "# ===========================\n",
        "# 7) Training + Autograd-JVP + Pruning\n",
        "# ===========================\n",
        "def full_finetuning_with_jdev(train_loader, dev_loader, device, tokenizer,\n",
        "                              jvp_k=2, jvp_every=1, rademacher=True, epochs=6):\n",
        "    \"\"\"\n",
        "    Stage 1: full FT while periodically collecting Autograd-JVP J-dev for decoder blocks.\n",
        "    \"\"\"\n",
        "    print(\"=== Stage 1: Full FT + Autograd-JVP (Decoder Blocks) ===\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n",
        "    model.config.use_cache = False\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "    dec_hooks, dec_bufs = register_decoder_block_jvp_hooks(model)\n",
        "    dec_sum, dec_cnt = defaultdict(float), defaultdict(int)\n",
        "    step = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            step += 1\n",
        "            opt.zero_grad()\n",
        "\n",
        "            out = model(\n",
        "                input_ids=batch[\"input_ids\"].to(device),\n",
        "                attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                labels=batch[\"labels\"].to(device),\n",
        "            )\n",
        "            loss = out.loss\n",
        "            if not torch.isfinite(loss):\n",
        "                print(\"Loss is NaN/Inf — skipping batch.\")\n",
        "                continue\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "\n",
        "            # Collect JVP every N steps\n",
        "            if step % max(1, jvp_every) == 0:\n",
        "                dec_scores = compute_decoder_block_jdev_autograd(\n",
        "                    model, dec_bufs, k_probes=jvp_k, rademacher=rademacher\n",
        "                )\n",
        "                for i, v in dec_scores.items():\n",
        "                    if math.isfinite(v):\n",
        "                        dec_sum[i] += v\n",
        "                        dec_cnt[i] += 1\n",
        "\n",
        "        epoch_dec = {i: dec_sum[i] / dec_cnt[i] for i in dec_sum if dec_cnt[i] > 0}\n",
        "        print(f\"[Epoch {epoch+1}] Decoder Block J-dev (mean): {epoch_dec}\")\n",
        "        acc = evaluate_model(model, dev_loader, tokenizer, device)\n",
        "        print(f\"[Epoch {epoch+1}] Dev Acc: {acc:.4f}\")\n",
        "\n",
        "    remove_hooks(dec_hooks)\n",
        "    final_dec = {i: dec_sum[i] / dec_cnt[i] for i in dec_sum if dec_cnt[i] > 0}\n",
        "    return model, final_dec\n",
        "\n",
        "def prune_and_finetuning(model, train_loader, dev_loader, device, tokenizer,\n",
        "                         dec_jdev_scores, num_prune=4, epochs=5):\n",
        "    print(\"=== Stage 2: Prune LOW-Jdev Decoder Blocks & Fine-tuning ===\")\n",
        "    _ = prune_decoder_blocks_by_jdev(\n",
        "        model.decoder.block, dec_jdev_scores, num_prune=num_prune,\n",
        "        protect_first=True, protect_last=False, verbose=True\n",
        "    )\n",
        "\n",
        "    model.config.use_cache = False\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            opt.zero_grad()\n",
        "            out = model(\n",
        "                input_ids=batch[\"input_ids\"].to(device),\n",
        "                attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                labels=batch[\"labels\"].to(device),\n",
        "            )\n",
        "            loss = out.loss\n",
        "            if not torch.isfinite(loss):\n",
        "                print(\"Loss is NaN/Inf — skipping batch.\")\n",
        "                continue\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "\n",
        "        acc = evaluate_model(model, dev_loader, tokenizer, device)\n",
        "        print(f\"[Prune FT Epoch {epoch+1}] SVAMP Acc: {acc:.4f}\")\n",
        "    return model\n",
        "\n",
        "# ===========================\n",
        "# 8) Main Entrypoint\n",
        "# ===========================\n",
        "def main():\n",
        "    model, dec_jdev_scores = full_finetuning_with_jdev(\n",
        "        train_loader, dev_loader, device, tokenizer,\n",
        "        jvp_k=2, jvp_every=1, rademacher=True, epochs=6\n",
        "    )\n",
        "    model = prune_and_finetuning(\n",
        "        model, train_loader, dev_loader, device, tokenizer,\n",
        "        dec_jdev_scores, num_prune=num, epochs=5\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3wDNkD86y-D"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}