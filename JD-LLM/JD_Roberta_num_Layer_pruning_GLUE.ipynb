{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ENR6EQ2b8R8",
        "outputId": "35b50709-ea68-49c2-dd1e-2e100a8d11aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: datasets 4.0.0\n",
            "Uninstalling datasets-4.0.0:\n",
            "  Successfully uninstalled datasets-4.0.0\n",
            "Collecting datasets==2.18.0\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (18.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets==2.18.0)\n",
            "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (0.70.16)\n",
            "Collecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0)\n",
            "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (3.13.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0) (6.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0) (1.22.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.4->datasets==2.18.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.4->datasets==2.18.0) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.18.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.18.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.18.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.18.0) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.18.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.18.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.18.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0) (1.17.0)\n",
            "Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: pyarrow-hotfix, fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.18.0 fsspec-2024.2.0 pyarrow-hotfix-0.7\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (0.7)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.13.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.6\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y datasets\n",
        "!pip install datasets==2.18.0\n",
        "!pip install evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num = 6  # Define the number of Layers to be pruned"
      ],
      "metadata": {
        "id": "yL6FkPZ26Fx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prune Layers Based on Jacobian Deviation (||J - I||) Using MNLI Dataset\n",
        "# FD\n",
        "\n",
        "# ========================================================\n",
        "# 1) Standard imports and warning suppression\n",
        "# ========================================================\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from transformers import (\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizerFast,\n",
        "    DataCollatorWithPadding,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "import evaluate\n",
        "import math\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# ========================================================\n",
        "# 2) Jacobian Deviation Hooks/Scoring\n",
        "#    Compare each layer's mapping f_ℓ via ||J_ℓ - I|| (Hutchinson-style)\n",
        "# ========================================================\n",
        "\n",
        "def _flatten_tokens(x):\n",
        "    # x: (batch, seq, hidden) or (batch, hidden)\n",
        "    if x.dim() == 3:\n",
        "        b, s, h = x.shape\n",
        "        return x.reshape(b * s, h)\n",
        "    elif x.dim() == 2:\n",
        "        return x\n",
        "    return x.view(x.size(0), -1)\n",
        "\n",
        "def register_jac_hooks(model):\n",
        "    \"\"\"\n",
        "    Capture each RobertaLayer's block input (pre), output (post), and non-input args.\n",
        "    We'll call the layer again locally to estimate JVPs via finite differences.\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    # store X, Y, args (everything after hidden_states), and a flag\n",
        "    activations = {i: {'X': None, 'Y': None, 'args': None} for i in range(len(layers))}\n",
        "    hooks = []\n",
        "\n",
        "    for i, layer in enumerate(layers):\n",
        "        def pre_hook(module, inputs, idx=i):\n",
        "            # inputs: (hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, ...)\n",
        "            xs = inputs[0]\n",
        "            extra = tuple(inputs[1:]) if len(inputs) > 1 else tuple()\n",
        "            activations[idx]['X'] = xs.detach()\n",
        "            # args can contain None/bool/tensors; keep as-is (no detach on non-tensors)\n",
        "            activations[idx]['args'] = extra\n",
        "\n",
        "        def post_hook(module, inputs, output, idx=i):\n",
        "            y = output[0] if isinstance(output, tuple) else output\n",
        "            activations[idx]['Y'] = y.detach()\n",
        "\n",
        "        hooks.append(layer.register_forward_pre_hook(pre_hook))\n",
        "        hooks.append(layer.register_forward_hook(post_hook))\n",
        "\n",
        "    return hooks, activations\n",
        "\n",
        "def remove_hooks(hooks):\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "@torch.no_grad()\n",
        "def _layer_forward_only(layer, x, args):\n",
        "    out = layer(x, *args)\n",
        "    return out[0] if isinstance(out, tuple) else out\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_batch_jacdev(model, acts, eps=1e-3, k_probes=1):\n",
        "    \"\"\"\n",
        "    For each layer, estimate ||J - I||_F^2 via finite-difference JVPs:\n",
        "      (J - I)v ≈ (f(x + eps*v) - f(x))/eps - v\n",
        "    We average squared norms over k_probes random v and all elements.\n",
        "    Returns dict: {layer_idx: score}\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    scores = {}\n",
        "\n",
        "    for idx, buf in acts.items():\n",
        "        X = buf['X']; Y = buf['Y']; args = buf['args']\n",
        "        if X is None or Y is None:\n",
        "            continue\n",
        "\n",
        "        layer = layers[idx]\n",
        "        was_training = layer.training\n",
        "        layer.eval()  # disable dropout for stable finite-diff\n",
        "        try:\n",
        "            # Recompute base output with dropout off to pair with the eps-perturbed run\n",
        "            y0 = _layer_forward_only(layer, X, args)\n",
        "\n",
        "            # Random-probe average\n",
        "            acc = 0.0\n",
        "            for _ in range(k_probes):\n",
        "                v = torch.randn_like(X)\n",
        "                y_eps = _layer_forward_only(layer, X + eps * v, args)\n",
        "                jd_vec = (y_eps - y0) / eps - v\n",
        "                acc += float(jd_vec.pow(2).mean().item())\n",
        "            scores[idx] = acc / k_probes\n",
        "        finally:\n",
        "            layer.train(was_training)\n",
        "\n",
        "        # Clear buffers so we don't reuse stale tensors\n",
        "        buf['X'] = None\n",
        "        buf['Y'] = None\n",
        "        buf['args'] = None\n",
        "\n",
        "    return scores\n",
        "\n",
        "# ========================================================\n",
        "# 3) Pruning Utilities with SkipFF (prune low-JacDev)\n",
        "# ========================================================\n",
        "class SkipFF(nn.Module):\n",
        "    def forward(self, hidden_states, input_tensor=None):\n",
        "        # Pass-through the residual (identity)\n",
        "        return input_tensor\n",
        "\n",
        "def prune_jac_layers(model, jac_scores, num_prune=4):\n",
        "    \"\"\"\n",
        "    Prune layers with the *lowest* Jacobian deviation (closest to identity).\n",
        "    We remove the FFN by replacing intermediate.dense with Identity\n",
        "    and the output block with SkipFF (residual passthrough).\n",
        "    \"\"\"\n",
        "    sorted_layers = sorted(jac_scores.items(), key=lambda x: x[1], reverse=False)  # lowest first\n",
        "    prune_idxs = [idx for idx, _ in sorted_layers[:num_prune]]\n",
        "    for idx in prune_idxs:\n",
        "        layer = model.roberta.encoder.layer[idx]\n",
        "        layer.intermediate.dense = nn.Identity()\n",
        "        layer.output = SkipFF()\n",
        "    return prune_idxs\n",
        "\n",
        "# ========================================================\n",
        "# 4) LoRA Modules (unchanged)\n",
        "# ========================================================\n",
        "class LoRA(nn.Module):\n",
        "    def __init__(self, W0, r=2, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"W0\", W0.clone().detach())\n",
        "        L, M = W0.shape\n",
        "        self.B = nn.Parameter(torch.randn(L, r) * 0.01)\n",
        "        self.A = nn.Parameter(torch.zeros(r, M))\n",
        "        self.scaling = alpha / r\n",
        "    def forward(self):\n",
        "        return self.W0 + self.scaling * (self.B @ self.A)\n",
        "\n",
        "def apply_lora_to_all_layers(model, r=2, alpha=1.0):\n",
        "    loras = {}\n",
        "    for idx, layer in enumerate(model.roberta.encoder.layer):\n",
        "        if not hasattr(layer.output, 'dense'):\n",
        "            continue\n",
        "        W0 = layer.output.dense.weight.data\n",
        "        lora = LoRA(W0, r, alpha).to(W0.device)\n",
        "        def fwd(x, layer=layer, lora=lora):\n",
        "            return F.linear(x, lora(), layer.output.dense.bias)\n",
        "        layer.output.dense.forward = fwd\n",
        "        loras[idx] = lora\n",
        "    return loras\n",
        "\n",
        "# ========================================================\n",
        "# 5) Data + Eval Helpers\n",
        "# ========================================================\n",
        "def preprocess_function(examples, tok, max_length=128):\n",
        "    return tok(examples['premise'],\n",
        "               examples['hypothesis'],\n",
        "               truncation=True,\n",
        "               padding='max_length',\n",
        "               max_length=max_length)\n",
        "\n",
        "def evaluate_model(model, dl, device):\n",
        "    model.eval()\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "    preds, labs = [], []\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids = b['input_ids'].to(device)\n",
        "            mask = b['attention_mask'].to(device)\n",
        "            labs.extend(b['labels'].cpu().numpy())\n",
        "            out = model(input_ids=ids, attention_mask=mask)\n",
        "            preds.extend(torch.argmax(out.logits, -1).cpu().numpy())\n",
        "    return metric.compute(predictions=preds, references=labs)[\"accuracy\"]\n",
        "\n",
        "# ========================================================\n",
        "# 6) Training Stages (Jacobian-Deviation scoring)\n",
        "# ========================================================\n",
        "def full_finetuning(train_loader, dev_loader, device):\n",
        "    print(\"=== Stage 1: Full Finetuning & Jacobian-Deviation Estimation ===\")\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=3).to(device)\n",
        "    model.gradient_checkpointing_enable()\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "    sched = get_linear_schedule_with_warmup(opt, 0, len(train_loader)*3)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    hooks, activations = register_jac_hooks(model)\n",
        "\n",
        "    last_jac = None\n",
        "    for epoch in range(6):\n",
        "        jac_sums, jac_counts = defaultdict(float), defaultdict(int)\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                out = model(\n",
        "                    input_ids=b['input_ids'].to(device),\n",
        "                    attention_mask=b['attention_mask'].to(device),\n",
        "                    labels=b['labels'].to(device),\n",
        "                )\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "\n",
        "            # Per-batch Jacobian deviation scores\n",
        "            batch_jac = compute_batch_jacdev(model, activations, eps=1e-3, k_probes=1)\n",
        "            for idx, v in batch_jac.items():\n",
        "                jac_sums[idx] += v\n",
        "                jac_counts[idx] += 1\n",
        "\n",
        "        epoch_jac = {idx: (jac_sums[idx] / max(1, jac_counts[idx])) for idx in jac_sums}\n",
        "        print(f\"[Epoch {epoch+1}] approx ||J-I||^2:\", {k: round(v, 6) for k, v in epoch_jac.items()})\n",
        "        last_jac = epoch_jac\n",
        "\n",
        "    acc = evaluate_model(model, dev_loader, device)\n",
        "    print(f\"-> Full Finetune MNLI Acc: {acc:.4f}\")\n",
        "    remove_hooks(hooks)\n",
        "    return model, last_jac\n",
        "\n",
        "def prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores):\n",
        "    print(\"=== Stage 2: Prune (Low-||J-I||) & Finetuning ===\")\n",
        "    prune_idxs = prune_jac_layers(model, jac_scores, num_prune=num)\n",
        "    print(\"Pruned layers (lowest ||J-I||):\", prune_idxs)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "    sched = get_linear_schedule_with_warmup(opt, 0, len(train_loader)*3)\n",
        "    for epoch in range(3):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            out = model(\n",
        "                input_ids=b['input_ids'].to(device),\n",
        "                attention_mask=b['attention_mask'].to(device),\n",
        "                labels=b['labels'].to(device),\n",
        "            )\n",
        "            out.loss.backward()\n",
        "            opt.step()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[Prune FT Epoch {epoch+1}] MNLI Acc: {acc:.4f}\")\n",
        "    return model\n",
        "\n",
        "def lora_only_finetuning(model, train_loader, dev_loader, device, r=2, alpha=1.0):\n",
        "    print(\"=== Stage 3: LoRA Finetuning ===\")\n",
        "    torch.cuda.empty_cache()\n",
        "    loras = apply_lora_to_all_layers(model, r, alpha)\n",
        "    for p in model.roberta.parameters():\n",
        "        p.requires_grad = False\n",
        "    for p in model.classifier.parameters():\n",
        "        p.requires_grad = True\n",
        "    for l in loras.values():\n",
        "        l.A.requires_grad = True\n",
        "        l.B.requires_grad = True\n",
        "\n",
        "    opt = torch.optim.Adam(\n",
        "        list(model.classifier.parameters()) + [p for l in loras.values() for p in (l.A, l.B)],\n",
        "        lr=2e-5\n",
        "    )\n",
        "    sched = get_linear_schedule_with_warmup(opt, 0, len(train_loader)*3)\n",
        "    scaler = GradScaler()\n",
        "    for epoch in range(3):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                out = model(\n",
        "                    input_ids=b['input_ids'].to(device),\n",
        "                    attention_mask=b['attention_mask'].to(device),\n",
        "                    labels=b['labels'].to(device),\n",
        "                )\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[LoRA Epoch {epoch+1}] MNLI Acc: {acc:.4f}\")\n",
        "\n",
        "# ========================================================\n",
        "# 7) Main Entrypoint\n",
        "# ========================================================\n",
        "def main():\n",
        "    seed = 42\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_ds = load_dataset(\"glue\", \"mnli\", split=\"train[:5000]\").shuffle(seed)\n",
        "    dev_ds   = load_dataset(\"glue\", \"mnli\", split=\"validation_matched[:1000]\")\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "\n",
        "    train = train_ds.map(lambda ex: preprocess_function(ex, tokenizer),\n",
        "                         batched=True,\n",
        "                         remove_columns=[\"premise\",\"hypothesis\",\"idx\"]) \\\n",
        "                    .rename_column(\"label\",\"labels\")\n",
        "    dev   = dev_ds.map(lambda ex: preprocess_function(ex, tokenizer),\n",
        "                       batched=True,\n",
        "                       remove_columns=[\"premise\",\"hypothesis\",\"idx\"]) \\\n",
        "                  .rename_column(\"label\",\"labels\")\n",
        "\n",
        "    collator = DataCollatorWithPadding(tokenizer, padding=\"max_length\", max_length=64)\n",
        "    train_loader = DataLoader(train, batch_size=8, shuffle=True,  collate_fn=collator)\n",
        "    dev_loader   = DataLoader(dev,   batch_size=16, shuffle=False, collate_fn=collator)\n",
        "\n",
        "    model, jac_scores = full_finetuning(train_loader, dev_loader, device)\n",
        "    model = prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores)\n",
        "    lora_only_finetuning(model, train_loader, dev_loader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "COvSncgs29xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aEY3Kc0ll6tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prune Layers Based on Jacobian Deviation (||J - I||) Using MNLI Dataset\n",
        "# - Autograd JVP scorer with SDPA-math fallback, AD\n",
        "# - Finite-difference fallback if higher-order grads are unavailable\n",
        "\n",
        "# ========================================================\n",
        "# 1) Standard imports and warning suppression\n",
        "# ========================================================\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from transformers import (\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizerFast,\n",
        "    DataCollatorWithPadding,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "import evaluate\n",
        "import math\n",
        "import warnings\n",
        "from contextlib import contextmanager\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# ========================================================\n",
        "# 1.1) Utility: force 'math' attention kernels during JVP\n",
        "# ========================================================\n",
        "@contextmanager\n",
        "def force_math_sdp():\n",
        "    \"\"\"\n",
        "    Force 'math' scaled-dot-product attention so higher-order grads/JVP work.\n",
        "    On CPU, this is a no-op.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n",
        "            yield\n",
        "    else:\n",
        "        yield\n",
        "\n",
        "# ========================================================\n",
        "# 2) Jacobian Deviation Hooks/Scoring\n",
        "#    Compare each layer's mapping f_ℓ via ||J_ℓ - I|| (Hutchinson-style)\n",
        "# ========================================================\n",
        "\n",
        "def _flatten_tokens(x):\n",
        "    # x: (batch, seq, hidden) or (batch, hidden)\n",
        "    if x.dim() == 3:\n",
        "        b, s, h = x.shape\n",
        "        return x.reshape(b * s, h)\n",
        "    elif x.dim() == 2:\n",
        "        return x\n",
        "    return x.view(x.size(0), -1)\n",
        "\n",
        "def register_jac_hooks(model):\n",
        "    \"\"\"\n",
        "    Capture each RobertaLayer's block input (pre), output (post), and non-input args.\n",
        "    We'll call the layer again locally to estimate JVPs or finite-diff JVPs.\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    activations = {i: {'X': None, 'Y': None, 'args': None} for i in range(len(layers))}\n",
        "    hooks = []\n",
        "\n",
        "    for i, layer in enumerate(layers):\n",
        "        def pre_hook(module, inputs, idx=i):\n",
        "            # inputs: (hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, ...)\n",
        "            xs = inputs[0]\n",
        "            extra = tuple(inputs[1:]) if len(inputs) > 1 else tuple()\n",
        "            activations[idx]['X'] = xs.detach()\n",
        "            activations[idx]['args'] = extra\n",
        "\n",
        "        def post_hook(module, inputs, output, idx=i):\n",
        "            y = output[0] if isinstance(output, tuple) else output\n",
        "            activations[idx]['Y'] = y.detach()\n",
        "\n",
        "        hooks.append(layer.register_forward_pre_hook(pre_hook))\n",
        "        hooks.append(layer.register_forward_hook(post_hook))\n",
        "\n",
        "    return hooks, activations\n",
        "\n",
        "def remove_hooks(hooks):\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "@torch.no_grad()\n",
        "def _layer_forward_only(layer, x, args):\n",
        "    out = layer(x, *args)\n",
        "    return out[0] if isinstance(out, tuple) else out\n",
        "\n",
        "# -------- Autograd JVP scorer (no finite differences) --------\n",
        "def compute_batch_jacdev_autograd(model, acts, k_probes=1, rademacher=True, max_tokens=None):\n",
        "    \"\"\"\n",
        "    Exact JVP version (no finite differences).\n",
        "    For each layer l, estimate E_v ||(J_l - I) v||^2 over k_probes random vectors.\n",
        "      - If rademacher=True: v ~ {+1,-1}\n",
        "      - Else: v ~ N(0, I)\n",
        "      - Optionally subsample tokens to reduce compute: max_tokens keeps at most that many (B*S) rows.\n",
        "    Returns: {layer_idx: score}\n",
        "    \"\"\"\n",
        "    from torch.autograd.functional import jvp\n",
        "\n",
        "    layers = model.roberta.encoder.layer\n",
        "    scores = {}\n",
        "\n",
        "    for idx, buf in acts.items():\n",
        "        X, args = buf['X'], buf['args']\n",
        "        if X is None:\n",
        "            continue\n",
        "\n",
        "        # Optional token subsampling for speed\n",
        "        if X.dim() == 3:\n",
        "            b, s, h = X.shape\n",
        "            X2 = X.reshape(b*s, h)\n",
        "            if max_tokens is not None and X2.size(0) > max_tokens:\n",
        "                sel = torch.randperm(X2.size(0), device=X2.device)[:max_tokens]\n",
        "                X = X2[sel].unsqueeze(0)  # (1, max_tokens, H)\n",
        "                if len(args) > 0 and isinstance(args[0], torch.Tensor):\n",
        "                    attn = torch.ones((1, X.size(1)), dtype=args[0].dtype, device=X.device)\n",
        "                    args = (attn,) + args[1:]\n",
        "\n",
        "        X = X.detach().requires_grad_(True)\n",
        "        layer = layers[idx]\n",
        "        was_training = layer.training\n",
        "        layer.eval()\n",
        "\n",
        "        try:\n",
        "            def f(inp):\n",
        "                out = layer(inp, *args)\n",
        "                return out[0] if isinstance(out, tuple) else out\n",
        "\n",
        "            acc = 0.0\n",
        "            # Force math attention so JVP has higher-order grad support\n",
        "            with force_math_sdp():\n",
        "                for _ in range(k_probes):\n",
        "                    if rademacher:\n",
        "                        v = torch.empty_like(X).bernoulli_(0.5).mul_(2).sub_(1)  # ±1\n",
        "                    else:\n",
        "                        v = torch.randn_like(X)\n",
        "\n",
        "                    # JVP: returns (f(X), Jv)\n",
        "                    _, Jv = jvp(f, (X,), (v,), create_graph=False, strict=True)\n",
        "                    jd_vec = Jv - v\n",
        "                    acc += float(jd_vec.pow(2).mean().item())\n",
        "\n",
        "            scores[idx] = acc / k_probes\n",
        "\n",
        "        finally:\n",
        "            layer.train(was_training)\n",
        "\n",
        "        # Clear to avoid stale tensors\n",
        "        buf['X'] = None\n",
        "        buf['Y'] = None\n",
        "        buf['args'] = None\n",
        "\n",
        "    return scores\n",
        "\n",
        "# -------- Finite-difference fallback scorer --------\n",
        "@torch.no_grad()\n",
        "def compute_batch_jacdev_fdiff(model, acts, eps=1e-3, k_probes=1):\n",
        "    \"\"\"\n",
        "    Finite differences: (J - I)v ≈ (f(x + eps*v) - f(x))/eps - v\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    scores = {}\n",
        "\n",
        "    for idx, buf in acts.items():\n",
        "        X, args = buf['X'], buf['args']\n",
        "        if X is None:\n",
        "            continue\n",
        "\n",
        "        layer = layers[idx]\n",
        "        was_training = layer.training\n",
        "        layer.eval()\n",
        "        try:\n",
        "            y0 = _layer_forward_only(layer, X, args)\n",
        "            acc = 0.0\n",
        "            for _ in range(k_probes):\n",
        "                v = torch.randn_like(X)\n",
        "                y_eps = _layer_forward_only(layer, X + eps * v, args)\n",
        "                jd_vec = (y_eps - y0) / eps - v\n",
        "                acc += float(jd_vec.pow(2).mean().item())\n",
        "            scores[idx] = acc / k_probes\n",
        "        finally:\n",
        "            layer.train(was_training)\n",
        "\n",
        "        buf['X'] = None\n",
        "        buf['Y'] = None\n",
        "        buf['args'] = None\n",
        "\n",
        "    return scores\n",
        "\n",
        "# -------- Safe wrapper that tries JVP then falls back --------\n",
        "def compute_batch_jacdev_safe(model, activations, **kw):\n",
        "    try:\n",
        "        return compute_batch_jacdev_autograd(model, activations, **kw)\n",
        "    except RuntimeError as e:\n",
        "        # Efficient SDPA kernel lacks higher-order grads -> fallback\n",
        "        if \"scaled_dot_product_efficient_attention_backward\" in str(e):\n",
        "            return compute_batch_jacdev_fdiff(model, activations, eps=1e-3, k_probes=max(1, kw.get(\"k_probes\", 1)))\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "# ========================================================\n",
        "# 3) Pruning Utilities with SkipFF (prune low-JacDev)\n",
        "# ========================================================\n",
        "class SkipFF(nn.Module):\n",
        "    def forward(self, hidden_states, input_tensor=None):\n",
        "        # Pass-through the residual (identity)\n",
        "        return input_tensor\n",
        "\n",
        "def prune_jac_layers(model, jac_scores, num_prune=4):\n",
        "    \"\"\"\n",
        "    Prune layers with the *lowest* Jacobian deviation (closest to identity).\n",
        "    We remove the FFN by replacing intermediate.dense with Identity\n",
        "    and the output block with SkipFF (residual passthrough).\n",
        "    \"\"\"\n",
        "    sorted_layers = sorted(jac_scores.items(), key=lambda x: x[1], reverse=False)  # lowest first\n",
        "    prune_idxs = [idx for idx, _ in sorted_layers[:num_prune]]\n",
        "    for idx in prune_idxs:\n",
        "        layer = model.roberta.encoder.layer[idx]\n",
        "        layer.intermediate.dense = nn.Identity()\n",
        "        layer.output = SkipFF()\n",
        "    return prune_idxs\n",
        "\n",
        "# ========================================================\n",
        "# 4) LoRA Modules (unchanged)\n",
        "# ========================================================\n",
        "class LoRA(nn.Module):\n",
        "    def __init__(self, W0, r=2, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"W0\", W0.clone().detach())\n",
        "        L, M = W0.shape\n",
        "        self.B = nn.Parameter(torch.randn(L, r) * 0.01)\n",
        "        self.A = nn.Parameter(torch.zeros(r, M))\n",
        "        self.scaling = alpha / r\n",
        "    def forward(self):\n",
        "        return self.W0 + self.scaling * (self.B @ self.A)\n",
        "\n",
        "def apply_lora_to_all_layers(model, r=2, alpha=1.0):\n",
        "    loras = {}\n",
        "    for idx, layer in enumerate(model.roberta.encoder.layer):\n",
        "        if not hasattr(layer.output, 'dense'):\n",
        "            continue\n",
        "        W0 = layer.output.dense.weight.data\n",
        "        lora = LoRA(W0, r, alpha).to(W0.device)\n",
        "        def fwd(x, layer=layer, lora=lora):\n",
        "            return F.linear(x, lora(), layer.output.dense.bias)\n",
        "        layer.output.dense.forward = fwd\n",
        "        loras[idx] = lora\n",
        "    return loras\n",
        "\n",
        "# ========================================================\n",
        "# 5) Data + Eval Helpers\n",
        "# ========================================================\n",
        "def preprocess_function(examples, tok, max_length=128):\n",
        "    return tok(examples['premise'],\n",
        "               examples['hypothesis'],\n",
        "               truncation=True,\n",
        "               padding='max_length',\n",
        "               max_length=max_length)\n",
        "\n",
        "def evaluate_model(model, dl, device):\n",
        "    model.eval()\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "    preds, labs = [], []\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids = b['input_ids'].to(device)\n",
        "            mask = b['attention_mask'].to(device)\n",
        "            labs.extend(b['labels'].cpu().numpy())\n",
        "            out = model(input_ids=ids, attention_mask=mask)\n",
        "            preds.extend(torch.argmax(out.logits, -1).cpu().numpy())\n",
        "    return metric.compute(predictions=preds, references=labs)[\"accuracy\"]\n",
        "\n",
        "# ========================================================\n",
        "# 6) Training Stages (Jacobian-Deviation scoring)\n",
        "# ========================================================\n",
        "def full_finetuning(train_loader, dev_loader, device):\n",
        "    print(\"=== Stage 1: Full Finetuning & Jacobian-Deviation Estimation ===\")\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=3).to(device)\n",
        "    model.gradient_checkpointing_enable()\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "    sched = get_linear_schedule_with_warmup(opt, 0, len(train_loader)*3)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    hooks, activations = register_jac_hooks(model)\n",
        "\n",
        "    last_jac = None\n",
        "    for epoch in range(6):\n",
        "        jac_sums, jac_counts = defaultdict(float), defaultdict(int)\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                out = model(\n",
        "                    input_ids=b['input_ids'].to(device),\n",
        "                    attention_mask=b['attention_mask'].to(device),\n",
        "                    labels=b['labels'].to(device),\n",
        "                )\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "\n",
        "            # Per-batch Jacobian deviation scores (safe JVP with fallback)\n",
        "            batch_jac = compute_batch_jacdev_safe(model, activations, k_probes=2, rademacher=True, max_tokens=1024)\n",
        "            for idx, v in batch_jac.items():\n",
        "                jac_sums[idx] += v\n",
        "                jac_counts[idx] += 1\n",
        "\n",
        "        epoch_jac = {idx: (jac_sums[idx] / max(1, jac_counts[idx])) for idx in jac_sums}\n",
        "        print(f\"[Epoch {epoch+1}] approx ||J-I||^2:\", {k: round(v, 6) for k, v in epoch_jac.items()})\n",
        "        last_jac = epoch_jac\n",
        "\n",
        "    acc = evaluate_model(model, dev_loader, device)\n",
        "    print(f\"-> Full Finetune MNLI Acc: {acc:.4f}\")\n",
        "    remove_hooks(hooks)\n",
        "    return model, last_jac\n",
        "\n",
        "def prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores):\n",
        "    print(\"=== Stage 2: Prune (Low-||J-I||) & Finetuning ===\")\n",
        "    prune_idxs = prune_jac_layers(model, jac_scores, num_prune=num)\n",
        "    print(\"Pruned layers (lowest ||J-I||):\", prune_idxs)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "    sched = get_linear_schedule_with_warmup(opt, 0, len(train_loader)*3)\n",
        "    for epoch in range(3):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            out = model(\n",
        "                input_ids=b['input_ids'].to(device),\n",
        "                attention_mask=b['attention_mask'].to(device),\n",
        "                labels=b['labels'].to(device),\n",
        "            )\n",
        "            out.loss.backward()\n",
        "            opt.step()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[Prune FT Epoch {epoch+1}] MNLI Acc: {acc:.4f}\")\n",
        "    return model\n",
        "\n",
        "def lora_only_finetuning(model, train_loader, dev_loader, device, r=2, alpha=1.0):\n",
        "    print(\"=== Stage 3: LoRA Finetuning ===\")\n",
        "    torch.cuda.empty_cache()\n",
        "    loras = apply_lora_to_all_layers(model, r, alpha)\n",
        "    for p in model.roberta.parameters():\n",
        "        p.requires_grad = False\n",
        "    for p in model.classifier.parameters():\n",
        "        p.requires_grad = True\n",
        "    for l in loras.values():\n",
        "        l.A.requires_grad = True\n",
        "        l.B.requires_grad = True\n",
        "\n",
        "    opt = torch.optim.Adam(\n",
        "        list(model.classifier.parameters()) + [p for l in loras.values() for p in (l.A, l.B)],\n",
        "        lr=2e-5\n",
        "    )\n",
        "    sched = get_linear_schedule_with_warmup(opt, 0, len(train_loader)*3)\n",
        "    scaler = GradScaler()\n",
        "    for epoch in range(5):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                out = model(\n",
        "                    input_ids=b['input_ids'].to(device),\n",
        "                    attention_mask=b['attention_mask'].to(device),\n",
        "                    labels=b['labels'].to(device),\n",
        "                )\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[LoRA Epoch {epoch+1}] MNLI Acc: {acc:.4f}\")\n",
        "\n",
        "# ========================================================\n",
        "# 7) Main Entrypoint\n",
        "# ========================================================\n",
        "def main():\n",
        "    seed = 42\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_ds = load_dataset(\"glue\", \"mnli\", split=\"train[:5000]\").shuffle(seed)\n",
        "    dev_ds   = load_dataset(\"glue\", \"mnli\", split=\"validation_matched[:1000]\")\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "\n",
        "    train = train_ds.map(lambda ex: preprocess_function(ex, tokenizer),\n",
        "                         batched=True,\n",
        "                         remove_columns=[\"premise\",\"hypothesis\",\"idx\"]) \\\n",
        "                    .rename_column(\"label\",\"labels\")\n",
        "    dev   = dev_ds.map(lambda ex: preprocess_function(ex, tokenizer),\n",
        "                       batched=True,\n",
        "                       remove_columns=[\"premise\",\"hypothesis\",\"idx\"]) \\\n",
        "                  .rename_column(\"label\",\"labels\")\n",
        "\n",
        "    collator = DataCollatorWithPadding(tokenizer, padding=\"max_length\", max_length=64)\n",
        "    train_loader = DataLoader(train, batch_size=8, shuffle=True,  collate_fn=collator)\n",
        "    dev_loader   = DataLoader(dev,   batch_size=16, shuffle=False, collate_fn=collator)\n",
        "\n",
        "    model, jac_scores = full_finetuning(train_loader, dev_loader, device)\n",
        "    model = prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores)\n",
        "    lora_only_finetuning(model, train_loader, dev_loader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "8vhFI3PGqEKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h10FH65y3epQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WXkh2bo-7qbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Jacobian Deviation, FD\n",
        "\n",
        "# ========================================================\n",
        "# 1) Standard imports and warning suppression\n",
        "# ========================================================\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from transformers import (\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizerFast,\n",
        "    DataCollatorWithPadding,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "import evaluate\n",
        "import math\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 2) Jacobian Deviation / Hook Utilities (per-layer)\n",
        "# ========================================================\n",
        "def remove_hooks(hooks):\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "def register_jac_hooks(model):\n",
        "    \"\"\"\n",
        "    Capture each RobertaLayer's input (hidden_states) and extra args.\n",
        "    We'll locally re-run the layer to estimate JVPs via finite differences.\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    activations = {i: {'X': None, 'args': None} for i in range(len(layers))}\n",
        "    hooks = []\n",
        "\n",
        "    for i, layer in enumerate(layers):\n",
        "        def pre_hook(module, inputs, idx=i):\n",
        "            xs = inputs[0]\n",
        "            extra = tuple(inputs[1:]) if len(inputs) > 1 else tuple()\n",
        "            activations[idx]['X'] = xs.detach()\n",
        "            activations[idx]['args'] = extra\n",
        "\n",
        "        hooks.append(layer.register_forward_pre_hook(pre_hook))\n",
        "\n",
        "    return hooks, activations\n",
        "\n",
        "@torch.no_grad()\n",
        "def _layer_forward_only(layer, x, args):\n",
        "    out = layer(x, *args)\n",
        "    return out[0] if isinstance(out, tuple) else out\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_batch_jacdev(model, acts, eps=1e-3, k_probes=1):\n",
        "    \"\"\"\n",
        "    For each layer ℓ, estimate ||Jℓ - I||_F^2 using finite differences:\n",
        "      (J - I)v ≈ (f(x + eps*v) - f(x))/eps - v\n",
        "    Average over k_probes random v and all elements.\n",
        "    Returns {layer_idx: score}.\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    scores = {}\n",
        "\n",
        "    for idx, buf in acts.items():\n",
        "        X = buf['X']; args = buf['args']\n",
        "        if X is None:\n",
        "            continue\n",
        "\n",
        "        layer = layers[idx]\n",
        "        was_training = layer.training\n",
        "        layer.eval()  # disable dropout for stable estimate\n",
        "        try:\n",
        "            y0 = _layer_forward_only(layer, X, args)\n",
        "\n",
        "            acc = 0.0\n",
        "            for _ in range(k_probes):\n",
        "                v = torch.randn_like(X)\n",
        "                y_eps = _layer_forward_only(layer, X + eps * v, args)\n",
        "                jd_vec = (y_eps - y0) / eps - v\n",
        "                acc += float(jd_vec.pow(2).mean().item())\n",
        "            scores[idx] = acc / k_probes\n",
        "        finally:\n",
        "            layer.train(was_training)\n",
        "\n",
        "        # clear buffers\n",
        "        buf['X'] = None\n",
        "        buf['args'] = None\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 3) Pruning Utilities with SkipFF (prune low-||J-I||)\n",
        "# ========================================================\n",
        "class SkipFF(nn.Module):\n",
        "    def forward(self, hidden_states, input_tensor=None):\n",
        "        return input_tensor\n",
        "\n",
        "def prune_jac_layers(model, jac_scores, num_prune=4):\n",
        "    \"\"\"\n",
        "    Prune layers with the *lowest* Jacobian deviation (closest to identity).\n",
        "    Replace FFN with Identity and use residual passthrough.\n",
        "    \"\"\"\n",
        "    sorted_layers = sorted(jac_scores.items(), key=lambda x: x[1], reverse=False)\n",
        "    prune_idxs = [idx for idx, _ in sorted_layers[:num_prune]]\n",
        "    for idx in prune_idxs:\n",
        "        layer = model.roberta.encoder.layer[idx]\n",
        "        layer.intermediate.dense = nn.Identity()\n",
        "        layer.output = SkipFF()\n",
        "    return prune_idxs\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 4) LoRA Modules (unchanged)\n",
        "# ========================================================\n",
        "class LoRA(nn.Module):\n",
        "    def __init__(self, W0, r=2, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"W0\", W0.clone().detach())\n",
        "        L, M = W0.shape\n",
        "        self.B = nn.Parameter(torch.randn(L, r) * 0.01)\n",
        "        self.A = nn.Parameter(torch.zeros(r, M))\n",
        "        self.scaling = alpha / r\n",
        "    def forward(self):\n",
        "        return self.W0 + self.scaling * (self.B @ self.A)\n",
        "\n",
        "def apply_lora_to_all_layers(model, r=2, alpha=1.0):\n",
        "    loras = {}\n",
        "    for idx, layer in enumerate(model.roberta.encoder.layer):\n",
        "        if not hasattr(layer.output, 'dense'):\n",
        "            continue\n",
        "        W0 = layer.output.dense.weight.data\n",
        "        lora = LoRA(W0, r, alpha).to(W0.device)\n",
        "        def fwd(x, layer=layer, lora=lora):\n",
        "            return F.linear(x, lora(), layer.output.dense.bias)\n",
        "        layer.output.dense.forward = fwd\n",
        "        loras[idx] = lora\n",
        "    return loras\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 5) Data + Eval Helpers\n",
        "# ========================================================\n",
        "def preprocess_function(examples, tok, max_length=64):\n",
        "    return tok(examples['sentence1'],\n",
        "               examples['sentence2'],\n",
        "               truncation=True,\n",
        "               padding='max_length',\n",
        "               max_length=max_length)\n",
        "\n",
        "def evaluate_model(model, dl, device):\n",
        "    model.eval()\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "    preds, labs = [], []\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids = b['input_ids'].to(device)\n",
        "            mask = b['attention_mask'].to(device)\n",
        "            labs.extend(b['labels'].cpu().numpy())\n",
        "            out = model(input_ids=ids, attention_mask=mask)\n",
        "            preds.extend(torch.argmax(out.logits, -1).cpu().numpy())\n",
        "    return metric.compute(predictions=preds, references=labs)[\"accuracy\"]\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 6) Training Stages (using Jacobian deviation)\n",
        "# ========================================================\n",
        "def full_finetuning(train_loader, dev_loader, device):\n",
        "    print(\"=== Stage 1: Full Finetuning & Jacobian-Deviation Estimation ===\")\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\n",
        "        \"roberta-base\", num_labels=2\n",
        "    ).to(device)\n",
        "    model.gradient_checkpointing_enable()\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "    sched = get_linear_schedule_with_warmup(\n",
        "        opt, num_warmup_steps=0, num_training_steps=len(train_loader)*6\n",
        "    )\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    hooks, activations = register_jac_hooks(model)\n",
        "    last_jac = None\n",
        "\n",
        "    for epoch in range(6):\n",
        "        jac_sums, jac_counts = defaultdict(float), defaultdict(int)\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad()\n",
        "            with autocast():\n",
        "                out = model(input_ids=b['input_ids'].to(device),\n",
        "                            attention_mask=b['attention_mask'].to(device),\n",
        "                            labels=b['labels'].to(device))\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "\n",
        "            # per-batch ||J - I||^2 per layer\n",
        "            batch_jac = compute_batch_jacdev(model, activations, eps=1e-3, k_probes=1)\n",
        "            for idx, v in batch_jac.items():\n",
        "                jac_sums[idx]   += v\n",
        "                jac_counts[idx] += 1\n",
        "\n",
        "        epoch_jac = {idx: jac_sums[idx]/jac_counts[idx]\n",
        "                     for idx in jac_sums if jac_counts[idx] > 0}\n",
        "        print(f\"[Epoch {epoch+1}] approx ||J - I||^2:\", epoch_jac)\n",
        "        last_jac = epoch_jac\n",
        "\n",
        "    acc = evaluate_model(model, dev_loader, device)\n",
        "    print(f\"-> Full Finetune MRPC Acc: {acc:.4f}\")\n",
        "\n",
        "    remove_hooks(hooks)\n",
        "    return model, last_jac\n",
        "\n",
        "\n",
        "def prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores):\n",
        "    print(\"=== Stage 2: Prune (Low-||J-I||) & Finetuning ===\")\n",
        "    prune_idxs = prune_jac_layers(model, jac_scores, num_prune=num)\n",
        "    print(\"Pruned layers (lowest ||J-I||):\", prune_idxs)\n",
        "\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "    sched = get_linear_schedule_with_warmup(\n",
        "        opt, num_warmup_steps=0, num_training_steps=len(train_loader)*3\n",
        "    )\n",
        "\n",
        "    for epoch in range(5):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad()\n",
        "            out = model(input_ids=b['input_ids'].to(device),\n",
        "                        attention_mask=b['attention_mask'].to(device),\n",
        "                        labels=b['labels'].to(device))\n",
        "            out.loss.backward()\n",
        "            opt.step()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[Prune FT Epoch {epoch+1}] MRPC Acc: {acc:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def lora_only_finetuning(model, train_loader, dev_loader, device, r=2, alpha=1.0):\n",
        "    print(\"=== Stage 3: LoRA Finetuning ===\")\n",
        "    torch.cuda.empty_cache()\n",
        "    loras = apply_lora_to_all_layers(model, r, alpha)\n",
        "    for p in model.roberta.parameters(): p.requires_grad=False\n",
        "    for p in model.classifier.parameters(): p.requires_grad=True\n",
        "    for l in loras.values():\n",
        "        l.A.requires_grad=True\n",
        "        l.B.requires_grad=True\n",
        "\n",
        "    opt   = torch.optim.Adam(\n",
        "        list(model.classifier.parameters())\n",
        "        + [p for l in loras.values() for p in (l.A, l.B)],\n",
        "        lr=2e-5\n",
        "    )\n",
        "    sched = get_linear_schedule_with_warmup(\n",
        "        opt, num_warmup_steps=0, num_training_steps=len(train_loader)*6\n",
        "    )\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(6):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad()\n",
        "            with autocast():\n",
        "                out = model(input_ids=b['input_ids'].to(device),\n",
        "                            attention_mask=b['attention_mask'].to(device),\n",
        "                            labels=b['labels'].to(device))\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[LoRA Epoch {epoch+1}] MRPC Acc: {acc:.4f}\")\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 7) Main Entrypoint\n",
        "# ========================================================\n",
        "def main():\n",
        "    seed = 42\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # load & preprocess MRPC subset\n",
        "    train_ds = load_dataset(\"glue\", \"mrpc\", split=\"train\")\\\n",
        "               .shuffle(seed).select(range(1000))\n",
        "    dev_ds   = load_dataset(\"glue\", \"mrpc\", split=\"validation\")\n",
        "\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "    train = train_ds.map(lambda ex: preprocess_function(ex, tokenizer),\n",
        "                         batched=True,\n",
        "                         remove_columns=[\"sentence1\",\"sentence2\",\"idx\"])\\\n",
        "                    .rename_column(\"label\",\"labels\")\n",
        "    dev   = dev_ds.map(lambda ex: preprocess_function(ex, tokenizer),\n",
        "                       batched=True,\n",
        "                       remove_columns=[\"sentence1\",\"sentence2\",\"idx\"])\\\n",
        "                  .rename_column(\"label\",\"labels\")\n",
        "\n",
        "    collator     = DataCollatorWithPadding(tokenizer,\n",
        "                                           padding=\"max_length\",\n",
        "                                           max_length=64)\n",
        "    train_loader = DataLoader(train, batch_size=8, shuffle=True,\n",
        "                              collate_fn=collator)\n",
        "    dev_loader   = DataLoader(dev,   batch_size=16, shuffle=False,\n",
        "                              collate_fn=collator)\n",
        "\n",
        "    model, jac_scores = full_finetuning(train_loader, dev_loader, device)\n",
        "    model = prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores)\n",
        "    lora_only_finetuning(model, train_loader, dev_loader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "C0h04Rio7BXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Jacobian Deviation with Autograd JVP (||J - I||) on GLUE MRPC, AD\n",
        "# - Uses torch.autograd.functional.jvp for (J - I)v\n",
        "# - Forces math SDPA during JVP to avoid missing higher-order grads\n",
        "\n",
        "# ========================================================\n",
        "# 1) Standard imports and warning suppression\n",
        "# ========================================================\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from transformers import (\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizerFast,\n",
        "    DataCollatorWithPadding,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "import evaluate\n",
        "import warnings\n",
        "from contextlib import contextmanager\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# ========================================================\n",
        "# 1.1) Utility: force 'math' attention kernels during JVP\n",
        "# ========================================================\n",
        "@contextmanager\n",
        "def force_math_sdp():\n",
        "    \"\"\"\n",
        "    Force 'math' scaled-dot-product attention so higher-order grads/JVP work.\n",
        "    No-op on CPU.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        with torch.backends.cuda.sdp_kernel(\n",
        "            enable_flash=False, enable_math=True, enable_mem_efficient=False\n",
        "        ):\n",
        "            yield\n",
        "    else:\n",
        "        yield\n",
        "\n",
        "# ========================================================\n",
        "# 2) Jacobian Deviation / Hook Utilities (per-layer)\n",
        "# ========================================================\n",
        "def remove_hooks(hooks):\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "def register_jac_hooks(model):\n",
        "    \"\"\"\n",
        "    Capture each RobertaLayer's input (hidden_states) and extra args.\n",
        "    We'll locally re-run the layer to estimate JVPs.\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    activations = {i: {'X': None, 'args': None} for i in range(len(layers))}\n",
        "    hooks = []\n",
        "\n",
        "    for i, layer in enumerate(layers):\n",
        "        def pre_hook(module, inputs, idx=i):\n",
        "            xs = inputs[0]\n",
        "            extra = tuple(inputs[1:]) if len(inputs) > 1 else tuple()\n",
        "            activations[idx]['X'] = xs.detach()\n",
        "            activations[idx]['args'] = extra\n",
        "\n",
        "        hooks.append(layer.register_forward_pre_hook(pre_hook))\n",
        "\n",
        "    return hooks, activations\n",
        "\n",
        "# (kept for completeness; not used by JVP path)\n",
        "@torch.no_grad()\n",
        "def _layer_forward_only(layer, x, args):\n",
        "    out = layer(x, *args)\n",
        "    return out[0] if isinstance(out, tuple) else out\n",
        "\n",
        "def compute_batch_jacdev_autograd(model, acts, k_probes=2, rademacher=True):\n",
        "    \"\"\"\n",
        "    Autograd JVP estimator of E_v ||(J - I)v||^2 for each Transformer layer.\n",
        "    - rademacher=True uses ±1 probes; else Gaussian.\n",
        "    - Returns dict {layer_idx: score}.\n",
        "    \"\"\"\n",
        "    from torch.autograd.functional import jvp\n",
        "    layers = model.roberta.encoder.layer\n",
        "    scores = {}\n",
        "\n",
        "    for idx, buf in acts.items():\n",
        "        X, args = buf['X'], buf['args']\n",
        "        if X is None:\n",
        "            continue\n",
        "\n",
        "        X = X.detach().requires_grad_(True)\n",
        "        layer = layers[idx]\n",
        "        was_training = layer.training\n",
        "        layer.eval()  # turn off dropout for determinism\n",
        "\n",
        "        try:\n",
        "            def f(inp):\n",
        "                out = layer(inp, *args)\n",
        "                return out[0] if isinstance(out, tuple) else out\n",
        "\n",
        "            acc = 0.0\n",
        "            # Avoid efficient SDPA kernels lacking higher-order grads\n",
        "            with force_math_sdp():\n",
        "                for _ in range(k_probes):\n",
        "                    if rademacher:\n",
        "                        v = torch.empty_like(X).bernoulli_(0.5).mul_(2).sub_(1)  # ±1\n",
        "                    else:\n",
        "                        v = torch.randn_like(X)\n",
        "\n",
        "                    # JVP: returns (f(X), Jv). We only need Jv\n",
        "                    _, Jv = jvp(f, (X,), (v,), create_graph=False, strict=True)\n",
        "                    jd = Jv - v\n",
        "                    acc += float(jd.pow(2).mean().item())\n",
        "\n",
        "            scores[idx] = acc / k_probes\n",
        "        finally:\n",
        "            layer.train(was_training)\n",
        "\n",
        "        # clear buffers\n",
        "        buf['X'] = None\n",
        "        buf['args'] = None\n",
        "\n",
        "    return scores\n",
        "\n",
        "# ========================================================\n",
        "# 3) Pruning Utilities with SkipFF (prune low-||J-I||)\n",
        "# ========================================================\n",
        "class SkipFF(nn.Module):\n",
        "    def forward(self, hidden_states, input_tensor=None):\n",
        "        # Residual passthrough\n",
        "        return input_tensor\n",
        "\n",
        "def prune_jac_layers(model, jac_scores, num_prune=4):\n",
        "    \"\"\"\n",
        "    Prune layers with the *lowest* Jacobian deviation (closest to identity).\n",
        "    Replace FFN with Identity and use residual passthrough.\n",
        "    \"\"\"\n",
        "    sorted_layers = sorted(jac_scores.items(), key=lambda x: x[1])  # lowest first\n",
        "    prune_idxs = [idx for idx, _ in sorted_layers[:num_prune]]\n",
        "    for idx in prune_idxs:\n",
        "        layer = model.roberta.encoder.layer[idx]\n",
        "        layer.intermediate.dense = nn.Identity()\n",
        "        layer.output = SkipFF()\n",
        "    return prune_idxs\n",
        "\n",
        "# ========================================================\n",
        "# 4) LoRA Modules (unchanged)\n",
        "# ========================================================\n",
        "class LoRA(nn.Module):\n",
        "    def __init__(self, W0, r=2, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"W0\", W0.clone().detach())\n",
        "        L, M = W0.shape\n",
        "        self.B = nn.Parameter(torch.randn(L, r) * 0.01)\n",
        "        self.A = nn.Parameter(torch.zeros(r, M))\n",
        "        self.scaling = alpha / r\n",
        "    def forward(self):\n",
        "        return self.W0 + self.scaling * (self.B @ self.A)\n",
        "\n",
        "def apply_lora_to_all_layers(model, r=2, alpha=1.0):\n",
        "    loras = {}\n",
        "    for idx, layer in enumerate(model.roberta.encoder.layer):\n",
        "        if not hasattr(layer.output, 'dense'):\n",
        "            continue\n",
        "        W0 = layer.output.dense.weight.data\n",
        "        lora = LoRA(W0, r, alpha).to(W0.device)\n",
        "        def fwd(x, layer=layer, lora=lora):\n",
        "            return F.linear(x, lora(), layer.output.dense.bias)\n",
        "        layer.output.dense.forward = fwd\n",
        "        loras[idx] = lora\n",
        "    return loras\n",
        "\n",
        "# ========================================================\n",
        "# 5) Data + Eval Helpers\n",
        "# ========================================================\n",
        "def preprocess_function(examples, tok, max_length=64):\n",
        "    return tok(\n",
        "        examples['sentence1'],\n",
        "        examples['sentence2'],\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "def evaluate_model(model, dl, device):\n",
        "    model.eval()\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "    preds, labs = [], []\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids = b['input_ids'].to(device)\n",
        "            mask = b['attention_mask'].to(device)\n",
        "            labs.extend(b['labels'].cpu().numpy())\n",
        "            out = model(input_ids=ids, attention_mask=mask)\n",
        "            preds.extend(torch.argmax(out.logits, -1).cpu().numpy())\n",
        "    return metric.compute(predictions=preds, references=labs)[\"accuracy\"]\n",
        "\n",
        "# ========================================================\n",
        "# 6) Training Stages (using Jacobian deviation)\n",
        "# ========================================================\n",
        "def full_finetuning(train_loader, dev_loader, device):\n",
        "    print(\"=== Stage 1: Full Finetuning & Jacobian-Deviation Estimation ===\")\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\n",
        "        \"roberta-base\", num_labels=2\n",
        "    ).to(device)\n",
        "    model.gradient_checkpointing_enable()\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "    sched = get_linear_schedule_with_warmup(\n",
        "        opt, num_warmup_steps=0, num_training_steps=len(train_loader)*6\n",
        "    )\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    hooks, activations = register_jac_hooks(model)\n",
        "    last_jac = None\n",
        "\n",
        "    for epoch in range(6):\n",
        "        jac_sums, jac_counts = defaultdict(float), defaultdict(int)\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                out = model(\n",
        "                    input_ids=b['input_ids'].to(device),\n",
        "                    attention_mask=b['attention_mask'].to(device),\n",
        "                    labels=b['labels'].to(device)\n",
        "                )\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "\n",
        "            # Per-batch ||J - I||^2 per layer via JVP (dropout off inside function)\n",
        "            batch_jac = compute_batch_jacdev_autograd(\n",
        "                model, activations, k_probes=2, rademacher=True\n",
        "            )\n",
        "            for idx, v in batch_jac.items():\n",
        "                jac_sums[idx]   += v\n",
        "                jac_counts[idx] += 1\n",
        "\n",
        "        epoch_jac = {idx: jac_sums[idx]/jac_counts[idx]\n",
        "                     for idx in jac_sums if jac_counts[idx] > 0}\n",
        "        print(f\"[Epoch {epoch+1}] approx ||J - I||^2:\", {k: round(v, 6) for k, v in epoch_jac.items()})\n",
        "        last_jac = epoch_jac\n",
        "\n",
        "    acc = evaluate_model(model, dev_loader, device)\n",
        "    print(f\"-> Full Finetune MRPC Acc: {acc:.4f}\")\n",
        "\n",
        "    remove_hooks(hooks)\n",
        "    return model, last_jac\n",
        "\n",
        "def prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores):\n",
        "    print(\"=== Stage 2: Prune (Low-||J-I||) & Finetuning ===\")\n",
        "    prune_idxs = prune_jac_layers(model, jac_scores, num_prune=num)\n",
        "    print(\"Pruned layers (lowest ||J-I||):\", prune_idxs)\n",
        "\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "    sched = get_linear_schedule_with_warmup(\n",
        "        opt, num_warmup_steps=0, num_training_steps=len(train_loader)*3\n",
        "    )\n",
        "\n",
        "    for epoch in range(5):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            out = model(\n",
        "                input_ids=b['input_ids'].to(device),\n",
        "                attention_mask=b['attention_mask'].to(device),\n",
        "                labels=b['labels'].to(device)\n",
        "            )\n",
        "            out.loss.backward()\n",
        "            opt.step()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[Prune FT Epoch {epoch+1}] MRPC Acc: {acc:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def lora_only_finetuning(model, train_loader, dev_loader, device, r=2, alpha=1.0):\n",
        "    print(\"=== Stage 3: LoRA Finetuning ===\")\n",
        "    torch.cuda.empty_cache()\n",
        "    loras = apply_lora_to_all_layers(model, r, alpha)\n",
        "    for p in model.roberta.parameters():\n",
        "        p.requires_grad = False\n",
        "    for p in model.classifier.parameters():\n",
        "        p.requires_grad = True\n",
        "    for l in loras.values():\n",
        "        l.A.requires_grad = True\n",
        "        l.B.requires_grad = True\n",
        "\n",
        "    opt   = torch.optim.Adam(\n",
        "        list(model.classifier.parameters()) + [p for l in loras.values() for p in (l.A, l.B)],\n",
        "        lr=2e-5\n",
        "    )\n",
        "    sched = get_linear_schedule_with_warmup(\n",
        "        opt, num_warmup_steps=0, num_training_steps=len(train_loader)*6\n",
        "    )\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(6):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                out = model(\n",
        "                    input_ids=b['input_ids'].to(device),\n",
        "                    attention_mask=b['attention_mask'].to(device),\n",
        "                    labels=b['labels'].to(device)\n",
        "                )\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[LoRA Epoch {epoch+1}] MRPC Acc: {acc:.4f}\")\n",
        "\n",
        "# ========================================================\n",
        "# 7) Main Entrypoint\n",
        "# ========================================================\n",
        "def main():\n",
        "    seed = 42\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # load & preprocess MRPC subset\n",
        "    train_ds = load_dataset(\"glue\", \"mrpc\", split=\"train\").shuffle(seed).select(range(1000))\n",
        "    dev_ds   = load_dataset(\"glue\", \"mrpc\", split=\"validation\")\n",
        "\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "    train = train_ds.map(lambda ex: preprocess_function(ex, tokenizer),\n",
        "                         batched=True,\n",
        "                         remove_columns=[\"sentence1\",\"sentence2\",\"idx\"])\\\n",
        "                    .rename_column(\"label\",\"labels\")\n",
        "    dev   = dev_ds.map(lambda ex: preprocess_function(ex, tokenizer),\n",
        "                       batched=True,\n",
        "                       remove_columns=[\"sentence1\",\"sentence2\",\"idx\"])\\\n",
        "                  .rename_column(\"label\",\"labels\")\n",
        "\n",
        "    collator     = DataCollatorWithPadding(tokenizer, padding=\"max_length\", max_length=64)\n",
        "    train_loader = DataLoader(train, batch_size=8, shuffle=True,  collate_fn=collator)\n",
        "    dev_loader   = DataLoader(dev,   batch_size=16, shuffle=False, collate_fn=collator)\n",
        "\n",
        "    model, jac_scores = full_finetuning(train_loader, dev_loader, device)\n",
        "    model = prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores)\n",
        "    lora_only_finetuning(model, train_loader, dev_loader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "zZFTh5EcvTrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XDd_XTY97Bym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GpK_pWtl7CLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "otRb607o7Cnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xYpBMdau7DHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q1bHamnpb-S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Jacobian, FD\n",
        "\n",
        "# ========================================================\n",
        "# 1) Standard imports and warning suppression\n",
        "# ========================================================\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from transformers import (\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizerFast,\n",
        "    DataCollatorWithPadding,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "import evaluate\n",
        "import math\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 2) Jacobian Deviation (||J - I||) / Hook Utilities\n",
        "#    Capture each layer's input so we can locally re-run it\n",
        "#    to estimate JVPs via finite differences.\n",
        "# ========================================================\n",
        "def remove_hooks(hooks):\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "def register_jac_hooks(model):\n",
        "    \"\"\"\n",
        "    Store each RobertaLayer's input (hidden_states) and extra args\n",
        "    (attention mask, etc.). We'll locally call the layer again to\n",
        "    compute finite-difference JVPs.\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    activations = {i: {'X': None, 'args': None} for i in range(len(layers))}\n",
        "    hooks = []\n",
        "\n",
        "    for i, layer in enumerate(layers):\n",
        "        def pre_hook(module, inputs, idx=i):\n",
        "            xs = inputs[0]\n",
        "            extra = tuple(inputs[1:]) if len(inputs) > 1 else tuple()\n",
        "            activations[idx]['X'] = xs.detach()\n",
        "            activations[idx]['args'] = extra\n",
        "        hooks.append(layer.register_forward_pre_hook(pre_hook))\n",
        "\n",
        "    return hooks, activations\n",
        "\n",
        "@torch.no_grad()\n",
        "def _layer_forward_only(layer, x, args):\n",
        "    # Disable AMP and do fp32 for numeric stability\n",
        "    with torch.cuda.amp.autocast(enabled=False):\n",
        "        out = layer(x.to(torch.float32), *args)\n",
        "        return out[0] if isinstance(out, tuple) else out\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_batch_jacdev(model, acts, eps=1e-3, k_probes=1):\n",
        "    \"\"\"\n",
        "    For each layer ℓ, estimate ||Jℓ - I||_F^2:\n",
        "      (J - I)v ≈ (f(x + eps*v) - f(x))/eps - v\n",
        "    Average squared norm over k_probes random v.\n",
        "    Returns {layer_idx: score}.\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    scores = {}\n",
        "\n",
        "    for idx, buf in acts.items():\n",
        "        X = buf['X']; args = buf['args']\n",
        "        if X is None:\n",
        "            continue\n",
        "\n",
        "        layer = layers[idx]\n",
        "        was_training = layer.training\n",
        "        layer.eval()  # turn off dropout during the probes\n",
        "        try:\n",
        "            y0 = _layer_forward_only(layer, X, args)\n",
        "\n",
        "            acc = 0.0\n",
        "            for _ in range(k_probes):\n",
        "                v = torch.randn_like(X)\n",
        "                y_eps = _layer_forward_only(layer, X + eps * v, args)\n",
        "                jd_vec = (y_eps - y0) / eps - v\n",
        "                acc += float(jd_vec.pow(2).mean().item())\n",
        "            scores[idx] = acc / k_probes\n",
        "        finally:\n",
        "            layer.train(was_training)\n",
        "\n",
        "        # clear buffers for next batch\n",
        "        buf['X'] = None\n",
        "        buf['args'] = None\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 3) Pruning Utilities with SkipFF (prune low-||J-I||)\n",
        "# ========================================================\n",
        "class SkipFF(nn.Module):\n",
        "    def forward(self, hidden_states, input_tensor=None):\n",
        "        # residual passthrough\n",
        "        return input_tensor\n",
        "\n",
        "def prune_jac_layers(model, jac_scores, num_prune=6):\n",
        "    \"\"\"\n",
        "    Prune layers with the *lowest* Jacobian deviation (closest to identity).\n",
        "    Replace FFN with Identity and use residual passthrough.\n",
        "    \"\"\"\n",
        "    sorted_layers = sorted(jac_scores.items(), key=lambda x: x[1], reverse=False)\n",
        "    prune_idxs = [idx for idx, _ in sorted_layers[:num_prune]]\n",
        "    for idx in prune_idxs:\n",
        "        layer = model.roberta.encoder.layer[idx]\n",
        "        layer.intermediate.dense = nn.Identity()\n",
        "        layer.output = SkipFF()\n",
        "    return prune_idxs\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 4) LoRA Modules (unchanged)\n",
        "# ========================================================\n",
        "class LoRA(nn.Module):\n",
        "    def __init__(self, W0, r=2, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"W0\", W0.clone().detach())\n",
        "        L, M = W0.shape\n",
        "        self.B = nn.Parameter(torch.randn(L, r) * 0.01)\n",
        "        self.A = nn.Parameter(torch.zeros(r, M))\n",
        "        self.scaling = alpha / r\n",
        "    def forward(self):\n",
        "        return self.W0 + self.scaling * (self.B @ self.A)\n",
        "\n",
        "def apply_lora_to_all_layers(model, r=2, alpha=1.0):\n",
        "    loras = {}\n",
        "    for idx, layer in enumerate(model.roberta.encoder.layer):\n",
        "        if not hasattr(layer.output, 'dense'):\n",
        "            continue\n",
        "        W0 = layer.output.dense.weight.data\n",
        "        lora = LoRA(W0, r, alpha).to(W0.device)\n",
        "        def fwd(x, layer=layer, lora=lora):\n",
        "            return F.linear(x, lora(), layer.output.dense.bias)\n",
        "        layer.output.dense.forward = fwd\n",
        "        loras[idx] = lora\n",
        "    return loras\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 5) Data + Eval Helpers\n",
        "# ========================================================\n",
        "def preprocess_function(examples, tok, max_length=64):\n",
        "    return tok(examples['sentence1'],\n",
        "               examples['sentence2'],\n",
        "               truncation=True,\n",
        "               padding='max_length',\n",
        "               max_length=max_length)\n",
        "\n",
        "def evaluate_model(model, dl, device):\n",
        "    model.eval()\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "    preds, labs = [], []\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids = b['input_ids'].to(device)\n",
        "            mask = b['attention_mask'].to(device)\n",
        "            labs.extend(b['labels'].cpu().numpy())\n",
        "            out = model(input_ids=ids, attention_mask=mask)\n",
        "            preds.extend(torch.argmax(out.logits, -1).cpu().numpy())\n",
        "    return metric.compute(predictions=preds, references=labs)[\"accuracy\"]\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 6) Training Stages (Jacobian-Deviation scoring)\n",
        "# ========================================================\n",
        "def full_finetuning(train_loader, dev_loader, device):\n",
        "    print(\"=== Stage 1: Full Finetuning & Jacobian-Deviation Estimation ===\")\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\n",
        "        \"roberta-base\", num_labels=2\n",
        "    ).to(device)\n",
        "    model.gradient_checkpointing_enable()\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "    sched = get_linear_schedule_with_warmup(\n",
        "        opt, num_warmup_steps=0, num_training_steps=len(train_loader)*6\n",
        "    )\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    hooks, activations = register_jac_hooks(model)\n",
        "    last_jac = None\n",
        "\n",
        "    for epoch in range(6):\n",
        "        jac_sums, jac_counts = defaultdict(float), defaultdict(int)\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad()\n",
        "            with autocast():\n",
        "                out = model(input_ids=b['input_ids'].to(device),\n",
        "                            attention_mask=b['attention_mask'].to(device),\n",
        "                            labels=b['labels'].to(device))\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "\n",
        "            # Per-batch ||J - I||^2 per layer\n",
        "            batch_jac = compute_batch_jacdev(model, activations, eps=1e-3, k_probes=1)\n",
        "            for idx, v in batch_jac.items():\n",
        "                jac_sums[idx]   += v\n",
        "                jac_counts[idx] += 1\n",
        "\n",
        "        epoch_jac = {idx: jac_sums[idx]/jac_counts[idx]\n",
        "                     for idx in jac_sums if jac_counts[idx] > 0}\n",
        "        print(f\"[Epoch {epoch+1}] approx ||J - I||^2:\", {k: round(v, 6) for k, v in epoch_jac.items()})\n",
        "        last_jac = epoch_jac\n",
        "\n",
        "    acc = evaluate_model(model, dev_loader, device)\n",
        "    print(f\"-> Full Finetune SST2 Acc: {acc:.4f}\")\n",
        "\n",
        "    remove_hooks(hooks)\n",
        "    return model, last_jac\n",
        "\n",
        "\n",
        "def prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores):\n",
        "    print(\"=== Stage 2: Prune (Low-||J-I||) & Finetuning ===\")\n",
        "    prune_idxs = prune_jac_layers(model, jac_scores, num_prune=num)\n",
        "    print(\"Pruned layers (lowest ||J-I||):\", prune_idxs)\n",
        "\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "    sched = get_linear_schedule_with_warmup(\n",
        "        opt, num_warmup_steps=0, num_training_steps=len(train_loader)*3\n",
        "    )\n",
        "\n",
        "    for epoch in range(5):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad()\n",
        "            out = model(input_ids=b['input_ids'].to(device),\n",
        "                        attention_mask=b['attention_mask'].to(device),\n",
        "                        labels=b['labels'].to(device))\n",
        "            out.loss.backward()\n",
        "            opt.step()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[Prune FT Epoch {epoch+1}] SST2 Acc: {acc:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def lora_only_finetuning(model, train_loader, dev_loader, device, r=2, alpha=1.0):\n",
        "    # (unchanged LoRA stage)\n",
        "    print(\"=== Stage 3: LoRA Finetuning ===\")\n",
        "    torch.cuda.empty_cache()\n",
        "    loras = apply_lora_to_all_layers(model, r, alpha)\n",
        "    for p in model.roberta.parameters(): p.requires_grad=False\n",
        "    for p in model.classifier.parameters(): p.requires_grad=True\n",
        "    for l in loras.values():\n",
        "        l.A.requires_grad=True\n",
        "        l.B.requires_grad=True\n",
        "\n",
        "    opt   = torch.optim.Adam(\n",
        "        list(model.classifier.parameters())\n",
        "        + [p for l in loras.values() for p in (l.A, l.B)],\n",
        "        lr=2e-5\n",
        "    )\n",
        "    sched = get_linear_schedule_with_warmup(\n",
        "        opt, num_warmup_steps=0, num_training_steps=len(train_loader)*6\n",
        "    )\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(6):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad()\n",
        "            with autocast():\n",
        "                out = model(input_ids=b['input_ids'].to(device),\n",
        "                            attention_mask=b['attention_mask'].to(device),\n",
        "                            labels=b['labels'].to(device))\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[LoRA Epoch {epoch+1}] SST2 Acc: {acc:.4f}\")\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 7) Main Entrypoint\n",
        "# ========================================================\n",
        "def main():\n",
        "    seed = 42\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # load & preprocess SST-2 subset\n",
        "    train_ds = load_dataset(\"glue\", \"sst2\", split=\"train\").shuffle(seed).select(range(5000))\n",
        "    dev_ds   = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
        "\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "    train = train_ds.map(lambda ex: tokenizer(ex[\"sentence\"], truncation=True, padding='max_length', max_length=64),\n",
        "                         batched=True)\\\n",
        "                    .rename_column(\"label\", \"labels\")\\\n",
        "                    .remove_columns([\"sentence\", \"idx\"])\n",
        "    dev = dev_ds.map(lambda ex: tokenizer(ex[\"sentence\"], truncation=True, padding='max_length', max_length=64),\n",
        "                         batched=True)\\\n",
        "                    .rename_column(\"label\", \"labels\")\\\n",
        "                    .remove_columns([\"sentence\", \"idx\"])\n",
        "\n",
        "    collator     = DataCollatorWithPadding(tokenizer, padding=\"max_length\", max_length=64)\n",
        "    train_loader = DataLoader(train, batch_size=8, shuffle=True,  collate_fn=collator)\n",
        "    dev_loader   = DataLoader(dev,   batch_size=16, shuffle=False, collate_fn=collator)\n",
        "\n",
        "    model, jac_scores = full_finetuning(train_loader, dev_loader, device)\n",
        "    model = prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores)\n",
        "    lora_only_finetuning(model, train_loader, dev_loader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ygXX_VPPErgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Jacobian (Autograd JVP version), AD\n",
        "\n",
        "# ========================================================\n",
        "# 1) Standard imports and warning suppression\n",
        "# ========================================================\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from transformers import (\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizerFast,\n",
        "    DataCollatorWithPadding,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "import evaluate\n",
        "import warnings\n",
        "from contextlib import contextmanager\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# ========================================================\n",
        "# 1.1) Utility: force 'math' attention kernels during JVP\n",
        "# ========================================================\n",
        "@contextmanager\n",
        "def force_math_sdp():\n",
        "    \"\"\"\n",
        "    Force 'math' scaled-dot-product attention so higher-order grads/JVP work.\n",
        "    No-op on CPU.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        with torch.backends.cuda.sdp_kernel(\n",
        "            enable_flash=False, enable_math=True, enable_mem_efficient=False\n",
        "        ):\n",
        "            yield\n",
        "    else:\n",
        "        yield\n",
        "\n",
        "# ========================================================\n",
        "# 2) Jacobian Deviation (||J - I||) / Hook Utilities\n",
        "#    Capture each layer's input so we can locally re-run it\n",
        "#    to estimate JVPs via autograd.\n",
        "# ========================================================\n",
        "def remove_hooks(hooks):\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "def register_jac_hooks(model):\n",
        "    \"\"\"\n",
        "    Store each RobertaLayer's input (hidden_states) and extra args\n",
        "    (attention mask, etc.). We'll locally call the layer again to\n",
        "    compute JVPs with autograd.\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    activations = {i: {'X': None, 'args': None} for i in range(len(layers))}\n",
        "    hooks = []\n",
        "\n",
        "    for i, layer in enumerate(layers):\n",
        "        def pre_hook(module, inputs, idx=i):\n",
        "            xs = inputs[0]\n",
        "            extra = tuple(inputs[1:]) if len(inputs) > 1 else tuple()\n",
        "            activations[idx]['X'] = xs.detach()\n",
        "            activations[idx]['args'] = extra\n",
        "        hooks.append(layer.register_forward_pre_hook(pre_hook))\n",
        "\n",
        "    return hooks, activations\n",
        "\n",
        "def compute_batch_jacdev_autograd(model, acts, k_probes=2, rademacher=True):\n",
        "    \"\"\"\n",
        "    For each layer ℓ, estimate E_v ||(Jℓ - I)v||^2 using autograd JVP:\n",
        "      Jv from torch.autograd.functional.jvp, then (J - I)v = Jv - v.\n",
        "    Average squared norm over k_probes random v.\n",
        "    Returns {layer_idx: score}.\n",
        "    \"\"\"\n",
        "    from torch.autograd.functional import jvp\n",
        "    layers = model.roberta.encoder.layer\n",
        "    scores = {}\n",
        "\n",
        "    for idx, buf in acts.items():\n",
        "        X = buf['X']; args = buf['args']\n",
        "        if X is None:\n",
        "            continue\n",
        "\n",
        "        X = X.detach().requires_grad_(True)\n",
        "        layer = layers[idx]\n",
        "        was_training = layer.training\n",
        "        layer.eval()  # turn off dropout during the probes\n",
        "\n",
        "        try:\n",
        "            def f(inp):\n",
        "                out = layer(inp, *args)\n",
        "                return out[0] if isinstance(out, tuple) else out\n",
        "\n",
        "            acc = 0.0\n",
        "            # Avoid efficient SDPA kernels lacking higher-order grads\n",
        "            with force_math_sdp():\n",
        "                for _ in range(k_probes):\n",
        "                    if rademacher:\n",
        "                        v = torch.empty_like(X).bernoulli_(0.5).mul_(2).sub_(1)  # ±1\n",
        "                    else:\n",
        "                        v = torch.randn_like(X)\n",
        "                    _, Jv = jvp(f, (X,), (v,), create_graph=False, strict=True)\n",
        "                    jd_vec = Jv - v\n",
        "                    acc += float(jd_vec.pow(2).mean().item())\n",
        "            scores[idx] = acc / k_probes\n",
        "        finally:\n",
        "            layer.train(was_training)\n",
        "\n",
        "        # clear buffers for next batch\n",
        "        buf['X'] = None\n",
        "        buf['args'] = None\n",
        "\n",
        "    return scores\n",
        "\n",
        "# ========================================================\n",
        "# 3) Pruning Utilities with SkipFF (prune low-||J-I||)\n",
        "# ========================================================\n",
        "class SkipFF(nn.Module):\n",
        "    def forward(self, hidden_states, input_tensor=None):\n",
        "        # residual passthrough\n",
        "        return input_tensor\n",
        "\n",
        "def prune_jac_layers(model, jac_scores, num_prune=6):\n",
        "    \"\"\"\n",
        "    Prune layers with the *lowest* Jacobian deviation (closest to identity).\n",
        "    Replace FFN with Identity and use residual passthrough.\n",
        "    \"\"\"\n",
        "    sorted_layers = sorted(jac_scores.items(), key=lambda x: x[1], reverse=False)\n",
        "    prune_idxs = [idx for idx, _ in sorted_layers[:num_prune]]\n",
        "    for idx in prune_idxs:\n",
        "        layer = model.roberta.encoder.layer[idx]\n",
        "        layer.intermediate.dense = nn.Identity()\n",
        "        layer.output = SkipFF()\n",
        "    return prune_idxs\n",
        "\n",
        "# ========================================================\n",
        "# 4) LoRA Modules (unchanged)\n",
        "# ========================================================\n",
        "class LoRA(nn.Module):\n",
        "    def __init__(self, W0, r=2, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"W0\", W0.clone().detach())\n",
        "        L, M = W0.shape\n",
        "        self.B = nn.Parameter(torch.randn(L, r) * 0.01)\n",
        "        self.A = nn.Parameter(torch.zeros(r, M))\n",
        "        self.scaling = alpha / r\n",
        "    def forward(self):\n",
        "        return self.W0 + self.scaling * (self.B @ self.A)\n",
        "\n",
        "def apply_lora_to_all_layers(model, r=2, alpha=1.0):\n",
        "    loras = {}\n",
        "    for idx, layer in enumerate(model.roberta.encoder.layer):\n",
        "        if not hasattr(layer.output, 'dense'):\n",
        "            continue\n",
        "        W0 = layer.output.dense.weight.data\n",
        "        lora = LoRA(W0, r, alpha).to(W0.device)\n",
        "        def fwd(x, layer=layer, lora=lora):\n",
        "            return F.linear(x, lora(), layer.output.dense.bias)\n",
        "        layer.output.dense.forward = fwd\n",
        "        loras[idx] = lora\n",
        "    return loras\n",
        "\n",
        "# ========================================================\n",
        "# 5) Data + Eval Helpers\n",
        "# ========================================================\n",
        "def preprocess_function(examples, tok, max_length=64):\n",
        "    return tok(examples['sentence'],\n",
        "               truncation=True,\n",
        "               padding='max_length',\n",
        "               max_length=max_length)\n",
        "\n",
        "def evaluate_model(model, dl, device):\n",
        "    model.eval()\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "    preds, labs = [], []\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids = b['input_ids'].to(device)\n",
        "            mask = b['attention_mask'].to(device)\n",
        "            labs.extend(b['labels'].cpu().numpy())\n",
        "            out = model(input_ids=ids, attention_mask=mask)\n",
        "            preds.extend(torch.argmax(out.logits, -1).cpu().numpy())\n",
        "    return metric.compute(predictions=preds, references=labs)[\"accuracy\"]\n",
        "\n",
        "# ========================================================\n",
        "# 6) Training Stages (Jacobian-Deviation scoring)\n",
        "# ========================================================\n",
        "def full_finetuning(train_loader, dev_loader, device):\n",
        "    print(\"=== Stage 1: Full Finetuning & Jacobian-Deviation Estimation ===\")\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\n",
        "        \"roberta-base\", num_labels=2\n",
        "    ).to(device)\n",
        "    model.gradient_checkpointing_enable()\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "    sched = get_linear_schedule_with_warmup(\n",
        "        opt, num_warmup_steps=0, num_training_steps=len(train_loader)*6\n",
        "    )\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    hooks, activations = register_jac_hooks(model)\n",
        "    last_jac = None\n",
        "\n",
        "    for epoch in range(6):\n",
        "        jac_sums, jac_counts = defaultdict(float), defaultdict(int)\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                out = model(input_ids=b['input_ids'].to(device),\n",
        "                            attention_mask=b['attention_mask'].to(device),\n",
        "                            labels=b['labels'].to(device))\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "\n",
        "            # Per-batch ||J - I||^2 per layer (Autograd JVP)\n",
        "            batch_jac = compute_batch_jacdev_autograd(model, activations, k_probes=2, rademacher=True)\n",
        "            for idx, v in batch_jac.items():\n",
        "                jac_sums[idx]   += v\n",
        "                jac_counts[idx] += 1\n",
        "\n",
        "        epoch_jac = {idx: jac_sums[idx]/jac_counts[idx]\n",
        "                     for idx in jac_sums if jac_counts[idx] > 0}\n",
        "        print(f\"[Epoch {epoch+1}] approx ||J - I||^2:\", {k: round(v, 6) for k, v in epoch_jac.items()})\n",
        "        last_jac = epoch_jac\n",
        "\n",
        "    acc = evaluate_model(model, dev_loader, device)\n",
        "    print(f\"-> Full Finetune SST2 Acc: {acc:.4f}\")\n",
        "\n",
        "    remove_hooks(hooks)\n",
        "    return model, last_jac\n",
        "\n",
        "def prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores):\n",
        "    print(\"=== Stage 2: Prune (Low-||J-I||) & Finetuning ===\")\n",
        "    prune_idxs = prune_jac_layers(model, jac_scores, num_prune=num)\n",
        "    print(\"Pruned layers (lowest ||J-I||):\", prune_idxs)\n",
        "\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "    sched = get_linear_schedule_with_warmup(\n",
        "        opt, num_warmup_steps=0, num_training_steps=len(train_loader)*3\n",
        "    )\n",
        "\n",
        "    for epoch in range(5):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            out = model(input_ids=b['input_ids'].to(device),\n",
        "                        attention_mask=b['attention_mask'].to(device),\n",
        "                        labels=b['labels'].to(device))\n",
        "            out.loss.backward()\n",
        "            opt.step()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[Prune FT Epoch {epoch+1}] SST2 Acc: {acc:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def lora_only_finetuning(model, train_loader, dev_loader, device, r=2, alpha=1.0):\n",
        "    # (unchanged LoRA stage)\n",
        "    print(\"=== Stage 3: LoRA Finetuning ===\")\n",
        "    torch.cuda.empty_cache()\n",
        "    loras = apply_lora_to_all_layers(model, r, alpha)\n",
        "    for p in model.roberta.parameters(): p.requires_grad=False\n",
        "    for p in model.classifier.parameters(): p.requires_grad=True\n",
        "    for l in loras.values():\n",
        "        l.A.requires_grad=True\n",
        "        l.B.requires_grad=True\n",
        "\n",
        "    opt   = torch.optim.Adam(\n",
        "        list(model.classifier.parameters())\n",
        "        + [p for l in loras.values() for p in (l.A, l.B)],\n",
        "        lr=2e-5\n",
        "    )\n",
        "    sched = get_linear_schedule_with_warmup(\n",
        "        opt, num_warmup_steps=0, num_training_steps=len(train_loader)*6\n",
        "    )\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(6):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                out = model(input_ids=b['input_ids'].to(device),\n",
        "                            attention_mask=b['attention_mask'].to(device),\n",
        "                            labels=b['labels'].to(device))\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[LoRA Epoch {epoch+1}] SST2 Acc: {acc:.4f}\")\n",
        "\n",
        "# ========================================================\n",
        "# 7) Main Entrypoint\n",
        "# ========================================================\n",
        "def main():\n",
        "    seed = 42\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # load & preprocess SST-2 subset\n",
        "    train_ds = load_dataset(\"glue\", \"sst2\", split=\"train\").shuffle(seed).select(range(5000))\n",
        "    dev_ds   = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
        "\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "    train = train_ds.map(lambda ex: preprocess_function(ex, tokenizer),\n",
        "                         batched=True)\\\n",
        "                    .rename_column(\"label\", \"labels\")\\\n",
        "                    .remove_columns([\"sentence\", \"idx\"])\n",
        "    dev = dev_ds.map(lambda ex: preprocess_function(ex, tokenizer),\n",
        "                     batched=True)\\\n",
        "                .rename_column(\"label\", \"labels\")\\\n",
        "                .remove_columns([\"sentence\", \"idx\"])\n",
        "\n",
        "    collator     = DataCollatorWithPadding(tokenizer, padding=\"max_length\", max_length=64)\n",
        "    train_loader = DataLoader(train, batch_size=8, shuffle=True,  collate_fn=collator)\n",
        "    dev_loader   = DataLoader(dev,   batch_size=16, shuffle=False, collate_fn=collator)\n",
        "\n",
        "    model, jac_scores = full_finetuning(train_loader, dev_loader, device)\n",
        "    model = prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores)\n",
        "    lora_only_finetuning(model, train_loader, dev_loader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "f0aZeTgSFwRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ms4NUhAaEr7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Jacobian FD for CoLA\n",
        "\n",
        "# Jacobian for CoLA\n",
        "\n",
        "# ========================================================\n",
        "# 1) Standard imports and warning suppression\n",
        "# ========================================================\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from transformers import (\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizerFast,\n",
        "    DataCollatorWithPadding,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "import evaluate\n",
        "import math\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 2) Jacobian-Deviation / Hook Utilities (per-layer J ~ I)\n",
        "# ========================================================\n",
        "def register_jac_hooks(model):\n",
        "    \"\"\"\n",
        "    Capture each RobertaLayer's input (hidden_states) + the rest of its args\n",
        "    so we can re-run the layer locally to estimate (J - I)v via finite diff.\n",
        "    We store:\n",
        "      activations[idx]['X']   = hidden_states (fp32, detached)\n",
        "      activations[idx]['args']= tuple of non-input args (kept as-is)\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    activations = {i: {'X': None, 'args': None} for i in range(len(layers))}\n",
        "    hooks = []\n",
        "\n",
        "    for i, layer in enumerate(layers):\n",
        "        def pre_hook(module, inputs, idx=i):\n",
        "            # inputs: (hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, ...)\n",
        "            xs = inputs[0]\n",
        "            extra = tuple(inputs[1:]) if len(inputs) > 1 else tuple()\n",
        "            activations[idx]['X'] = xs.detach().to(torch.float32)\n",
        "            activations[idx]['args'] = extra\n",
        "\n",
        "        hooks.append(layer.register_forward_pre_hook(pre_hook))\n",
        "\n",
        "    return hooks, activations\n",
        "\n",
        "def remove_hooks(hooks):\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "@torch.no_grad()\n",
        "def _layer_forward_only(layer, x, args):\n",
        "    out = layer(x, *args)\n",
        "    return out[0] if isinstance(out, tuple) else out\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_batch_jacdev(model, acts, eps=1e-3, k_probes=1, rms_norm_v=True):\n",
        "    \"\"\"\n",
        "    For each layer, estimate ||J - I||^2 via finite-difference JVPs:\n",
        "       (J - I)v ≈ (f(x + eps*v) - f(x))/eps - v\n",
        "    Average squared norms over `k_probes` random v.\n",
        "    Returns dict: {layer_idx: score}\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    scores = {}\n",
        "\n",
        "    for idx, buf in acts.items():\n",
        "        X = buf['X']; args = buf['args']\n",
        "        if X is None:\n",
        "            continue\n",
        "\n",
        "        layer = layers[idx]\n",
        "        was_training = layer.training\n",
        "        layer.eval()  # disable dropout etc.\n",
        "\n",
        "        try:\n",
        "            Xf = X  # already fp32\n",
        "            y0 = _layer_forward_only(layer, Xf, args).to(torch.float32)\n",
        "\n",
        "            acc = 0.0\n",
        "            for _ in range(k_probes):\n",
        "                v = torch.randn_like(Xf)\n",
        "\n",
        "                if rms_norm_v:\n",
        "                    # match v's RMS to X's RMS for a well-scaled finite diff\n",
        "                    vx = v.reshape(v.size(0), -1)\n",
        "                    xx = Xf.reshape(Xf.size(0), -1)\n",
        "                    v_rms = vx.std(dim=1, keepdim=True) + 1e-6\n",
        "                    x_rms = xx.std(dim=1, keepdim=True) + 1e-6\n",
        "                    v = (v / v_rms.view(-1, 1, 1)) * x_rms.view(-1, 1, 1)\n",
        "\n",
        "                y_eps = _layer_forward_only(layer, Xf + eps * v, args).to(torch.float32)\n",
        "                jd_vec = (y_eps - y0) / eps - v\n",
        "                acc += float(jd_vec.pow(2).mean().item())\n",
        "\n",
        "            scores[idx] = acc / k_probes\n",
        "\n",
        "        finally:\n",
        "            layer.train(was_training)\n",
        "\n",
        "        # clear buffers for next batch\n",
        "        buf['X'] = None\n",
        "        buf['args'] = None\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 3) Pruning Utilities with SkipFF (prune low-||J-I||) — preserve first & last\n",
        "# ========================================================\n",
        "class SkipFF(nn.Module):\n",
        "    def forward(self, hidden_states, input_tensor=None):\n",
        "        return input_tensor  # residual passthrough\n",
        "\n",
        "def prune_jac_layers(model, jac_scores, num_prune=4):\n",
        "    \"\"\"\n",
        "    Prune layers with the lowest Jacobian deviation (closest to identity),\n",
        "    while preserving the first (idx=0) and last (idx=L-1) Transformer blocks.\n",
        "    We remove the FFN by replacing intermediate.dense with Identity\n",
        "    and the output block with SkipFF (residual passthrough).\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    L = len(layers)\n",
        "    preserve = {0, L - 1}\n",
        "\n",
        "    # Eligible = all scored layers except first/last\n",
        "    eligible = [(idx, s) for idx, s in jac_scores.items() if idx not in preserve and 0 <= idx < L]\n",
        "    if not eligible:\n",
        "        print(\"[Prune] No eligible layers to prune after preserving first/last.\")\n",
        "        return []\n",
        "\n",
        "    eligible_sorted = sorted(eligible, key=lambda x: x[1])  # lowest first\n",
        "    take = max(0, min(num_prune, len(eligible_sorted)))\n",
        "    prune_idxs = [idx for idx, _ in eligible_sorted[:take]]\n",
        "\n",
        "    for idx in prune_idxs:\n",
        "        layer = layers[idx]\n",
        "        layer.intermediate.dense = nn.Identity()\n",
        "        layer.output = SkipFF()\n",
        "\n",
        "    print(f\"[Prune] Preserved layers: {sorted(list(preserve))}; Pruned layers: {sorted(prune_idxs)}\")\n",
        "    return prune_idxs\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 4) LoRA Modules (unchanged)\n",
        "# ========================================================\n",
        "class LoRA(nn.Module):\n",
        "    def __init__(self, W0, r=2, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"W0\", W0.clone().detach())\n",
        "        L, M = W0.shape\n",
        "        self.B = nn.Parameter(torch.randn(L, r) * 0.01)\n",
        "        self.A = nn.Parameter(torch.zeros(r, M))\n",
        "        self.scaling = alpha / r\n",
        "    def forward(self):\n",
        "        return self.W0 + self.scaling * (self.B @ self.A)\n",
        "\n",
        "def apply_lora_to_all_layers(model, r=2, alpha=1.0):\n",
        "    loras = {}\n",
        "    for idx, layer in enumerate(model.roberta.encoder.layer):\n",
        "        if not hasattr(layer.output, 'dense'):\n",
        "            continue\n",
        "        W0 = layer.output.dense.weight.data\n",
        "        lora = LoRA(W0, r, alpha).to(W0.device)\n",
        "        def fwd(x, layer=layer, lora=lora):\n",
        "            return F.linear(x, lora(), layer.output.dense.bias)\n",
        "        layer.output.dense.forward = fwd\n",
        "        loras[idx] = lora\n",
        "    return loras\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 5) Data + Eval Helpers\n",
        "# ========================================================\n",
        "def preprocess_function(examples, tok, max_length=64):\n",
        "    return tok(examples['sentence1'],\n",
        "               examples['sentence2'],\n",
        "               truncation=True,\n",
        "               padding='max_length',\n",
        "               max_length=max_length)\n",
        "\n",
        "def evaluate_model(model, dl, device):\n",
        "    model.eval()\n",
        "    metric = evaluate.load(\"accuracy\")  # You can switch to CoLA MCC if you prefer\n",
        "    preds, labs = [], []\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids = b['input_ids'].to(device)\n",
        "            mask = b['attention_mask'].to(device)\n",
        "            labs.extend(b['labels'].cpu().numpy())\n",
        "            out = model(input_ids=ids, attention_mask=mask)\n",
        "            preds.extend(torch.argmax(out.logits, -1).cpu().numpy())\n",
        "    return metric.compute(predictions=preds, references=labs)[\"accuracy\"]\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 6) Training Stages (Jacobian-Deviation scoring)\n",
        "# ========================================================\n",
        "def full_finetuning(train_loader, dev_loader, device):\n",
        "    print(\"=== Stage 1: Full Finetuning & Jacobian-Deviation Estimation ===\")\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\n",
        "        \"roberta-base\", num_labels=2\n",
        "    ).to(device)\n",
        "    model.gradient_checkpointing_enable()\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "    sched = get_linear_schedule_with_warmup(opt,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(train_loader)*6)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    hooks, activations = register_jac_hooks(model)\n",
        "    last_jac = None\n",
        "\n",
        "    for epoch in range(6):\n",
        "        jac_sums, jac_counts = defaultdict(float), defaultdict(int)\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                out = model(input_ids=b['input_ids'].to(device),\n",
        "                            attention_mask=b['attention_mask'].to(device),\n",
        "                            labels=b['labels'].to(device))\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "\n",
        "            # Per-batch ||J - I||^2 scores (finite difference)\n",
        "            batch_jac = compute_batch_jacdev(model, activations, eps=1e-3, k_probes=1)\n",
        "            for idx, v in batch_jac.items():\n",
        "                jac_sums[idx]   += v\n",
        "                jac_counts[idx] += 1\n",
        "\n",
        "        epoch_jac = {idx: jac_sums[idx] / max(1, jac_counts[idx]) for idx in jac_sums}\n",
        "        print(f\"[Epoch {epoch+1}] approx ||J-I||^2:\", {k: round(v, 6) for k, v in epoch_jac.items()})\n",
        "        last_jac = epoch_jac\n",
        "\n",
        "    acc = evaluate_model(model, dev_loader, device)\n",
        "    print(f\"-> Full Finetune CoLA Acc: {acc:.4f}\")\n",
        "\n",
        "    remove_hooks(hooks)\n",
        "    return model, last_jac\n",
        "\n",
        "\n",
        "def prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores, num_prune=4):\n",
        "    print(\"=== Stage 2: Prune (Low-||J-I||) & Finetuning ===\")\n",
        "    prune_idxs = prune_jac_layers(model, jac_scores, num_prune=num_prune)\n",
        "    print(\"Pruned layers (lowest ||J-I||):\", prune_idxs)\n",
        "\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "    sched = get_linear_schedule_with_warmup(opt,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(train_loader)*3)\n",
        "\n",
        "    for epoch in range(5):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            out = model(input_ids=b['input_ids'].to(device),\n",
        "                        attention_mask=b['attention_mask'].to(device),\n",
        "                        labels=b['labels'].to(device))\n",
        "            out.loss.backward()\n",
        "            opt.step()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[Prune FT Epoch {epoch+1}] CoLA Acc: {acc:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def lora_only_finetuning(model, train_loader, dev_loader, device, r=2, alpha=1.0):\n",
        "    print(\"=== Stage 3: LoRA Finetuning ===\")\n",
        "    torch.cuda.empty_cache()\n",
        "    loras = apply_lora_to_all_layers(model, r, alpha)\n",
        "    for p in model.roberta.parameters(): p.requires_grad=False\n",
        "    for p in model.classifier.parameters(): p.requires_grad=True\n",
        "    for l in loras.values():\n",
        "        l.A.requires_grad=True\n",
        "        l.B.requires_grad=True\n",
        "\n",
        "    opt   = torch.optim.Adam(\n",
        "        list(model.classifier.parameters())\n",
        "        + [p for l in loras.values() for p in (l.A, l.B)],\n",
        "        lr=2e-5\n",
        "    )\n",
        "    sched = get_linear_schedule_with_warmup(opt,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(train_loader)*6)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(6):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                out = model(input_ids=b['input_ids'].to(device),\n",
        "                            attention_mask=b['attention_mask'].to(device),\n",
        "                            labels=b['labels'].to(device))\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[LoRA Epoch {epoch+1}] CoLA Acc: {acc:.4f}\")\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 7) Main Entrypoint\n",
        "# ========================================================\n",
        "def main():\n",
        "    seed = 42\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # load & preprocess CoLA subset\n",
        "    train_ds = load_dataset(\"glue\", \"cola\", split=\"train\").shuffle(seed).select(range(5000))\n",
        "    dev_ds   = load_dataset(\"glue\", \"cola\", split=\"validation\")\n",
        "\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "    train = train_ds.map(lambda ex: tokenizer(ex[\"sentence\"], truncation=True, padding='max_length', max_length=64),\n",
        "                         batched=True)\\\n",
        "                    .rename_column(\"label\", \"labels\")\\\n",
        "                    .remove_columns([\"sentence\", \"idx\"])\n",
        "    dev = dev_ds.map(lambda ex: tokenizer(ex[\"sentence\"], truncation=True, padding='max_length', max_length=64),\n",
        "                     batched=True)\\\n",
        "                .rename_column(\"label\", \"labels\")\\\n",
        "                .remove_columns([\"sentence\", \"idx\"])\n",
        "\n",
        "    collator     = DataCollatorWithPadding(tokenizer, padding=\"max_length\", max_length=64)\n",
        "    train_loader = DataLoader(train, batch_size=8, shuffle=True,  collate_fn=collator)\n",
        "    dev_loader   = DataLoader(dev,   batch_size=16, shuffle=False, collate_fn=collator)\n",
        "\n",
        "    model, jac_scores = full_finetuning(train_loader, dev_loader, device)\n",
        "\n",
        "    # Preserve first & last layers during pruning. Adjust num_prune as desired.\n",
        "    model = prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores, num_prune=4)\n",
        "\n",
        "    lora_only_finetuning(model, train_loader, dev_loader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0LfxZC4KwvrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Jacobian for CoLA (Autograd JVP version)\n",
        "\n",
        "# ========================================================\n",
        "# 1) Standard imports and warning suppression\n",
        "# ========================================================\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from transformers import (\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizerFast,\n",
        "    DataCollatorWithPadding,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "import evaluate\n",
        "import warnings\n",
        "from contextlib import contextmanager  # <-- correct import\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# ========================================================\n",
        "# 1.1) Utility: force 'math' attention kernels during JVP\n",
        "# ========================================================\n",
        "@contextmanager\n",
        "def force_math_sdp():\n",
        "    \"\"\"\n",
        "    Force 'math' scaled-dot-product attention so higher-order grads/JVP work.\n",
        "    No-op on CPU.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        with torch.backends.cuda.sdp_kernel(\n",
        "            enable_flash=False, enable_math=True, enable_mem_efficient=False\n",
        "        ):\n",
        "            yield\n",
        "    else:\n",
        "        yield\n",
        "\n",
        "# ========================================================\n",
        "# 2) Jacobian-Deviation / Hook Utilities (per-layer J ~ I)\n",
        "#    Uses autograd JVP, not finite differences.\n",
        "# ========================================================\n",
        "def register_jac_hooks(model):\n",
        "    \"\"\"\n",
        "    Capture each RobertaLayer's input (hidden_states) + the rest of its args\n",
        "    so we can re-run the layer locally and compute JVPs.\n",
        "      activations[idx]['X']    = hidden_states (detached)\n",
        "      activations[idx]['args'] = tuple of non-input args (kept as-is)\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    activations = {i: {'X': None, 'args': None} for i in range(len(layers))}\n",
        "    hooks = []\n",
        "\n",
        "    for i, layer in enumerate(layers):\n",
        "        def pre_hook(module, inputs, idx=i):\n",
        "            xs = inputs[0]\n",
        "            extra = tuple(inputs[1:]) if len(inputs) > 1 else tuple()\n",
        "            activations[idx]['X'] = xs.detach()\n",
        "            activations[idx]['args'] = extra\n",
        "        hooks.append(layer.register_forward_pre_hook(pre_hook))\n",
        "\n",
        "    return hooks, activations\n",
        "\n",
        "def remove_hooks(hooks):\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "def compute_batch_jacdev_autograd(model, acts, k_probes=2, rademacher=True):\n",
        "    \"\"\"\n",
        "    For each layer ℓ, estimate E_v ||(Jℓ - I)v||^2 using autograd JVP:\n",
        "        _, Jv = jvp(f, (X,), (v,));  (J - I)v = Jv - v\n",
        "    Average squared norm over k_probes random v.\n",
        "    Returns {layer_idx: score}.\n",
        "    \"\"\"\n",
        "    from torch.autograd.functional import jvp\n",
        "    layers = model.roberta.encoder.layer\n",
        "    scores = {}\n",
        "\n",
        "    for idx, buf in acts.items():\n",
        "        X = buf['X']; args = buf['args']\n",
        "        if X is None:\n",
        "            continue\n",
        "\n",
        "        X = X.detach().requires_grad_(True)\n",
        "        layer = layers[idx]\n",
        "        was_training = layer.training\n",
        "        layer.eval()  # turn off dropout for determinism\n",
        "\n",
        "        try:\n",
        "            def f(inp):\n",
        "                out = layer(inp, *args)\n",
        "                return out[0] if isinstance(out, tuple) else out\n",
        "\n",
        "            acc = 0.0\n",
        "            # Avoid efficient SDPA kernels lacking higher-order grads\n",
        "            with force_math_sdp():\n",
        "                for _ in range(k_probes):\n",
        "                    if rademacher:\n",
        "                        v = torch.empty_like(X).bernoulli_(0.5).mul_(2).sub_(1)  # ±1\n",
        "                    else:\n",
        "                        v = torch.randn_like(X)\n",
        "\n",
        "                    _, Jv = jvp(f, (X,), (v,), create_graph=False, strict=True)\n",
        "                    jd_vec = Jv - v\n",
        "                    acc += float(jd_vec.pow(2).mean().item())\n",
        "\n",
        "            scores[idx] = acc / k_probes\n",
        "        finally:\n",
        "            layer.train(was_training)\n",
        "\n",
        "        # clear buffers for next batch\n",
        "        buf['X'] = None\n",
        "        buf['args'] = None\n",
        "\n",
        "    return scores\n",
        "\n",
        "# ========================================================\n",
        "# 3) Pruning Utilities with SkipFF (prune low-||J-I||)\n",
        "# ========================================================\n",
        "class SkipFF(nn.Module):\n",
        "    def forward(self, hidden_states, input_tensor=None):\n",
        "        return input_tensor  # residual passthrough\n",
        "\n",
        "def prune_jac_layers(model, jac_scores, num_prune=4):\n",
        "    \"\"\"\n",
        "    Prune layers with the lowest Jacobian deviation (closest to identity).\n",
        "    We remove the FFN by replacing intermediate.dense with Identity\n",
        "    and the output block with SkipFF (residual passthrough).\n",
        "    \"\"\"\n",
        "    sorted_layers = sorted(jac_scores.items(), key=lambda x: x[1])  # lowest first\n",
        "    prune_idxs = [idx for idx, _ in sorted_layers[:num_prune]]\n",
        "    for idx in prune_idxs:\n",
        "        layer = model.roberta.encoder.layer[idx]\n",
        "        layer.intermediate.dense = nn.Identity()\n",
        "        layer.output = SkipFF()\n",
        "    return prune_idxs\n",
        "\n",
        "# ========================================================\n",
        "# 4) LoRA Modules (unchanged)\n",
        "# ========================================================\n",
        "class LoRA(nn.Module):\n",
        "    def __init__(self, W0, r=2, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"W0\", W0.clone().detach())\n",
        "        L, M = W0.shape\n",
        "        self.B = nn.Parameter(torch.randn(L, r) * 0.01)\n",
        "        self.A = nn.Parameter(torch.zeros(r, M))\n",
        "        self.scaling = alpha / r\n",
        "    def forward(self):\n",
        "        return self.W0 + self.scaling * (self.B @ self.A)\n",
        "\n",
        "def apply_lora_to_all_layers(model, r=2, alpha=1.0):\n",
        "    loras = {}\n",
        "    for idx, layer in enumerate(model.roberta.encoder.layer):\n",
        "        if not hasattr(layer.output, 'dense'):\n",
        "            continue\n",
        "        W0 = layer.output.dense.weight.data\n",
        "        lora = LoRA(W0, r, alpha).to(W0.device)\n",
        "        def fwd(x, layer=layer, lora=lora):\n",
        "            return F.linear(x, lora(), layer.output.dense.bias)\n",
        "        layer.output.dense.forward = fwd\n",
        "        loras[idx] = lora\n",
        "    return loras\n",
        "\n",
        "# ========================================================\n",
        "# 5) Data + Eval Helpers\n",
        "# ========================================================\n",
        "def preprocess_function(examples, tok, max_length=64):\n",
        "    return tok(examples['sentence'],\n",
        "               truncation=True,\n",
        "               padding='max_length',\n",
        "               max_length=max_length)\n",
        "\n",
        "def evaluate_model(model, dl, device):\n",
        "    # CoLA's official metric is MCC, but we'll keep accuracy for simplicity;\n",
        "    # swap to evaluate.load(\"glue\", \"cola\") if you want MCC.\n",
        "    model.eval()\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "    preds, labs = [], []\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids  = b['input_ids'].to(device)\n",
        "            mask = b['attention_mask'].to(device)\n",
        "            labs.extend(b['labels'].cpu().numpy())\n",
        "            out = model(input_ids=ids, attention_mask=mask)\n",
        "            preds.extend(torch.argmax(out.logits, -1).cpu().numpy())\n",
        "    return metric.compute(predictions=preds, references=labs)[\"accuracy\"]\n",
        "\n",
        "# ========================================================\n",
        "# 6) Training Stages (Jacobian-Deviation scoring)\n",
        "# ========================================================\n",
        "def full_finetuning(train_loader, dev_loader, device):\n",
        "    print(\"=== Stage 1: Full Finetuning & Jacobian-Deviation Estimation ===\")\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\n",
        "        \"roberta-base\", num_labels=2\n",
        "    ).to(device)\n",
        "    model.gradient_checkpointing_enable()\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "    sched = get_linear_schedule_with_warmup(\n",
        "        opt, num_warmup_steps=0, num_training_steps=len(train_loader)*6\n",
        "    )\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    hooks, activations = register_jac_hooks(model)\n",
        "    last_jac = None\n",
        "\n",
        "    for epoch in range(6):\n",
        "        jac_sums, jac_counts = defaultdict(float), defaultdict(int)\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                out = model(\n",
        "                    input_ids=b['input_ids'].to(device),\n",
        "                    attention_mask=b['attention_mask'].to(device),\n",
        "                    labels=b['labels'].to(device)\n",
        "                )\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "\n",
        "            # Per-batch ||J - I||^2 per layer via Autograd JVP\n",
        "            batch_jac = compute_batch_jacdev_autograd(\n",
        "                model, activations, k_probes=2, rademacher=True\n",
        "            )\n",
        "            for idx, v in batch_jac.items():\n",
        "                jac_sums[idx]   += v\n",
        "                jac_counts[idx] += 1\n",
        "\n",
        "        # epoch-averaged ||J - I||^2 per layer\n",
        "        epoch_jac = {idx: jac_sums[idx] / max(1, jac_counts[idx]) for idx in jac_sums}\n",
        "        print(f\"[Epoch {epoch+1}] approx ||J-I||^2:\",\n",
        "              {k: round(v, 6) for k, v in epoch_jac.items()})\n",
        "        last_jac = epoch_jac\n",
        "\n",
        "    acc = evaluate_model(model, dev_loader, device)\n",
        "    print(f\"-> Full Finetune CoLA Acc: {acc:.4f}\")\n",
        "\n",
        "    remove_hooks(hooks)\n",
        "    return model, last_jac\n",
        "\n",
        "def prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores):\n",
        "    print(\"=== Stage 2: Prune (Low-||J-I||) & Finetuning ===\")\n",
        "    prune_idxs = prune_jac_layers(model, jac_scores, num_prune=num)\n",
        "    print(\"Pruned layers (lowest ||J-I||):\", prune_idxs)\n",
        "\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "    sched = get_linear_schedule_with_warmup(\n",
        "        opt, num_warmup_steps=0, num_training_steps=len(train_loader)*3\n",
        "    )\n",
        "\n",
        "    for epoch in range(5):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            out = model(\n",
        "                input_ids=b['input_ids'].to(device),\n",
        "                attention_mask=b['attention_mask'].to(device),\n",
        "                labels=b['labels'].to(device)\n",
        "            )\n",
        "            out.loss.backward()\n",
        "            opt.step()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[Prune FT Epoch {epoch+1}] CoLA Acc: {acc:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def lora_only_finetuning(model, train_loader, dev_loader, device, r=2, alpha=1.0):\n",
        "    print(\"=== Stage 3: LoRA Finetuning ===\")\n",
        "    torch.cuda.empty_cache()\n",
        "    loras = apply_lora_to_all_layers(model, r, alpha)\n",
        "    for p in model.roberta.parameters():  p.requires_grad = False\n",
        "    for p in model.classifier.parameters(): p.requires_grad = True\n",
        "    for l in loras.values():\n",
        "        l.A.requires_grad = True\n",
        "        l.B.requires_grad = True\n",
        "\n",
        "    opt   = torch.optim.Adam(\n",
        "        list(model.classifier.parameters()) + [p for l in loras.values() for p in (l.A, l.B)],\n",
        "        lr=2e-5\n",
        "    )\n",
        "    sched = get_linear_schedule_with_warmup(\n",
        "        opt, num_warmup_steps=0, num_training_steps=len(train_loader)*6\n",
        "    )\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(6):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                out = model(\n",
        "                    input_ids=b['input_ids'].to(device),\n",
        "                    attention_mask=b['attention_mask'].to(device),\n",
        "                    labels=b['labels'].to(device)\n",
        "                )\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[LoRA Epoch {epoch+1}] CoLA Acc: {acc:.4f}\")\n",
        "\n",
        "# ========================================================\n",
        "# 7) Main Entrypoint\n",
        "# ========================================================\n",
        "def main():\n",
        "    seed = 42\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # load & preprocess CoLA subset\n",
        "    train_ds = load_dataset(\"glue\", \"cola\", split=\"train\").shuffle(seed).select(range(5000))\n",
        "    dev_ds   = load_dataset(\"glue\", \"cola\", split=\"validation\")\n",
        "\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "    train = train_ds.map(lambda ex: preprocess_function(ex, tokenizer),\n",
        "                         batched=True)\\\n",
        "                    .rename_column(\"label\", \"labels\")\\\n",
        "                    .remove_columns([\"sentence\", \"idx\"])\n",
        "    dev = dev_ds.map(lambda ex: preprocess_function(ex, tokenizer),\n",
        "                     batched=True)\\\n",
        "                .rename_column(\"label\", \"labels\")\\\n",
        "                .remove_columns([\"sentence\", \"idx\"])\n",
        "\n",
        "    collator     = DataCollatorWithPadding(tokenizer, padding=\"max_length\", max_length=64)\n",
        "    train_loader = DataLoader(train, batch_size=8, shuffle=True,  collate_fn=collator)\n",
        "    dev_loader   = DataLoader(dev,   batch_size=16, shuffle=False, collate_fn=collator)\n",
        "\n",
        "    model, jac_scores = full_finetuning(train_loader, dev_loader, device)\n",
        "    model = prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores)\n",
        "    lora_only_finetuning(model, train_loader, dev_loader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "tWSUWRUgJ5Vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fFqAgYvzxzfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ISJiDnozx0cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GUXz-L1DiH-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Jacobian Deviation for QNLI\n",
        "\n",
        "# ========================================================\n",
        "# 1) Standard imports and warning suppression\n",
        "# ========================================================\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from transformers import (\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizerFast,\n",
        "    DataCollatorWithPadding,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "import evaluate\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 2) Jacobian-Deviation / Hook Utilities (per-layer)\n",
        "# ========================================================\n",
        "def register_jac_hooks(model):\n",
        "    \"\"\"\n",
        "    Capture each RobertaLayer's input X and all non-input args.\n",
        "    We'll locally re-run the layer (with dropout disabled) to estimate JVPs.\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    activations = {i: {'X': None, 'args': None} for i in range(len(layers))}\n",
        "    hooks = []\n",
        "\n",
        "    for i, layer in enumerate(layers):\n",
        "        def pre_hook(module, inputs, idx=i):\n",
        "            x = inputs[0]\n",
        "            extra = tuple(inputs[1:]) if len(inputs) > 1 else tuple()\n",
        "            activations[idx]['X'] = x.detach()\n",
        "            activations[idx]['args'] = extra\n",
        "\n",
        "        hooks.append(layer.register_forward_pre_hook(pre_hook))\n",
        "\n",
        "    return hooks, activations\n",
        "\n",
        "def remove_hooks(hooks):\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "@torch.no_grad()\n",
        "def _layer_forward_only(layer, x, args, use_amp=False):\n",
        "    # Call the layer and return the hidden states (0th output)\n",
        "    if use_amp:\n",
        "        with autocast():\n",
        "            out = layer(x, *args)\n",
        "    else:\n",
        "        out = layer(x, *args)\n",
        "    return out[0] if isinstance(out, tuple) else out\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_batch_jacdev(model, acts, eps_fp32=1e-3, eps_fp16=1e-2, k_probes=1):\n",
        "    \"\"\"\n",
        "    For each layer ℓ, estimate  ||J_ℓ - I||_F^2 via finite-difference JVPs:\n",
        "        (J - I)v  ≈  (f(x + eps v) - f(x))/eps - v\n",
        "    Average squared norm over k_probes random v and all elements.\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    scores = {}\n",
        "\n",
        "    for idx, buf in acts.items():\n",
        "        X = buf['X']; args = buf['args']\n",
        "        if X is None:\n",
        "            continue\n",
        "\n",
        "        layer = layers[idx]\n",
        "        was_training = layer.training\n",
        "        layer.eval()  # disable dropout for stable finite-diff\n",
        "\n",
        "        try:\n",
        "            # Choose epsilon based on dtype stability\n",
        "            dtype = X.dtype\n",
        "            use_amp = dtype in (torch.float16, torch.bfloat16)\n",
        "            eps = eps_fp16 if use_amp else eps_fp32\n",
        "\n",
        "            # Base output with dropout off, matching dtype\n",
        "            x0 = X\n",
        "            y0 = _layer_forward_only(layer, x0, args, use_amp=use_amp)\n",
        "\n",
        "            acc = 0.0\n",
        "            for _ in range(k_probes):\n",
        "                v = torch.randn_like(x0)\n",
        "\n",
        "                # Optional normalization to keep step scale reasonable\n",
        "                v = v / (v.std() + 1e-6)\n",
        "\n",
        "                y_eps = _layer_forward_only(layer, x0 + eps * v, args, use_amp=use_amp)\n",
        "                jd_vec = (y_eps - y0) / eps - v\n",
        "                acc += float(jd_vec.pow(2).mean().item())\n",
        "\n",
        "            scores[idx] = acc / k_probes\n",
        "\n",
        "        finally:\n",
        "            layer.train(was_training)\n",
        "\n",
        "        # Clear buffers\n",
        "        buf['X'] = None\n",
        "        buf['args'] = None\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 3) Pruning Utilities with SkipFF (prune low-||J-I||)\n",
        "# ========================================================\n",
        "class SkipFF(nn.Module):\n",
        "    def forward(self, hidden_states, input_tensor=None):\n",
        "        # residual passthrough\n",
        "        return input_tensor\n",
        "\n",
        "def prune_jac_layers(model, jac_scores, num_prune=4):\n",
        "    \"\"\"\n",
        "    Prune layers whose mappings are closest to identity (lowest ||J-I||).\n",
        "    We remove the FFN by replacing intermediate.dense with Identity\n",
        "    and the output block with SkipFF (residual passthrough).\n",
        "    \"\"\"\n",
        "    sorted_layers = sorted(jac_scores.items(), key=lambda x: x[1])  # lowest first\n",
        "    prune_idxs = [idx for idx, _ in sorted_layers[:num_prune]]\n",
        "    for idx in prune_idxs:\n",
        "        layer = model.roberta.encoder.layer[idx]\n",
        "        layer.intermediate.dense = nn.Identity()\n",
        "        layer.output = SkipFF()\n",
        "    return prune_idxs\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 4) LoRA Modules (unchanged)\n",
        "# ========================================================\n",
        "class LoRA(nn.Module):\n",
        "    def __init__(self, W0, r=2, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"W0\", W0.clone().detach())\n",
        "        L, M = W0.shape\n",
        "        self.B = nn.Parameter(torch.randn(L, r) * 0.01)\n",
        "        self.A = nn.Parameter(torch.zeros(r, M))\n",
        "        self.scaling = alpha / r\n",
        "    def forward(self):\n",
        "        return self.W0 + self.scaling * (self.B @ self.A)\n",
        "\n",
        "def apply_lora_to_all_layers(model, r=2, alpha=1.0):\n",
        "    loras = {}\n",
        "    for idx, layer in enumerate(model.roberta.encoder.layer):\n",
        "        if not hasattr(layer.output, 'dense'):\n",
        "            continue\n",
        "        W0 = layer.output.dense.weight.data\n",
        "        lora = LoRA(W0, r, alpha).to(W0.device)\n",
        "        def fwd(x, layer=layer, lora=lora):\n",
        "            return F.linear(x, lora(), layer.output.dense.bias)\n",
        "        layer.output.dense.forward = fwd\n",
        "        loras[idx] = lora\n",
        "    return loras\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 5) Data + Eval Helpers\n",
        "# ========================================================\n",
        "def preprocess_function(examples, tok, max_length=128):\n",
        "    return tok(examples['question'],\n",
        "               examples['sentence'],\n",
        "               truncation=True,\n",
        "               padding='max_length',\n",
        "               max_length=max_length)\n",
        "\n",
        "def evaluate_model(model, dl, device):\n",
        "    model.eval()\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "    preds, labs = [], []\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids = b['input_ids'].to(device)\n",
        "            mask = b['attention_mask'].to(device)\n",
        "            labs.extend(b['labels'].cpu().numpy())\n",
        "            out = model(input_ids=ids, attention_mask=mask)\n",
        "            preds.extend(torch.argmax(out.logits, -1).cpu().numpy())\n",
        "    return metric.compute(predictions=preds, references=labs)[\"accuracy\"]\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 6) Training Stages (Jacobian-Deviation scoring)\n",
        "# ========================================================\n",
        "def full_finetuning(train_loader, dev_loader, device):\n",
        "    print(\"=== Stage 1: Full Finetuning & Jacobian-Deviation Estimation ===\")\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\n",
        "        \"roberta-base\", num_labels=2\n",
        "    ).to(device)\n",
        "    model.gradient_checkpointing_enable()\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "    sched = get_linear_schedule_with_warmup(opt,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(train_loader)*6)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    hooks, activations = register_jac_hooks(model)\n",
        "    last_jac = None\n",
        "\n",
        "    for epoch in range(6):\n",
        "        jac_sums, jac_counts = defaultdict(float), defaultdict(int)\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad()\n",
        "            with autocast():\n",
        "                out = model(input_ids=b['input_ids'].to(device),\n",
        "                            attention_mask=b['attention_mask'].to(device),\n",
        "                            labels=b['labels'].to(device))\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "\n",
        "            # Per-batch ||J - I||^2 per layer\n",
        "            batch_jac = compute_batch_jacdev(model, activations, eps_fp32=1e-3, eps_fp16=1e-2, k_probes=1)\n",
        "            for idx, v in batch_jac.items():\n",
        "                jac_sums[idx]   += v\n",
        "                jac_counts[idx] += 1\n",
        "\n",
        "        epoch_jac = {idx: jac_sums[idx]/max(1, jac_counts[idx]) for idx in jac_sums}\n",
        "        print(f\"[Epoch {epoch+1}] approx ||J-I||^2:\", {k: round(v, 6) for k, v in epoch_jac.items()})\n",
        "        last_jac = epoch_jac\n",
        "\n",
        "    acc = evaluate_model(model, dev_loader, device)\n",
        "    print(f\"-> Full Finetune QNLI Acc: {acc:.4f}\")\n",
        "\n",
        "    remove_hooks(hooks)\n",
        "    return model, last_jac\n",
        "\n",
        "\n",
        "def prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores):\n",
        "    print(\"=== Stage 2: Prune (Low-||J-I||) & Finetuning ===\")\n",
        "    prune_idxs = prune_jac_layers(model, jac_scores, num_prune=num)\n",
        "    print(\"Pruned layers (lowest ||J-I||):\", prune_idxs)\n",
        "\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "    sched = get_linear_schedule_with_warmup(opt,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(train_loader)*3)\n",
        "\n",
        "    for epoch in range(5):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad()\n",
        "            out = model(input_ids=b['input_ids'].to(device),\n",
        "                        attention_mask=b['attention_mask'].to(device),\n",
        "                        labels=b['labels'].to(device))\n",
        "            out.loss.backward()\n",
        "            opt.step()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[Prune FT Epoch {epoch+1}] QNLI Acc: {acc:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def lora_only_finetuning(model, train_loader, dev_loader, device, r=2, alpha=1.0):\n",
        "    print(\"=== Stage 3: LoRA Finetuning ===\")\n",
        "    torch.cuda.empty_cache()\n",
        "    loras = apply_lora_to_all_layers(model, r, alpha)\n",
        "    for p in model.roberta.parameters(): p.requires_grad=False\n",
        "    for p in model.classifier.parameters(): p.requires_grad=True\n",
        "    for l in loras.values():\n",
        "        l.A.requires_grad=True\n",
        "        l.B.requires_grad=True\n",
        "\n",
        "    opt   = torch.optim.Adam(\n",
        "        list(model.classifier.parameters())\n",
        "        + [p for l in loras.values() for p in (l.A, l.B)],\n",
        "        lr=2e-5\n",
        "    )\n",
        "    sched = get_linear_schedule_with_warmup(opt,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(train_loader)*6)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(6):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad()\n",
        "            with autocast():\n",
        "                out = model(input_ids=b['input_ids'].to(device),\n",
        "                            attention_mask=b['attention_mask'].to(device),\n",
        "                            labels=b['labels'].to(device))\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[LoRA Epoch {epoch+1}] QNLI Acc: {acc:.4f}\")\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 7) Main Entrypoint\n",
        "# ========================================================\n",
        "def main():\n",
        "    seed = 42\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load & preprocess QNLI\n",
        "    train_ds = load_dataset(\"glue\", \"qnli\", split=\"train\").shuffle(seed).select(range(5000))\n",
        "    dev_ds   = load_dataset(\"glue\", \"qnli\", split=\"validation\")\n",
        "\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "    def preprocess(examples):\n",
        "        return tokenizer(examples[\"question\"],\n",
        "                         examples[\"sentence\"],\n",
        "                         truncation=True,\n",
        "                         padding='max_length',\n",
        "                         max_length=128)\n",
        "\n",
        "    train = train_ds.map(preprocess, batched=True)\\\n",
        "                    .rename_column(\"label\", \"labels\")\\\n",
        "                    .remove_columns([\"question\", \"sentence\", \"idx\"])\n",
        "    dev = dev_ds.map(preprocess, batched=True)\\\n",
        "                .rename_column(\"label\", \"labels\")\\\n",
        "                .remove_columns([\"question\", \"sentence\", \"idx\"])\n",
        "\n",
        "    collator     = DataCollatorWithPadding(tokenizer, padding=\"max_length\", max_length=128)\n",
        "    train_loader = DataLoader(train, batch_size=8, shuffle=True, collate_fn=collator)\n",
        "    dev_loader   = DataLoader(dev,   batch_size=16, shuffle=False, collate_fn=collator)\n",
        "\n",
        "    model, jac_scores = full_finetuning(train_loader, dev_loader, device)\n",
        "    model = prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores)\n",
        "    lora_only_finetuning(model, train_loader, dev_loader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SafS-s9PIStN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Jacobian Deviation for QNLI (Autograd JVP version)\n",
        "# ========================================================\n",
        "# 1) Standard imports and warning suppression\n",
        "# ========================================================\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from transformers import (\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizerFast,\n",
        "    DataCollatorWithPadding,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "import evaluate\n",
        "import warnings\n",
        "from contextlib import contextmanager  # <-- correct import\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# ========================================================\n",
        "# 1.1) Utility: force 'math' attention kernels during JVP\n",
        "# ========================================================\n",
        "@contextmanager\n",
        "def force_math_sdp():\n",
        "    \"\"\"\n",
        "    Force 'math' scaled-dot-product attention so higher-order grads/JVP work.\n",
        "    No-op on CPU.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        with torch.backends.cuda.sdp_kernel(\n",
        "            enable_flash=False, enable_math=True, enable_mem_efficient=False\n",
        "        ):\n",
        "            yield\n",
        "    else:\n",
        "        yield\n",
        "\n",
        "# ========================================================\n",
        "# 2) Jacobian-Deviation / Hook Utilities (per-layer)\n",
        "# ========================================================\n",
        "def register_jac_hooks(model):\n",
        "    \"\"\"\n",
        "    Capture each RobertaLayer's input X and all non-input args.\n",
        "    We'll locally re-run the layer (with dropout disabled) to estimate JVPs.\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    activations = {i: {'X': None, 'args': None} for i in range(len(layers))}\n",
        "    hooks = []\n",
        "\n",
        "    for i, layer in enumerate(layers):\n",
        "        def pre_hook(module, inputs, idx=i):\n",
        "            x = inputs[0]\n",
        "            extra = tuple(inputs[1:]) if len(inputs) > 1 else tuple()\n",
        "            activations[idx]['X'] = x.detach()\n",
        "            activations[idx]['args'] = extra\n",
        "        hooks.append(layer.register_forward_pre_hook(pre_hook))\n",
        "\n",
        "    return hooks, activations\n",
        "\n",
        "def remove_hooks(hooks):\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "def compute_batch_jacdev_autograd(model, acts, k_probes=2, rademacher=True):\n",
        "    \"\"\"\n",
        "    For each layer ℓ, estimate  E_v ||(J_ℓ - I)v||^2  using autograd JVP:\n",
        "        _, Jv = jvp(f, (X,), (v,));  (J - I)v = Jv - v\n",
        "    Average squared norm over k_probes random v and all elements.\n",
        "    Returns dict: {layer_idx: score}\n",
        "    \"\"\"\n",
        "    from torch.autograd.functional import jvp\n",
        "    layers = model.roberta.encoder.layer\n",
        "    scores = {}\n",
        "\n",
        "    for idx, buf in acts.items():\n",
        "        X, args = buf['X'], buf['args']\n",
        "        if X is None:\n",
        "            continue\n",
        "\n",
        "        X = X.detach().requires_grad_(True)\n",
        "        layer = layers[idx]\n",
        "        was_training = layer.training\n",
        "        layer.eval()  # disable dropout for stable JVP\n",
        "\n",
        "        try:\n",
        "            def f(inp):\n",
        "                out = layer(inp, *args)\n",
        "                return out[0] if isinstance(out, tuple) else out\n",
        "\n",
        "            acc = 0.0\n",
        "            # Avoid efficient SDPA kernels that lack higher-order grads\n",
        "            with force_math_sdp():\n",
        "                for _ in range(k_probes):\n",
        "                    if rademacher:\n",
        "                        v = torch.empty_like(X).bernoulli_(0.5).mul_(2).sub_(1)  # ±1 probes\n",
        "                    else:\n",
        "                        v = torch.randn_like(X)\n",
        "                    _, Jv = jvp(f, (X,), (v,), create_graph=False, strict=True)\n",
        "                    jd_vec = Jv - v\n",
        "                    acc += float(jd_vec.pow(2).mean().item())\n",
        "\n",
        "            scores[idx] = acc / k_probes\n",
        "        finally:\n",
        "            layer.train(was_training)\n",
        "\n",
        "        # Clear buffers\n",
        "        buf['X'] = None\n",
        "        buf['args'] = None\n",
        "\n",
        "    return scores\n",
        "\n",
        "# ========================================================\n",
        "# 3) Pruning Utilities with SkipFF (prune low-||J-I||) — preserves first & last\n",
        "# ========================================================\n",
        "class SkipFF(nn.Module):\n",
        "    def forward(self, hidden_states, input_tensor=None):\n",
        "        # residual passthrough\n",
        "        return input_tensor\n",
        "\n",
        "def prune_jac_layers(model, jac_scores, num_prune=4):\n",
        "    \"\"\"\n",
        "    Prune layers whose mappings are closest to identity (lowest ||J-I||) while\n",
        "    preserving the first (idx=0) and last (idx=L-1) Transformer blocks.\n",
        "\n",
        "    Implementation details:\n",
        "      • Exclude idx in {0, L-1} from eligibility\n",
        "      • If jac_scores is missing some eligible layers, skip them safely\n",
        "      • If num_prune > eligible_count, prune as many as possible\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    L = len(layers)\n",
        "    preserve = {0, L - 1}\n",
        "\n",
        "    # Filter to eligible layers only\n",
        "    eligible = [(idx, score) for idx, score in jac_scores.items() if idx not in preserve and 0 <= idx < L]\n",
        "    if not eligible:\n",
        "        print(\"[Prune] No eligible layers to prune (after preserving first/last).\")\n",
        "        return []\n",
        "\n",
        "    # Sort by lowest ||J-I|| first\n",
        "    eligible_sorted = sorted(eligible, key=lambda x: x[1])\n",
        "\n",
        "    # Take up to num_prune indices\n",
        "    prune_idxs = [idx for idx, _ in eligible_sorted[:max(0, min(num_prune, len(eligible_sorted)))]]\n",
        "\n",
        "    # Apply structural pruning to selected layers\n",
        "    for idx in prune_idxs:\n",
        "        layer = layers[idx]\n",
        "        # Remove FFN contribution by bypassing through the residual\n",
        "        layer.intermediate.dense = nn.Identity()\n",
        "        layer.output = SkipFF()\n",
        "\n",
        "    print(f\"[Prune] Preserved layers: {sorted(list(preserve))}; Pruned layers: {sorted(prune_idxs)}\")\n",
        "    return prune_idxs\n",
        "\n",
        "# ========================================================\n",
        "# 4) LoRA Modules (unchanged)\n",
        "# ========================================================\n",
        "class LoRA(nn.Module):\n",
        "    def __init__(self, W0, r=2, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"W0\", W0.clone().detach())\n",
        "        L, M = W0.shape\n",
        "        self.B = nn.Parameter(torch.randn(L, r) * 0.01)\n",
        "        self.A = nn.Parameter(torch.zeros(r, M))\n",
        "        self.scaling = alpha / r\n",
        "    def forward(self):\n",
        "        return self.W0 + self.scaling * (self.B @ self.A)\n",
        "\n",
        "def apply_lora_to_all_layers(model, r=2, alpha=1.0):\n",
        "    loras = {}\n",
        "    for idx, layer in enumerate(model.roberta.encoder.layer):\n",
        "        if not hasattr(layer.output, 'dense'):\n",
        "            continue\n",
        "        W0 = layer.output.dense.weight.data\n",
        "        lora = LoRA(W0, r, alpha).to(W0.device)\n",
        "        def fwd(x, layer=layer, lora=lora):\n",
        "            return F.linear(x, lora(), layer.output.dense.bias)\n",
        "        layer.output.dense.forward = fwd\n",
        "        loras[idx] = lora\n",
        "    return loras\n",
        "\n",
        "# ========================================================\n",
        "# 5) Data + Eval Helpers\n",
        "# ========================================================\n",
        "def preprocess_function(examples, tok, max_length=128):\n",
        "    return tok(examples['question'],\n",
        "               examples['sentence'],\n",
        "               truncation=True,\n",
        "               padding='max_length',\n",
        "               max_length=max_length)\n",
        "\n",
        "def evaluate_model(model, dl, device):\n",
        "    model.eval()\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "    preds, labs = [], []\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids = b['input_ids'].to(device)\n",
        "            mask = b['attention_mask'].to(device)\n",
        "            labs.extend(b['labels'].cpu().numpy())\n",
        "            out = model(input_ids=ids, attention_mask=mask)\n",
        "            preds.extend(torch.argmax(out.logits, -1).cpu().numpy())\n",
        "    return metric.compute(predictions=preds, references=labs)[\"accuracy\"]\n",
        "\n",
        "# ========================================================\n",
        "# 6) Training Stages (Jacobian-Deviation scoring)\n",
        "# ========================================================\n",
        "def full_finetuning(train_loader, dev_loader, device):\n",
        "    print(\"=== Stage 1: Full Finetuning & Jacobian-Deviation Estimation ===\")\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\n",
        "        \"roberta-base\", num_labels=2\n",
        "    ).to(device)\n",
        "    model.gradient_checkpointing_enable()\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "    sched = get_linear_schedule_with_warmup(\n",
        "        opt, num_warmup_steps=0, num_training_steps=len(train_loader)*6\n",
        "    )\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    hooks, activations = register_jac_hooks(model)\n",
        "    last_jac = None\n",
        "\n",
        "    for epoch in range(6):\n",
        "        jac_sums, jac_counts = defaultdict(float), defaultdict(int)\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                out = model(input_ids=b['input_ids'].to(device),\n",
        "                            attention_mask=b['attention_mask'].to(device),\n",
        "                            labels=b['labels'].to(device))\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "\n",
        "            # Per-batch ||J - I||^2 per layer (Autograd JVP)\n",
        "            batch_jac = compute_batch_jacdev_autograd(model, activations, k_probes=2, rademacher=True)\n",
        "            for idx, v in batch_jac.items():\n",
        "                jac_sums[idx]   += v\n",
        "                jac_counts[idx] += 1\n",
        "\n",
        "        epoch_jac = {idx: jac_sums[idx]/max(1, jac_counts[idx]) for idx in jac_sums}\n",
        "        print(f\"[Epoch {epoch+1}] approx ||J-I||^2:\", {k: round(v, 6) for k, v in epoch_jac.items()})\n",
        "        last_jac = epoch_jac\n",
        "\n",
        "    acc = evaluate_model(model, dev_loader, device)\n",
        "    print(f\"-> Full Finetune QNLI Acc: {acc:.4f}\")\n",
        "\n",
        "    remove_hooks(hooks)\n",
        "    return model, last_jac\n",
        "\n",
        "def prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores, num_prune=4):\n",
        "    print(\"=== Stage 2: Prune (Low-||J-I||) & Finetuning ===\")\n",
        "    prune_idxs = prune_jac_layers(model, jac_scores, num_prune=num)\n",
        "    print(\"Pruned layers (lowest ||J-I||):\", prune_idxs)\n",
        "\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "    sched = get_linear_schedule_with_warmup(\n",
        "        opt, num_warmup_steps=0, num_training_steps=len(train_loader)*3\n",
        "    )\n",
        "\n",
        "    for epoch in range(5):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            out = model(input_ids=b['input_ids'].to(device),\n",
        "                        attention_mask=b['attention_mask'].to(device),\n",
        "                        labels=b['labels'].to(device))\n",
        "            out.loss.backward()\n",
        "            opt.step()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[Prune FT Epoch {epoch+1}] QNLI Acc: {acc:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def lora_only_finetuning(model, train_loader, dev_loader, device, r=2, alpha=1.0):\n",
        "    print(\"=== Stage 3: LoRA Finetuning ===\")\n",
        "    torch.cuda.empty_cache()\n",
        "    loras = apply_lora_to_all_layers(model, r, alpha)\n",
        "    for p in model.roberta.parameters(): p.requires_grad=False\n",
        "    for p in model.classifier.parameters(): p.requires_grad=True\n",
        "    for l in loras.values():\n",
        "        l.A.requires_grad=True\n",
        "        l.B.requires_grad=True\n",
        "\n",
        "    opt   = torch.optim.Adam(\n",
        "        list(model.classifier.parameters())\n",
        "        + [p for l in loras.values() for p in (l.A, l.B)],\n",
        "        lr=2e-5\n",
        "    )\n",
        "    sched = get_linear_schedule_with_warmup(\n",
        "        opt, num_warmup_steps=0, num_training_steps=len(train_loader)*6\n",
        "    )\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(6):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                out = model(input_ids=b['input_ids'].to(device),\n",
        "                            attention_mask=b['attention_mask'].to(device),\n",
        "                            labels=b['labels'].to(device))\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[LoRA Epoch {epoch+1}] QNLI Acc: {acc:.4f}\")\n",
        "\n",
        "# ========================================================\n",
        "# 7) Main Entrypoint\n",
        "# ========================================================\n",
        "def main():\n",
        "    seed = 42\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load & preprocess QNLI\n",
        "    train_ds = load_dataset(\"glue\", \"qnli\", split=\"train\").shuffle(seed).select(range(5000))\n",
        "    dev_ds   = load_dataset(\"glue\", \"qnli\", split=\"validation\")\n",
        "\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "\n",
        "    def preprocess(examples):\n",
        "        return tokenizer(examples[\"question\"],\n",
        "                         examples[\"sentence\"],\n",
        "                         truncation=True,\n",
        "                         padding='max_length',\n",
        "                         max_length=128)\n",
        "\n",
        "    train = train_ds.map(preprocess, batched=True)\\\n",
        "                    .rename_column(\"label\", \"labels\")\\\n",
        "                    .remove_columns([\"question\", \"sentence\", \"idx\"])\n",
        "    dev = dev_ds.map(preprocess, batched=True)\\\n",
        "                .rename_column(\"label\", \"labels\")\\\n",
        "                .remove_columns([\"question\", \"sentence\", \"idx\"])\n",
        "\n",
        "    # Already padded to max_length in preprocess; default collator is fine\n",
        "    collator     = DataCollatorWithPadding(tokenizer, padding=\"max_length\")\n",
        "\n",
        "    train_loader = DataLoader(train, batch_size=8, shuffle=True, collate_fn=collator)\n",
        "    dev_loader   = DataLoader(dev,   batch_size=16, shuffle=False, collate_fn=collator)\n",
        "\n",
        "    model, jac_scores = full_finetuning(train_loader, dev_loader, device)\n",
        "\n",
        "    # >>> Preserve first & last layers during pruning; choose how many to prune <<<\n",
        "    model = prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores, num_prune=4)\n",
        "\n",
        "    lora_only_finetuning(model, train_loader, dev_loader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "hbhUaBXny8S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JwBRTiwSITOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Jacobian for QQP\n",
        "\n",
        "# ========================================================\n",
        "# 1) Standard imports and warning suppression\n",
        "# ========================================================\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from transformers import (\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizerFast,\n",
        "    DataCollatorWithPadding,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "import evaluate\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 2) Jacobian Deviation / Hook Utilities (per-layer)\n",
        "#    We capture each RobertaLayer's input (and extra args),\n",
        "#    then locally eval f(x+eps v) to estimate (J - I)v.\n",
        "# ========================================================\n",
        "def _flatten_tokens(t):\n",
        "    # Not used in Jacobian path, but kept for parity if needed later\n",
        "    if t.dim() == 3:\n",
        "        b, s, h = t.shape\n",
        "        return t.reshape(b * s, h)\n",
        "    elif t.dim() == 2:\n",
        "        return t\n",
        "    return t.view(t.size(0), -1)\n",
        "\n",
        "def register_jac_hooks(model):\n",
        "    \"\"\"\n",
        "    Capture each RobertaLayer's block input (pre) and the extra forward args.\n",
        "    We'll re-call that layer to estimate Jacobian-vector products.\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    activations = {i: {'X': None, 'args': None} for i in range(len(layers))}\n",
        "    hooks = []\n",
        "\n",
        "    for i, layer in enumerate(layers):\n",
        "        def pre_hook(module, inputs, idx=i):\n",
        "            # inputs: (hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, ...)\n",
        "            xs = inputs[0]\n",
        "            extra = tuple(inputs[1:]) if len(inputs) > 1 else tuple()\n",
        "            # store as fp32 to avoid AMP/half issues during finite diff\n",
        "            activations[idx]['X'] = xs.detach().to(torch.float32)\n",
        "            activations[idx]['args'] = extra\n",
        "\n",
        "        hooks.append(layer.register_forward_pre_hook(pre_hook))\n",
        "\n",
        "    return hooks, activations\n",
        "\n",
        "def remove_hooks(hooks):\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "@torch.no_grad()\n",
        "def _layer_forward_only(layer, x, args):\n",
        "    out = layer(x, *args)\n",
        "    return out[0] if isinstance(out, tuple) else out\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_batch_jacdev(model, acts, eps=1e-3, k_probes=1):\n",
        "    \"\"\"\n",
        "    Estimate per-layer ||J - I||_F^2 via finite differences:\n",
        "      (J - I)v ≈ (f(x + eps*v) - f(x))/eps - v\n",
        "    Average squared norm over probes and all elements.\n",
        "    Returns dict: {layer_idx: score}\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    scores = {}\n",
        "\n",
        "    for idx, buf in acts.items():\n",
        "        X = buf['X']; args = buf['args']\n",
        "        if X is None:\n",
        "            continue\n",
        "\n",
        "        layer = layers[idx]\n",
        "        # Ensure deterministic behavior during probe (disable dropout)\n",
        "        was_training = layer.training\n",
        "        layer.eval()\n",
        "        try:\n",
        "            # Base forward at fp32 (pairs with eps-perturbed forward)\n",
        "            y0 = _layer_forward_only(layer, X, args).to(torch.float32)\n",
        "\n",
        "            acc = 0.0\n",
        "            for _ in range(k_probes):\n",
        "                v = torch.randn_like(X)                         # random probe\n",
        "                y_eps = _layer_forward_only(layer, X + eps * v, args).to(torch.float32)\n",
        "                jd_vec = (y_eps - y0) / eps - v                # (J - I)v\n",
        "                acc += float(jd_vec.pow(2).mean().item())      # mean over all elts\n",
        "\n",
        "            scores[idx] = acc / k_probes\n",
        "        finally:\n",
        "            layer.train(was_training)\n",
        "\n",
        "        # clear buffers\n",
        "        buf['X'] = None\n",
        "        buf['args'] = None\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 3) Pruning Utilities with SkipFF (prune low-||J - I||)\n",
        "# ========================================================\n",
        "class SkipFF(nn.Module):\n",
        "    def forward(self, hidden_states, input_tensor=None):\n",
        "        return input_tensor  # residual passthrough\n",
        "\n",
        "def prune_jac_layers(model, jac_scores, num_prune=4):\n",
        "    \"\"\"\n",
        "    Prune layers with the lowest Jacobian deviation (closest to identity).\n",
        "    Implementation: drop FFN by Identity and route residual through.\n",
        "    \"\"\"\n",
        "    sorted_layers = sorted(jac_scores.items(), key=lambda x: x[1])  # lowest first\n",
        "    prune_idxs = [idx for idx, _ in sorted_layers[:num_prune]]\n",
        "    for idx in prune_idxs:\n",
        "        layer = model.roberta.encoder.layer[idx]\n",
        "        layer.intermediate.dense = nn.Identity()\n",
        "        layer.output = SkipFF()\n",
        "    return prune_idxs\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 4) LoRA Modules (unchanged)\n",
        "# ========================================================\n",
        "class LoRA(nn.Module):\n",
        "    def __init__(self, W0, r=2, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"W0\", W0.clone().detach())\n",
        "        L, M = W0.shape\n",
        "        self.B = nn.Parameter(torch.randn(L, r) * 0.01)\n",
        "        self.A = nn.Parameter(torch.zeros(r, M))\n",
        "        self.scaling = alpha / r\n",
        "    def forward(self):\n",
        "        return self.W0 + self.scaling * (self.B @ self.A)\n",
        "\n",
        "def apply_lora_to_all_layers(model, r=2, alpha=1.0):\n",
        "    loras = {}\n",
        "    for idx, layer in enumerate(model.roberta.encoder.layer):\n",
        "        if not hasattr(layer.output, 'dense'):\n",
        "            continue\n",
        "        W0 = layer.output.dense.weight.data\n",
        "        lora = LoRA(W0, r, alpha).to(W0.device)\n",
        "        def fwd(x, layer=layer, lora=lora):\n",
        "            return F.linear(x, lora(), layer.output.dense.bias)\n",
        "        layer.output.dense.forward = fwd\n",
        "        loras[idx] = lora\n",
        "    return loras\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 5) Data + Eval Helpers\n",
        "# ========================================================\n",
        "def preprocess_function(examples, tok, max_length=128):\n",
        "    return tok(examples['question1'],\n",
        "               examples['question2'],\n",
        "               truncation=True,\n",
        "               padding='max_length',\n",
        "               max_length=max_length)\n",
        "\n",
        "def evaluate_model(model, dl, device):\n",
        "    model.eval()\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "    preds, labs = [], []\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids = b['input_ids'].to(device)\n",
        "            mask = b['attention_mask'].to(device)\n",
        "            labs.extend(b['labels'].cpu().numpy())\n",
        "            out = model(input_ids=ids, attention_mask=mask)\n",
        "            preds.extend(torch.argmax(out.logits, -1).cpu().numpy())\n",
        "    return metric.compute(predictions=preds, references=labs)[\"accuracy\"]\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 6) Training Stages (Jacobian-Deviation scoring)\n",
        "# ========================================================\n",
        "def full_finetuning(train_loader, dev_loader, device):\n",
        "    print(\"=== Stage 1: Full Finetuning & Jacobian-Deviation Estimation ===\")\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2).to(device)\n",
        "    model.gradient_checkpointing_enable()\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "    sched = get_linear_schedule_with_warmup(opt, 0, len(train_loader)*6)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    hooks, activations = register_jac_hooks(model)\n",
        "    last_jac = None\n",
        "\n",
        "    for epoch in range(6):\n",
        "        jac_sums, jac_counts = defaultdict(float), defaultdict(int)\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                out = model(input_ids=b['input_ids'].to(device),\n",
        "                            attention_mask=b['attention_mask'].to(device),\n",
        "                            labels=b['labels'].to(device))\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "\n",
        "            # Per-batch ||J - I||^2 for all layers we saw this step\n",
        "            batch_jac = compute_batch_jacdev(model, activations, eps=1e-3, k_probes=1)\n",
        "            for idx, v in batch_jac.items():\n",
        "                jac_sums[idx]   += v\n",
        "                jac_counts[idx] += 1\n",
        "\n",
        "        epoch_jac = {idx: jac_sums[idx]/max(1, jac_counts[idx]) for idx in jac_sums}\n",
        "        print(f\"[Epoch {epoch+1}] approx ||J-I||^2:\", {k: round(v, 6) for k, v in epoch_jac.items()})\n",
        "        last_jac = epoch_jac\n",
        "\n",
        "    acc = evaluate_model(model, dev_loader, device)\n",
        "    print(f\"-> Full Finetune QQP Acc: {acc:.4f}\")\n",
        "\n",
        "    remove_hooks(hooks)\n",
        "    return model, last_jac\n",
        "\n",
        "\n",
        "def prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores):\n",
        "    print(\"=== Stage 2: Prune (Low-||J-I||) & Finetuning ===\")\n",
        "    prune_idxs = prune_jac_layers(model, jac_scores, num_prune=num)\n",
        "    print(\"Pruned layers (lowest ||J-I||):\", prune_idxs)\n",
        "\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "    sched = get_linear_schedule_with_warmup(opt, 0, len(train_loader)*3)\n",
        "\n",
        "    for epoch in range(5):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            out = model(input_ids=b['input_ids'].to(device),\n",
        "                        attention_mask=b['attention_mask'].to(device),\n",
        "                        labels=b['labels'].to(device))\n",
        "            out.loss.backward()\n",
        "            opt.step()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[Prune FT Epoch {epoch+1}] QQP Acc: {acc:.4f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def lora_only_finetuning(model, train_loader, dev_loader, device, r=2, alpha=1.0):\n",
        "    print(\"=== Stage 3: LoRA Finetuning ===\")\n",
        "    torch.cuda.empty_cache()\n",
        "    loras = apply_lora_to_all_layers(model, r, alpha)\n",
        "    for p in model.roberta.parameters(): p.requires_grad = False\n",
        "    for p in model.classifier.parameters(): p.requires_grad = True\n",
        "    for l in loras.values():\n",
        "        l.A.requires_grad = True\n",
        "        l.B.requires_grad = True\n",
        "\n",
        "    opt   = torch.optim.Adam(\n",
        "        list(model.classifier.parameters()) + [p for l in loras.values() for p in (l.A, l.B)],\n",
        "        lr=2e-5\n",
        "    )\n",
        "    sched = get_linear_schedule_with_warmup(opt, 0, len(train_loader)*6)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(6):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                out = model(input_ids=b['input_ids'].to(device),\n",
        "                            attention_mask=b['attention_mask'].to(device),\n",
        "                            labels=b['labels'].to(device))\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[LoRA Epoch {epoch+1}] QQP Acc: {acc:.4f}\")\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 7) Main Entrypoint\n",
        "# ========================================================\n",
        "def main():\n",
        "    seed = 42\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "    train_ds = load_dataset(\"glue\", \"qqp\", split=\"train\").shuffle(seed).select(range(5000))\n",
        "    dev_ds   = load_dataset(\"glue\", \"qqp\", split=\"validation\")\n",
        "\n",
        "    def preprocess(examples):\n",
        "        return tokenizer(examples[\"question1\"], examples[\"question2\"],\n",
        "                         truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "    train = train_ds.map(preprocess, batched=True)\\\n",
        "                    .rename_column(\"label\", \"labels\")\\\n",
        "                    .remove_columns([\"question1\", \"question2\", \"idx\"])\n",
        "    dev   = dev_ds.map(preprocess, batched=True)\\\n",
        "                  .rename_column(\"label\", \"labels\")\\\n",
        "                  .remove_columns([\"question1\", \"question2\", \"idx\"])\n",
        "\n",
        "    collator     = DataCollatorWithPadding(tokenizer, padding=\"max_length\", max_length=128)\n",
        "    train_loader = DataLoader(train, batch_size=8, shuffle=True,  collate_fn=collator)\n",
        "    dev_loader   = DataLoader(dev,   batch_size=16, shuffle=False, collate_fn=collator)\n",
        "\n",
        "    model, jac_scores = full_finetuning(train_loader, dev_loader, device)\n",
        "    model = prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores)\n",
        "    lora_only_finetuning(model, train_loader, dev_loader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nnz2tI-3GWlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Jacobian for QQP (Autograd JVP version)\n",
        "\n",
        "# ========================================================\n",
        "# 1) Standard imports and warning suppression\n",
        "# ========================================================\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from transformers import (\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizerFast,\n",
        "    DataCollatorWithPadding,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "import evaluate\n",
        "import warnings\n",
        "from contextlib import contextmanager  # <-- ensure correct import\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# ========================================================\n",
        "# 1.1) Utility: force 'math' attention kernels during JVP\n",
        "# ========================================================\n",
        "@contextmanager\n",
        "def force_math_sdp():\n",
        "    \"\"\"\n",
        "    Force 'math' scaled-dot-product attention so higher-order grads/JVP work.\n",
        "    No-op on CPU.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        with torch.backends.cuda.sdp_kernel(\n",
        "            enable_flash=False, enable_math=True, enable_mem_efficient=False\n",
        "        ):\n",
        "            yield\n",
        "    else:\n",
        "        yield\n",
        "\n",
        "# ========================================================\n",
        "# 2) Jacobian Deviation / Hook Utilities (per-layer)\n",
        "#    We capture each RobertaLayer's input (and extra args),\n",
        "#    then locally evaluate JVP to estimate (J - I)v.\n",
        "# ========================================================\n",
        "def register_jac_hooks(model):\n",
        "    \"\"\"\n",
        "    Capture each RobertaLayer's block input (pre) and the extra forward args.\n",
        "    We'll re-call that layer to compute autograd JVPs.\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    activations = {i: {'X': None, 'args': None} for i in range(len(layers))}\n",
        "    hooks = []\n",
        "\n",
        "    for i, layer in enumerate(layers):\n",
        "        def pre_hook(module, inputs, idx=i):\n",
        "            # inputs: (hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, ...)\n",
        "            xs = inputs[0]\n",
        "            extra = tuple(inputs[1:]) if len(inputs) > 1 else tuple()\n",
        "            activations[idx]['X'] = xs.detach()\n",
        "            activations[idx]['args'] = extra\n",
        "\n",
        "        hooks.append(layer.register_forward_pre_hook(pre_hook))\n",
        "\n",
        "    return hooks, activations\n",
        "\n",
        "def remove_hooks(hooks):\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "def compute_batch_jacdev_autograd(model, acts, k_probes=2, rademacher=True):\n",
        "    \"\"\"\n",
        "    Estimate per-layer E_v ||(J - I)v||^2 via autograd JVP:\n",
        "        _, Jv = jvp(f, (X,), (v,));  (J - I)v = Jv - v\n",
        "    Average squared norm over probes and all elements.\n",
        "    Returns dict: {layer_idx: score}\n",
        "    \"\"\"\n",
        "    from torch.autograd.functional import jvp\n",
        "    layers = model.roberta.encoder.layer\n",
        "    scores = {}\n",
        "\n",
        "    for idx, buf in acts.items():\n",
        "        X = buf['X']; args = buf['args']\n",
        "        if X is None:\n",
        "            continue\n",
        "\n",
        "        X = X.detach().requires_grad_(True)\n",
        "        layer = layers[idx]\n",
        "        was_training = layer.training\n",
        "        layer.eval()  # disable dropout for stable JVP\n",
        "\n",
        "        try:\n",
        "            def f(inp):\n",
        "                out = layer(inp, *args)\n",
        "                return out[0] if isinstance(out, tuple) else out\n",
        "\n",
        "            acc = 0.0\n",
        "            # Avoid efficient SDPA kernels that lack higher-order grads\n",
        "            with force_math_sdp():\n",
        "                for _ in range(k_probes):\n",
        "                    if rademacher:\n",
        "                        v = torch.empty_like(X).bernoulli_(0.5).mul_(2).sub_(1)  # ±1 probes\n",
        "                    else:\n",
        "                        v = torch.randn_like(X)\n",
        "                    _, Jv = jvp(f, (X,), (v,), create_graph=False, strict=True)\n",
        "                    jd_vec = Jv - v\n",
        "                    acc += float(jd_vec.pow(2).mean().item())\n",
        "\n",
        "            scores[idx] = acc / k_probes\n",
        "        finally:\n",
        "            layer.train(was_training)\n",
        "\n",
        "        # clear buffers\n",
        "        buf['X'] = None\n",
        "        buf['args'] = None\n",
        "\n",
        "    return scores\n",
        "\n",
        "# ========================================================\n",
        "# 3) Pruning Utilities with SkipFF (prune low-||J - I||)\n",
        "# ========================================================\n",
        "class SkipFF(nn.Module):\n",
        "    def forward(self, hidden_states, input_tensor=None):\n",
        "        return input_tensor  # residual passthrough\n",
        "\n",
        "def prune_jac_layers(model, jac_scores, num_prune=4):\n",
        "    \"\"\"\n",
        "    Prune layers with the lowest Jacobian deviation (closest to identity).\n",
        "    Implementation: drop FFN by Identity and route residual through.\n",
        "    \"\"\"\n",
        "    sorted_layers = sorted(jac_scores.items(), key=lambda x: x[1])  # lowest first\n",
        "    prune_idxs = [idx for idx, _ in sorted_layers[:num_prune]]\n",
        "    for idx in prune_idxs:\n",
        "        layer = model.roberta.encoder.layer[idx]\n",
        "        layer.intermediate.dense = nn.Identity()\n",
        "        layer.output = SkipFF()\n",
        "    return prune_idxs\n",
        "\n",
        "# ========================================================\n",
        "# 4) LoRA Modules (unchanged)\n",
        "# ========================================================\n",
        "class LoRA(nn.Module):\n",
        "    def __init__(self, W0, r=2, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"W0\", W0.clone().detach())\n",
        "        L, M = W0.shape\n",
        "        self.B = nn.Parameter(torch.randn(L, r) * 0.01)\n",
        "        self.A = nn.Parameter(torch.zeros(r, M))\n",
        "        self.scaling = alpha / r\n",
        "    def forward(self):\n",
        "        return self.W0 + self.scaling * (self.B @ self.A)\n",
        "\n",
        "def apply_lora_to_all_layers(model, r=2, alpha=1.0):\n",
        "    loras = {}\n",
        "    for idx, layer in enumerate(model.roberta.encoder.layer):\n",
        "        if not hasattr(layer.output, 'dense'):\n",
        "            continue\n",
        "        W0 = layer.output.dense.weight.data\n",
        "        lora = LoRA(W0, r, alpha).to(W0.device)\n",
        "        def fwd(x, layer=layer, lora=lora):\n",
        "            return F.linear(x, lora(), layer.output.dense.bias)\n",
        "        layer.output.dense.forward = fwd\n",
        "        loras[idx] = lora\n",
        "    return loras\n",
        "\n",
        "# ========================================================\n",
        "# 5) Data + Eval Helpers\n",
        "# ========================================================\n",
        "def preprocess_function(examples, tok, max_length=128):\n",
        "    return tok(examples['question1'],\n",
        "               examples['question2'],\n",
        "               truncation=True,\n",
        "               padding='max_length',\n",
        "               max_length=max_length)\n",
        "\n",
        "def evaluate_model(model, dl, device):\n",
        "    model.eval()\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "    preds, labs = [], []\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids = b['input_ids'].to(device)\n",
        "            mask = b['attention_mask'].to(device)\n",
        "            labs.extend(b['labels'].cpu().numpy())\n",
        "            out = model(input_ids=ids, attention_mask=mask)\n",
        "            preds.extend(torch.argmax(out.logits, -1).cpu().numpy())\n",
        "    return metric.compute(predictions=preds, references=labs)[\"accuracy\"]\n",
        "\n",
        "# ========================================================\n",
        "# 6) Training Stages (Jacobian-Deviation scoring)\n",
        "# ========================================================\n",
        "def full_finetuning(train_loader, dev_loader, device):\n",
        "    print(\"=== Stage 1: Full Finetuning & Jacobian-Deviation Estimation ===\")\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2).to(device)\n",
        "    model.gradient_checkpointing_enable()\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "    sched = get_linear_schedule_with_warmup(opt, 0, len(train_loader)*6)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    hooks, activations = register_jac_hooks(model)\n",
        "    last_jac = None\n",
        "\n",
        "    for epoch in range(6):\n",
        "        jac_sums, jac_counts = defaultdict(float), defaultdict(int)\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                out = model(input_ids=b['input_ids'].to(device),\n",
        "                            attention_mask=b['attention_mask'].to(device),\n",
        "                            labels=b['labels'].to(device))\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "\n",
        "            # Per-batch ||J - I||^2 for all layers we saw this step (Autograd JVP)\n",
        "            batch_jac = compute_batch_jacdev_autograd(model, activations, k_probes=2, rademacher=True)\n",
        "            for idx, v in batch_jac.items():\n",
        "                jac_sums[idx]   += v\n",
        "                jac_counts[idx] += 1\n",
        "\n",
        "        epoch_jac = {idx: jac_sums[idx]/max(1, jac_counts[idx]) for idx in jac_sums}\n",
        "        print(f\"[Epoch {epoch+1}] approx ||J-I||^2:\", {k: round(v, 6) for k, v in epoch_jac.items()})\n",
        "        last_jac = epoch_jac\n",
        "\n",
        "    acc = evaluate_model(model, dev_loader, device)\n",
        "    print(f\"-> Full Finetune QQP Acc: {acc:.4f}\")\n",
        "\n",
        "    remove_hooks(hooks)\n",
        "    return model, last_jac\n",
        "\n",
        "def prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores):\n",
        "    print(\"=== Stage 2: Prune (Low-||J-I||) & Finetuning ===\")\n",
        "    prune_idxs = prune_jac_layers(model, jac_scores, num_prune=num)\n",
        "    print(\"Pruned layers (lowest ||J-I||):\", prune_idxs)\n",
        "\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "    sched = get_linear_schedule_with_warmup(opt, 0, len(train_loader)*3)\n",
        "\n",
        "    for epoch in range(5):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            out = model(input_ids=b['input_ids'].to(device),\n",
        "                        attention_mask=b['attention_mask'].to(device),\n",
        "                        labels=b['labels'].to(device))\n",
        "            out.loss.backward()\n",
        "            opt.step()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[Prune FT Epoch {epoch+1}] QQP Acc: {acc:.4f}\")\n",
        "    return model\n",
        "\n",
        "def lora_only_finetuning(model, train_loader, dev_loader, device, r=2, alpha=1.0):\n",
        "    print(\"=== Stage 3: LoRA Finetuning ===\")\n",
        "    torch.cuda.empty_cache()\n",
        "    loras = apply_lora_to_all_layers(model, r, alpha)\n",
        "    for p in model.roberta.parameters(): p.requires_grad = False\n",
        "    for p in model.classifier.parameters(): p.requires_grad = True\n",
        "    for l in loras.values():\n",
        "        l.A.requires_grad = True\n",
        "        l.B.requires_grad = True\n",
        "\n",
        "    opt   = torch.optim.Adam(\n",
        "        list(model.classifier.parameters()) + [p for l in loras.values() for p in (l.A, l.B)],\n",
        "        lr=2e-5\n",
        "    )\n",
        "    sched = get_linear_schedule_with_warmup(opt, 0, len(train_loader)*6)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(6):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                out = model(input_ids=b['input_ids'].to(device),\n",
        "                            attention_mask=b['attention_mask'].to(device),\n",
        "                            labels=b['labels'].to(device))\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[LoRA Epoch {epoch+1}] QQP Acc: {acc:.4f}\")\n",
        "\n",
        "# ========================================================\n",
        "# 7) Main Entrypoint\n",
        "# ========================================================\n",
        "def main():\n",
        "    seed = 42\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "    train_ds = load_dataset(\"glue\", \"qqp\", split=\"train\").shuffle(seed).select(range(5000))\n",
        "    dev_ds   = load_dataset(\"glue\", \"qqp\", split=\"validation\")\n",
        "\n",
        "    def preprocess(examples):\n",
        "        return tokenizer(examples[\"question1\"], examples[\"question2\"],\n",
        "                         truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "    train = train_ds.map(preprocess, batched=True)\\\n",
        "                    .rename_column(\"label\", \"labels\")\\\n",
        "                    .remove_columns([\"question1\", \"question2\", \"idx\"])\n",
        "    dev   = dev_ds.map(preprocess, batched=True)\\\n",
        "                  .rename_column(\"label\", \"labels\")\\\n",
        "                  .remove_columns([\"question1\", \"question2\", \"idx\"])\n",
        "\n",
        "    collator     = DataCollatorWithPadding(tokenizer, padding=\"max_length\", max_length=128)\n",
        "    train_loader = DataLoader(train, batch_size=8, shuffle=True,  collate_fn=collator)\n",
        "    dev_loader   = DataLoader(dev,   batch_size=16, shuffle=False, collate_fn=collator)\n",
        "\n",
        "    model, jac_scores = full_finetuning(train_loader, dev_loader, device)\n",
        "    model = prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores)\n",
        "    lora_only_finetuning(model, train_loader, dev_loader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "sHRewx31HBJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aMLzL8nvGW8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Jacobian for RTE\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 1) Standard imports and warning suppression\n",
        "# ========================================================\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from transformers import (\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizerFast,\n",
        "    DataCollatorWithPadding,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "import evaluate\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 2) Jacobian-Deviation Hooks/Scoring\n",
        "#    Compare each layer's mapping f_ℓ via ||J_ℓ - I|| (Hutchinson-style)\n",
        "# ========================================================\n",
        "\n",
        "def register_jac_hooks(model):\n",
        "    \"\"\"\n",
        "    Capture each RobertaLayer's input (X), output (Y), and non-input args.\n",
        "    We'll call the layer again locally to estimate JVPs via finite differences.\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    activations = {i: {'X': None, 'Y': None, 'args': None} for i in range(len(layers))}\n",
        "    hooks = []\n",
        "\n",
        "    for i, layer in enumerate(layers):\n",
        "        def pre_hook(module, inputs, idx=i):\n",
        "            xs = inputs[0]\n",
        "            extra = tuple(inputs[1:]) if len(inputs) > 1 else tuple()\n",
        "            activations[idx]['X'] = xs.detach()\n",
        "            activations[idx]['args'] = extra\n",
        "\n",
        "        def post_hook(module, inputs, output, idx=i):\n",
        "            y = output[0] if isinstance(output, tuple) else output\n",
        "            activations[idx]['Y'] = y.detach()\n",
        "\n",
        "        hooks.append(layer.register_forward_pre_hook(pre_hook))\n",
        "        hooks.append(layer.register_forward_hook(post_hook))\n",
        "\n",
        "    return hooks, activations\n",
        "\n",
        "def remove_hooks(hooks):\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "@torch.no_grad()\n",
        "def _layer_forward_only(layer, x, args):\n",
        "    out = layer(x, *args)\n",
        "    return out[0] if isinstance(out, tuple) else out\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_batch_jacdev(model, acts, eps=1e-3, k_probes=1):\n",
        "    \"\"\"\n",
        "    For each layer, estimate ||J - I||_F^2 via finite-difference JVPs:\n",
        "      (J - I)v ≈ (f(x + eps*v) - f(x))/eps - v\n",
        "    Average squared norms over k_probes and all elements.\n",
        "    Returns dict: {layer_idx: score}\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    scores = {}\n",
        "\n",
        "    for idx, buf in acts.items():\n",
        "        X = buf['X']; Y = buf['Y']; args = buf['args']\n",
        "        if X is None or Y is None:\n",
        "            continue\n",
        "\n",
        "        layer = layers[idx]\n",
        "        was_training = layer.training\n",
        "        layer.eval()  # disable dropout for stable finite-diff\n",
        "        try:\n",
        "            # Base output with dropout off to pair with eps-perturbed run\n",
        "            y0 = _layer_forward_only(layer, X, args)\n",
        "\n",
        "            acc = 0.0\n",
        "            for _ in range(k_probes):\n",
        "                v = torch.randn_like(X)\n",
        "                y_eps = _layer_forward_only(layer, X + eps * v, args)\n",
        "                jd_vec = (y_eps - y0) / eps - v\n",
        "                acc += float(jd_vec.pow(2).mean().item())\n",
        "            scores[idx] = acc / k_probes\n",
        "        finally:\n",
        "            layer.train(was_training)\n",
        "\n",
        "        # Clear buffers so we don't reuse stale tensors\n",
        "        buf['X'] = None\n",
        "        buf['Y'] = None\n",
        "        buf['args'] = None\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 3) Pruning Utilities with SkipFF (prune low-||J-I||)\n",
        "# ========================================================\n",
        "class SkipFF(nn.Module):\n",
        "    def forward(self, hidden_states, input_tensor=None):\n",
        "        # residual passthrough\n",
        "        return input_tensor\n",
        "\n",
        "def prune_jac_layers(model, jac_scores, num_prune=4):\n",
        "    \"\"\"\n",
        "    Prune layers with the *lowest* Jacobian deviation (closest to identity).\n",
        "    Remove FFN by replacing intermediate.dense with Identity\n",
        "    and the output block with SkipFF (residual passthrough).\n",
        "    \"\"\"\n",
        "    sorted_layers = sorted(jac_scores.items(), key=lambda x: x[1])  # lowest first\n",
        "    prune_idxs = [idx for idx, _ in sorted_layers[:num_prune]]\n",
        "    for idx in prune_idxs:\n",
        "        layer = model.roberta.encoder.layer[idx]\n",
        "        layer.intermediate.dense = nn.Identity()\n",
        "        layer.output = SkipFF()\n",
        "    return prune_idxs\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 4) LoRA Modules (unchanged)\n",
        "# ========================================================\n",
        "class LoRA(nn.Module):\n",
        "    def __init__(self, W0, r=2, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"W0\", W0.clone().detach())\n",
        "        L, M = W0.shape\n",
        "        self.B = nn.Parameter(torch.randn(L, r) * 0.01)\n",
        "        self.A = nn.Parameter(torch.zeros(r, M))\n",
        "        self.scaling = alpha / r\n",
        "    def forward(self):\n",
        "        return self.W0 + self.scaling * (self.B @ self.A)\n",
        "\n",
        "def apply_lora_to_all_layers(model, r=2, alpha=1.0):\n",
        "    loras = {}\n",
        "    for idx, layer in enumerate(model.roberta.encoder.layer):\n",
        "        if not hasattr(layer.output, 'dense'):\n",
        "            continue\n",
        "        W0 = layer.output.dense.weight.data\n",
        "        lora = LoRA(W0, r, alpha).to(W0.device)\n",
        "        def fwd(x, layer=layer, lora=lora):\n",
        "            return F.linear(x, lora(), layer.output.dense.bias)\n",
        "        layer.output.dense.forward = fwd\n",
        "        loras[idx] = lora\n",
        "    return loras\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 5) Data + Eval Helpers\n",
        "# ========================================================\n",
        "def preprocess_function(examples, tok, max_length=128):\n",
        "    return tok(examples['sentence1'],\n",
        "               examples['sentence2'],\n",
        "               truncation=True,\n",
        "               padding='max_length',\n",
        "               max_length=max_length)\n",
        "\n",
        "def evaluate_model(model, dl, device):\n",
        "    model.eval()\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "    preds, labs = [], []\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids = b['input_ids'].to(device)\n",
        "            mask = b['attention_mask'].to(device)\n",
        "            labs.extend(b['labels'].cpu().numpy())\n",
        "            out = model(input_ids=ids, attention_mask=mask)\n",
        "            preds.extend(torch.argmax(out.logits, -1).cpu().numpy())\n",
        "    return metric.compute(predictions=preds, references=labs)[\"accuracy\"]\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 6) Training Stages (Jacobian-Deviation scoring)\n",
        "# ========================================================\n",
        "def full_finetuning(train_loader, dev_loader, device):\n",
        "    print(\"=== Stage 1: Full Finetuning & Jacobian-Deviation Estimation ===\")\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\n",
        "        \"roberta-base\", num_labels=2\n",
        "    ).to(device)\n",
        "    model.gradient_checkpointing_enable()\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "    sched = get_linear_schedule_with_warmup(opt,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(train_loader)*6)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    hooks, activations = register_jac_hooks(model)\n",
        "    last_jac = None\n",
        "\n",
        "    for epoch in range(6):\n",
        "        jac_sums, jac_counts = defaultdict(float), defaultdict(int)\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                out = model(input_ids=b['input_ids'].to(device),\n",
        "                            attention_mask=b['attention_mask'].to(device),\n",
        "                            labels=b['labels'].to(device))\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "\n",
        "            # Per-batch Jacobian deviation scores\n",
        "            batch_jac = compute_batch_jacdev(model, activations, eps=1e-3, k_probes=1)\n",
        "            for idx, v in batch_jac.items():\n",
        "                jac_sums[idx]   += v\n",
        "                jac_counts[idx] += 1\n",
        "\n",
        "        epoch_jac = {idx: jac_sums[idx]/max(1, jac_counts[idx]) for idx in jac_sums}\n",
        "        print(f\"[Epoch {epoch+1}] approx ||J-I||^2:\", {k: round(v, 6) for k, v in epoch_jac.items()})\n",
        "        last_jac = epoch_jac\n",
        "\n",
        "    acc = evaluate_model(model, dev_loader, device)\n",
        "    print(f\"-> Full Finetune RTE Acc: {acc:.4f}\")\n",
        "\n",
        "    remove_hooks(hooks)\n",
        "    return model, last_jac\n",
        "\n",
        "\n",
        "def prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores):\n",
        "    print(\"=== Stage 2: Prune (Low-||J-I||) & Finetuning ===\")\n",
        "    prune_idxs = prune_jac_layers(model, jac_scores, num_prune=num)\n",
        "    print(\"Pruned layers (lowest ||J-I||):\", prune_idxs)\n",
        "\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "    sched = get_linear_schedule_with_warmup(opt,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(train_loader)*3)\n",
        "\n",
        "    for epoch in range(5):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            out = model(input_ids=b['input_ids'].to(device),\n",
        "                        attention_mask=b['attention_mask'].to(device),\n",
        "                        labels=b['labels'].to(device))\n",
        "            out.loss.backward()\n",
        "            opt.step()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[Prune FT Epoch {epoch+1}] RTE Acc: {acc:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def lora_only_finetuning(model, train_loader, dev_loader, device, r=2, alpha=1.0):\n",
        "    print(\"=== Stage 3: LoRA Finetuning ===\")\n",
        "    torch.cuda.empty_cache()\n",
        "    loras = apply_lora_to_all_layers(model, r, alpha)\n",
        "    for p in model.roberta.parameters(): p.requires_grad=False\n",
        "    for p in model.classifier.parameters(): p.requires_grad=True\n",
        "    for l in loras.values():\n",
        "        l.A.requires_grad=True\n",
        "        l.B.requires_grad=True\n",
        "\n",
        "    opt   = torch.optim.Adam(\n",
        "        list(model.classifier.parameters())\n",
        "        + [p for l in loras.values() for p in (l.A, l.B)],\n",
        "        lr=2e-5\n",
        "    )\n",
        "    sched = get_linear_schedule_with_warmup(opt,\n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(train_loader)*6)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(6):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                out = model(input_ids=b['input_ids'].to(device),\n",
        "                            attention_mask=b['attention_mask'].to(device),\n",
        "                            labels=b['labels'].to(device))\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[LoRA Epoch {epoch+1}] RTE Acc: {acc:.4f}\")\n",
        "\n",
        "\n",
        "# ========================================================\n",
        "# 7) Main Entrypoint\n",
        "# ========================================================\n",
        "def main():\n",
        "    seed = 42\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load & preprocess RTE\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "    train_ds = load_dataset(\"glue\", \"rte\", split=\"train\").shuffle(seed)\n",
        "    dev_ds   = load_dataset(\"glue\", \"rte\", split=\"validation\")\n",
        "\n",
        "    def preprocess(examples):\n",
        "        return tokenizer(examples[\"sentence1\"],\n",
        "                         examples[\"sentence2\"],\n",
        "                         truncation=True,\n",
        "                         padding=\"max_length\",\n",
        "                         max_length=128)\n",
        "\n",
        "    train = train_ds.map(preprocess, batched=True)\\\n",
        "                    .rename_column(\"label\", \"labels\")\\\n",
        "                    .remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
        "    dev = dev_ds.map(preprocess, batched=True)\\\n",
        "                .rename_column(\"label\", \"labels\")\\\n",
        "                .remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
        "\n",
        "    collator = DataCollatorWithPadding(tokenizer, padding=\"max_length\", max_length=128)\n",
        "    train_loader = DataLoader(train, batch_size=8, shuffle=True, collate_fn=collator)\n",
        "    dev_loader   = DataLoader(dev,  batch_size=16, shuffle=False, collate_fn=collator)\n",
        "\n",
        "    model, jac_scores = full_finetuning(train_loader, dev_loader, device)\n",
        "    model = prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores)\n",
        "    lora_only_finetuning(model, train_loader, dev_loader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "N_NpPww-NjCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Jacobian for RTE (Autograd JVP version)\n",
        "\n",
        "# ========================================================\n",
        "# 1) Standard imports and warning suppression\n",
        "# ========================================================\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from transformers import (\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizerFast,\n",
        "    DataCollatorWithPadding,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "import evaluate\n",
        "import warnings\n",
        "from contextlib import contextmanager  # <-- correct import\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# ========================================================\n",
        "# 1.1) Utility: force 'math' attention kernels during JVP\n",
        "# ========================================================\n",
        "@contextmanager\n",
        "def force_math_sdp():\n",
        "    \"\"\"\n",
        "    Force 'math' scaled-dot-product attention so higher-order grads/JVP work.\n",
        "    No-op on CPU.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        with torch.backends.cuda.sdp_kernel(\n",
        "            enable_flash=False, enable_math=True, enable_mem_efficient=False\n",
        "        ):\n",
        "            yield\n",
        "    else:\n",
        "        yield\n",
        "\n",
        "# ========================================================\n",
        "# 2) Jacobian-Deviation Hooks/Scoring (Autograd JVP)\n",
        "#    Compare each layer's mapping f_ℓ via ||J_ℓ - I|| using Hutchinson probes\n",
        "# ========================================================\n",
        "def register_jac_hooks(model):\n",
        "    \"\"\"\n",
        "    Capture each RobertaLayer's input (X) and non-input args so we can\n",
        "    locally re-run the layer to compute JVPs.\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    activations = {i: {'X': None, 'args': None} for i in range(len(layers))}\n",
        "    hooks = []\n",
        "\n",
        "    for i, layer in enumerate(layers):\n",
        "        def pre_hook(module, inputs, idx=i):\n",
        "            xs = inputs[0]\n",
        "            extra = tuple(inputs[1:]) if len(inputs) > 1 else tuple()\n",
        "            activations[idx]['X'] = xs.detach()\n",
        "            activations[idx]['args'] = extra\n",
        "\n",
        "        hooks.append(layer.register_forward_pre_hook(pre_hook))\n",
        "\n",
        "    return hooks, activations\n",
        "\n",
        "def remove_hooks(hooks):\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "def compute_batch_jacdev_autograd(model, acts, k_probes=2, rademacher=True):\n",
        "    \"\"\"\n",
        "    For each layer ℓ, estimate E_v ||(J_ℓ - I)v||^2 via autograd JVP:\n",
        "       _, Jv = jvp(f, (X,), (v,));  (J - I)v = Jv - v\n",
        "    Average squared norms over probes and all elements.\n",
        "    Returns dict: {layer_idx: score}\n",
        "    \"\"\"\n",
        "    from torch.autograd.functional import jvp\n",
        "    layers = model.roberta.encoder.layer\n",
        "    scores = {}\n",
        "\n",
        "    for idx, buf in acts.items():\n",
        "        X, args = buf['X'], buf['args']\n",
        "        if X is None:\n",
        "            continue\n",
        "\n",
        "        X = X.detach().requires_grad_(True)\n",
        "        layer = layers[idx]\n",
        "        was_training = layer.training\n",
        "        layer.eval()  # disable dropout for stable JVP\n",
        "\n",
        "        try:\n",
        "            def f(inp):\n",
        "                out = layer(inp, *args)\n",
        "                return out[0] if isinstance(out, tuple) else out\n",
        "\n",
        "            acc = 0.0\n",
        "            # Avoid efficient SDPA kernels that lack higher-order grads\n",
        "            with force_math_sdp():\n",
        "                for _ in range(k_probes):\n",
        "                    if rademacher:\n",
        "                        v = torch.empty_like(X).bernoulli_(0.5).mul_(2).sub_(1)  # ±1 probes\n",
        "                    else:\n",
        "                        v = torch.randn_like(X)\n",
        "                    _, Jv = jvp(f, (X,), (v,), create_graph=False, strict=True)\n",
        "                    jd_vec = Jv - v\n",
        "                    acc += float(jd_vec.pow(2).mean().item())\n",
        "\n",
        "            scores[idx] = acc / k_probes\n",
        "        finally:\n",
        "            layer.train(was_training)\n",
        "\n",
        "        # clear buffers\n",
        "        buf['X'] = None\n",
        "        buf['args'] = None\n",
        "\n",
        "    return scores\n",
        "\n",
        "# ========================================================\n",
        "# 3) Pruning Utilities with SkipFF (prune low-||J-I||)\n",
        "# ========================================================\n",
        "class SkipFF(nn.Module):\n",
        "    def forward(self, hidden_states, input_tensor=None):\n",
        "        # residual passthrough\n",
        "        return input_tensor\n",
        "\n",
        "def prune_jac_layers(model, jac_scores, num_prune=4):\n",
        "    \"\"\"\n",
        "    Prune layers with the *lowest* Jacobian deviation (closest to identity).\n",
        "    Remove FFN by replacing intermediate.dense with Identity\n",
        "    and the output block with SkipFF (residual passthrough).\n",
        "    \"\"\"\n",
        "    sorted_layers = sorted(jac_scores.items(), key=lambda x: x[1])  # lowest first\n",
        "    prune_idxs = [idx for idx, _ in sorted_layers[:num_prune]]\n",
        "    for idx in prune_idxs:\n",
        "        layer = model.roberta.encoder.layer[idx]\n",
        "        layer.intermediate.dense = nn.Identity()\n",
        "        layer.output = SkipFF()\n",
        "    return prune_idxs\n",
        "\n",
        "# ========================================================\n",
        "# 4) LoRA Modules (unchanged)\n",
        "# ========================================================\n",
        "class LoRA(nn.Module):\n",
        "    def __init__(self, W0, r=2, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"W0\", W0.clone().detach())\n",
        "        L, M = W0.shape\n",
        "        self.B = nn.Parameter(torch.randn(L, r) * 0.01)\n",
        "        self.A = nn.Parameter(torch.zeros(r, M))\n",
        "        self.scaling = alpha / r\n",
        "    def forward(self):\n",
        "        return self.W0 + self.scaling * (self.B @ self.A)\n",
        "\n",
        "def apply_lora_to_all_layers(model, r=2, alpha=1.0):\n",
        "    loras = {}\n",
        "    for idx, layer in enumerate(model.roberta.encoder.layer):\n",
        "        if not hasattr(layer.output, 'dense'):\n",
        "            continue\n",
        "        W0 = layer.output.dense.weight.data\n",
        "        lora = LoRA(W0, r, alpha).to(W0.device)\n",
        "        def fwd(x, layer=layer, lora=lora):\n",
        "            return F.linear(x, lora(), layer.output.dense.bias)\n",
        "        layer.output.dense.forward = fwd\n",
        "        loras[idx] = lora\n",
        "    return loras\n",
        "\n",
        "# ========================================================\n",
        "# 5) Data + Eval Helpers\n",
        "# ========================================================\n",
        "def preprocess_function(examples, tok, max_length=128):\n",
        "    return tok(examples['sentence1'],\n",
        "               examples['sentence2'],\n",
        "               truncation=True,\n",
        "               padding='max_length',\n",
        "               max_length=max_length)\n",
        "\n",
        "def evaluate_model(model, dl, device):\n",
        "    model.eval()\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "    preds, labs = [], []\n",
        "    with torch.no_grad():\n",
        "        for b in dl:\n",
        "            ids  = b['input_ids'].to(device)\n",
        "            mask = b['attention_mask'].to(device)\n",
        "            labs.extend(b['labels'].cpu().numpy())\n",
        "            out = model(input_ids=ids, attention_mask=mask)\n",
        "            preds.extend(torch.argmax(out.logits, -1).cpu().numpy())\n",
        "    return metric.compute(predictions=preds, references=labs)[\"accuracy\"]\n",
        "\n",
        "# ========================================================\n",
        "# 6) Training Stages (Jacobian-Deviation scoring)\n",
        "# ========================================================\n",
        "def full_finetuning(train_loader, dev_loader, device):\n",
        "    print(\"=== Stage 1: Full Finetuning & Jacobian-Deviation Estimation ===\")\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\n",
        "        \"roberta-base\", num_labels=2\n",
        "    ).to(device)\n",
        "    model.gradient_checkpointing_enable()\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "    sched = get_linear_schedule_with_warmup(\n",
        "        opt, num_warmup_steps=0, num_training_steps=len(train_loader)*6\n",
        "    )\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    hooks, activations = register_jac_hooks(model)\n",
        "    last_jac = None\n",
        "\n",
        "    for epoch in range(6):\n",
        "        jac_sums, jac_counts = defaultdict(float), defaultdict(int)\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                out = model(\n",
        "                    input_ids=b['input_ids'].to(device),\n",
        "                    attention_mask=b['attention_mask'].to(device),\n",
        "                    labels=b['labels'].to(device)\n",
        "                )\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "\n",
        "            # Per-batch ||J - I||^2 per layer (Autograd JVP)\n",
        "            batch_jac = compute_batch_jacdev_autograd(\n",
        "                model, activations, k_probes=2, rademacher=True\n",
        "            )\n",
        "            for idx, v in batch_jac.items():\n",
        "                jac_sums[idx]   += v\n",
        "                jac_counts[idx] += 1\n",
        "\n",
        "        epoch_jac = {idx: jac_sums[idx] / max(1, jac_counts[idx]) for idx in jac_sums}\n",
        "        print(f\"[Epoch {epoch+1}] approx ||J-I||^2:\",\n",
        "              {k: round(v, 6) for k, v in epoch_jac.items()})\n",
        "        last_jac = epoch_jac\n",
        "\n",
        "    acc = evaluate_model(model, dev_loader, device)\n",
        "    print(f\"-> Full Finetune RTE Acc: {acc:.4f}\")\n",
        "\n",
        "    remove_hooks(hooks)\n",
        "    return model, last_jac\n",
        "\n",
        "def prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores):\n",
        "    print(\"=== Stage 2: Prune (Low-||J-I||) & Finetuning ===\")\n",
        "    prune_idxs = prune_jac_layers(model, jac_scores, num_prune=num)\n",
        "    print(\"Pruned layers (lowest ||J-I||):\", prune_idxs)\n",
        "\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "    sched = get_linear_schedule_with_warmup(\n",
        "        opt, num_warmup_steps=0, num_training_steps=len(train_loader)*3\n",
        "    )\n",
        "\n",
        "    for epoch in range(5):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            out = model(\n",
        "                input_ids=b['input_ids'].to(device),\n",
        "                attention_mask=b['attention_mask'].to(device),\n",
        "                labels=b['labels'].to(device)\n",
        "            )\n",
        "            out.loss.backward()\n",
        "            opt.step()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[Prune FT Epoch {epoch+1}] RTE Acc: {acc:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def lora_only_finetuning(model, train_loader, dev_loader, device, r=2, alpha=1.0):\n",
        "    print(\"=== Stage 3: LoRA Finetuning ===\")\n",
        "    torch.cuda.empty_cache()\n",
        "    loras = apply_lora_to_all_layers(model, r, alpha)\n",
        "    for p in model.roberta.parameters():  p.requires_grad = False\n",
        "    for p in model.classifier.parameters(): p.requires_grad = True\n",
        "    for l in loras.values():\n",
        "        l.A.requires_grad = True\n",
        "        l.B.requires_grad = True\n",
        "\n",
        "    opt   = torch.optim.Adam(\n",
        "        list(model.classifier.parameters())\n",
        "        + [p for l in loras.values() for p in (l.A, l.B)],\n",
        "        lr=2e-5\n",
        "    )\n",
        "    sched = get_linear_schedule_with_warmup(\n",
        "        opt, num_warmup_steps=0, num_training_steps=len(train_loader)*6\n",
        "    )\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(6):\n",
        "        model.train()\n",
        "        for b in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                out = model(\n",
        "                    input_ids=b['input_ids'].to(device),\n",
        "                    attention_mask=b['attention_mask'].to(device),\n",
        "                    labels=b['labels'].to(device)\n",
        "                )\n",
        "                scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "        acc = evaluate_model(model, dev_loader, device)\n",
        "        print(f\"[LoRA Epoch {epoch+1}] RTE Acc: {acc:.4f}\")\n",
        "\n",
        "# ========================================================\n",
        "# 7) Main Entrypoint\n",
        "# ========================================================\n",
        "def main():\n",
        "    seed = 42\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load & preprocess RTE\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "    train_ds = load_dataset(\"glue\", \"rte\", split=\"train\").shuffle(seed)\n",
        "    dev_ds   = load_dataset(\"glue\", \"rte\", split=\"validation\")\n",
        "\n",
        "    def preprocess(examples):\n",
        "        return tokenizer(\n",
        "            examples[\"sentence1\"], examples[\"sentence2\"],\n",
        "            truncation=True, padding=\"max_length\", max_length=128\n",
        "        )\n",
        "\n",
        "    train = train_ds.map(preprocess, batched=True)\\\n",
        "                    .rename_column(\"label\", \"labels\")\\\n",
        "                    .remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
        "    dev = dev_ds.map(preprocess, batched=True)\\\n",
        "                .rename_column(\"label\", \"labels\")\\\n",
        "                .remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
        "\n",
        "    collator = DataCollatorWithPadding(tokenizer, padding=\"max_length\", max_length=128)\n",
        "    train_loader = DataLoader(train, batch_size=8, shuffle=True, collate_fn=collator)\n",
        "    dev_loader   = DataLoader(dev,  batch_size=16, shuffle=False, collate_fn=collator)\n",
        "\n",
        "    model, jac_scores = full_finetuning(train_loader, dev_loader, device)\n",
        "    model = prune_and_finetuning(model, train_loader, dev_loader, device, jac_scores)\n",
        "    lora_only_finetuning(model, train_loader, dev_loader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "y0IlhGuFNjd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gAJz_p3jPzEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uWn8MV69Rh53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RWFBdem0YlDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Jacobian for STS-B\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import warnings\n",
        "\n",
        "# ── Robust NumPy 'copy' kwarg patch (NumPy 1.x and 2.x; idempotent; no recursion) ──\n",
        "# Many libs call np.array(..., copy=False). In NumPy 2.0 this is *strict* and may raise\n",
        "# ValueError if a copy is required. We relax this by removing copy=False (but honor copy=True).\n",
        "try:\n",
        "    from numpy.core.multiarray import array as _np_array_c\n",
        "except Exception:\n",
        "    _np_array_c = None\n",
        "\n",
        "if _np_array_c is not None and not getattr(np, \"_array_copy_patched_relaxed\", False):\n",
        "    def _np_array_relaxed(obj, *args, **kwargs):\n",
        "        # If caller asked for copy=False (strict in NumPy 2.0), drop it to allow fallback\n",
        "        if \"copy\" in kwargs and kwargs[\"copy\"] is False:\n",
        "            kwargs = dict(kwargs)\n",
        "            kwargs.pop(\"copy\", None)\n",
        "            return _np_array_c(obj, *args, **kwargs)\n",
        "        try:\n",
        "            # Normal path\n",
        "            return _np_array_c(obj, *args, **kwargs)\n",
        "        except ValueError as e:\n",
        "            # If strict 'copy' triggered a failure, retry without it\n",
        "            if \"copy\" in kwargs:\n",
        "                kwargs = dict(kwargs)\n",
        "                kwargs.pop(\"copy\", None)\n",
        "                return _np_array_c(obj, *args, **kwargs)\n",
        "            raise\n",
        "        except TypeError:\n",
        "            # NumPy < 2.0 may not accept 'copy' kwarg—remove it if present\n",
        "            kwargs = dict(kwargs)\n",
        "            kwargs.pop(\"copy\", None)\n",
        "            return _np_array_c(obj, *args, **kwargs)\n",
        "\n",
        "    # Reset np.array to C-level impl (break any prior patches), then wrap\n",
        "    np.array = _np_array_relaxed\n",
        "    np._array_copy_patched_relaxed = True\n",
        "# ───────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.utils.data import DataLoader\n",
        "import evaluate\n",
        "\n",
        "from transformers import (\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizerFast,\n",
        "    DataCollatorWithPadding,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from datasets import load_dataset\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# ─── 1) Jacobian-Deviation (JD) Hooks ──────────────────────────────────────\n",
        "def register_jd_hooks(model):\n",
        "    \"\"\"\n",
        "    Capture (X_i, Y_{i+1}) at each adjacent pair's output.dense.\n",
        "    IMPORTANT: do NOT detach — we need the graph for autograd.grad.\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    activations = {i: {'X': None, 'Y': None} for i in range(len(layers)-1)}\n",
        "    hooks = []\n",
        "\n",
        "    for i in range(len(layers)-1):\n",
        "        def hook_x(module, inp, out, idx=i):\n",
        "            activations[idx]['X'] = out  # keep graph\n",
        "        def hook_y(module, inp, out, idx=i):\n",
        "            activations[idx]['Y'] = out  # keep graph\n",
        "        hooks.append(layers[i].output.dense.register_forward_hook(hook_x))\n",
        "        hooks.append(layers[i+1].output.dense.register_forward_hook(hook_y))\n",
        "\n",
        "    return hooks, activations\n",
        "\n",
        "def remove_hooks(hooks):\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "@torch.no_grad()\n",
        "def _numel(t):\n",
        "    return int(t.numel())\n",
        "\n",
        "def _flatten(t):\n",
        "    return t.reshape(-1)\n",
        "\n",
        "def _safe_item(x):\n",
        "    return float(torch.nan_to_num(x, nan=0.0, posinf=1e6, neginf=-1e6).item())\n",
        "\n",
        "\n",
        "def compute_batch_jd(\n",
        "    activations,\n",
        "    num_probes: int = 1,\n",
        "    sample_pairs: int | None = None\n",
        "):\n",
        "    \"\"\"\n",
        "    JD(J) = ||J - I||_F^2 / D with Hutchinson:\n",
        "      ||J||_F^2  ≈  E[ ||J^T z||^2 ]\n",
        "      tr(J)      ≈  E[ z^T J z ] = E[ (J^T z)·z ]\n",
        "      ||I||_F^2  =  D  (D = dim of X)\n",
        "\n",
        "    We compute VJP g = J^T z via autograd.grad(outputs=Y, inputs=X, grad_outputs=z).\n",
        "    Do the *reductions* in float32 to avoid AMP/FP16 overflow.\n",
        "    \"\"\"\n",
        "    all_idxs = [i for i, buf in activations.items() if buf['X'] is not None and buf['Y'] is not None]\n",
        "    if sample_pairs is not None and sample_pairs < len(all_idxs) and all_idxs:\n",
        "        with torch.no_grad():\n",
        "            device = activations[all_idxs[0]]['X'].device\n",
        "            perm = torch.randperm(len(all_idxs), device=device)\n",
        "        chosen = [all_idxs[int(i)] for i in perm[:sample_pairs]]\n",
        "    else:\n",
        "        chosen = all_idxs\n",
        "\n",
        "    scores = {}\n",
        "    if not chosen:\n",
        "        return scores\n",
        "\n",
        "    for idx in chosen:\n",
        "        X = activations[idx]['X']\n",
        "        Y = activations[idx]['Y']\n",
        "\n",
        "        # Must match shape and require grads\n",
        "        if (\n",
        "            X is None or Y is None or X.shape != Y.shape\n",
        "            or (not getattr(X, \"requires_grad\", False))\n",
        "            or (not getattr(Y, \"requires_grad\", False))\n",
        "        ):\n",
        "            scores[idx] = 0.0\n",
        "            continue\n",
        "\n",
        "        D = int(X.numel())\n",
        "        D32 = torch.tensor(D, device=Y.device, dtype=torch.float32)\n",
        "\n",
        "        jd_sum32 = torch.zeros((), device=Y.device, dtype=torch.float32)\n",
        "\n",
        "        for _ in range(num_probes):\n",
        "            # grad_outputs must match outputs dtype; keep AMP-friendly here\n",
        "            z = torch.randn_like(Y)\n",
        "\n",
        "            g = torch.autograd.grad(\n",
        "                outputs=Y,\n",
        "                inputs=X,\n",
        "                grad_outputs=z,\n",
        "                retain_graph=True,\n",
        "                allow_unused=True,\n",
        "                create_graph=False\n",
        "            )[0]\n",
        "\n",
        "            if g is None:\n",
        "                g = torch.zeros_like(X)\n",
        "\n",
        "            # Do the heavy reductions in float32 to prevent overflow\n",
        "            with torch.cuda.amp.autocast(enabled=False):\n",
        "                g_flat32 = g.reshape(-1).float()\n",
        "                z_flat32 = z.reshape(-1).float()\n",
        "\n",
        "                # Clean NaNs/infs defensively (rare, but keeps scores finite)\n",
        "                g_flat32 = torch.nan_to_num(g_flat32, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "                z_flat32 = torch.nan_to_num(z_flat32, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "                term_normJ = torch.dot(g_flat32, g_flat32)      # ||J^T z||^2\n",
        "                term_trJ   = torch.dot(g_flat32, z_flat32)      # z^T J z\n",
        "\n",
        "                jd_probe32 = term_normJ - 2.0 * term_trJ + D32  # float32 math\n",
        "                # Final safety: clamp extreme explosions\n",
        "                jd_probe32 = torch.clamp(jd_probe32, min=-1e12, max=1e12)\n",
        "\n",
        "            jd_sum32 += jd_probe32\n",
        "\n",
        "        jd_mean32 = jd_sum32 / max(1, num_probes)\n",
        "        jd32 = jd_mean32 / D32  # normalize by dimensionality\n",
        "\n",
        "        # Ensure finite scalar\n",
        "        jd32 = torch.nan_to_num(jd32, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "        scores[idx] = float(jd32.item())\n",
        "\n",
        "        # Clear references so the graph can be freed after backward\n",
        "        activations[idx]['X'] = None\n",
        "        activations[idx]['Y'] = None\n",
        "\n",
        "    # Also clear any non-chosen to avoid leaking graph to the next step\n",
        "    for i in activations:\n",
        "        activations[i]['X'] = None\n",
        "        activations[i]['Y'] = None\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ─── 2) Pruning Utilities (prune low-JD → downstream layer) ────────────────\n",
        "class SkipFF(nn.Module):\n",
        "    def forward(self, hidden_states, input_tensor=None):\n",
        "        return input_tensor  # residual passthrough\n",
        "\n",
        "\n",
        "\n",
        "def prune_jd_layers(model, jd_scores, num_prune=4):\n",
        "    \"\"\"\n",
        "    Sort ascending by JD (closest to identity first), then prune the *downstream*\n",
        "    layer (j = i+1) — but NEVER prune the first two (0,1) or last two (L-2,L-1)\n",
        "    encoder layers.\n",
        "    \"\"\"\n",
        "    L = len(model.roberta.encoder.layer)\n",
        "    # Need at least 5 layers to have any room after reserving 2 + 2\n",
        "    assert L >= 5, \"Need at least 5 layers to preserve first/last two and still prune.\"\n",
        "\n",
        "    # Candidate downstream layers are j in [2, L-3]\n",
        "    allowed = set(range(2, L - 2))\n",
        "    if not allowed:\n",
        "        return []\n",
        "\n",
        "    # Sort pairs by JD; consider downstream j = i+1\n",
        "    candidate_pairs = sorted(jd_scores.items(), key=lambda x: x[1])\n",
        "\n",
        "    prune_idxs = []\n",
        "    for i, _score in candidate_pairs:\n",
        "        j = i + 1\n",
        "        if j in allowed and j not in prune_idxs:\n",
        "            prune_idxs.append(j)\n",
        "        if len(prune_idxs) >= min(num_prune, len(allowed)):\n",
        "            break\n",
        "\n",
        "    # Apply pruning\n",
        "    for idx in prune_idxs:\n",
        "        layer = model.roberta.encoder.layer[idx]\n",
        "        layer.intermediate.dense = nn.Identity()\n",
        "        layer.output = SkipFF()\n",
        "\n",
        "    return prune_idxs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ─── 3) LoRA Modules ──────────────────────────────────────────────────────\n",
        "class LoRA(nn.Module):\n",
        "    def __init__(self, W0, r=2, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"W0\", W0.clone().detach())\n",
        "        L, M = W0.shape\n",
        "        self.B = nn.Parameter(torch.randn(L, r) * 0.01)\n",
        "        self.A = nn.Parameter(torch.zeros(r, M))\n",
        "        self.scaling = alpha / r\n",
        "    def forward(self):\n",
        "        return self.W0 + self.scaling * (self.B @ self.A)\n",
        "\n",
        "def apply_lora_to_all_layers(model, r=2, alpha=1.0):\n",
        "    loras = {}\n",
        "    for idx, layer in enumerate(model.roberta.encoder.layer):\n",
        "        if not hasattr(layer.output, 'dense'):\n",
        "            continue\n",
        "        W0 = layer.output.dense.weight.data\n",
        "        lora = LoRA(W0, r, alpha).to(W0.device)\n",
        "        def fwd(x, layer=layer, lora=lora):\n",
        "            return F.linear(x, lora(), layer.output.dense.bias)\n",
        "        layer.output.dense.forward = fwd\n",
        "        loras[idx] = lora\n",
        "    return loras\n",
        "\n",
        "# ─── 4) STS-B Evaluation ───────────────────────────────────────────────────\n",
        "def evaluate_stsb(model, dataloader, device):\n",
        "    model.eval()\n",
        "    metric = evaluate.load(\"glue\", \"stsb\")\n",
        "    preds, refs = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            out = model(\n",
        "                input_ids=batch[\"input_ids\"].to(device),\n",
        "                attention_mask=batch[\"attention_mask\"].to(device),\n",
        "            )\n",
        "            p = out.logits.squeeze(-1).cpu().tolist()\n",
        "            preds.extend(p if isinstance(p, list) else [p])\n",
        "            r = batch[\"labels\"].cpu().tolist()\n",
        "            for x in r:\n",
        "                if isinstance(x, (list, tuple, np.ndarray)):\n",
        "                    refs.append(float(x[0]))\n",
        "                else:\n",
        "                    refs.append(float(x))\n",
        "    return metric.compute(predictions=preds, references=refs)\n",
        "\n",
        "# ─── 5) Training Stages (JD-based scoring) ─────────────────────────────────\n",
        "def full_finetuning(train_loader, dev_loader, device):\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\n",
        "        \"roberta-base\", num_labels=1\n",
        "    ).to(device)\n",
        "\n",
        "    # Keep checkpointing OFF while computing JD (hooks need grad paths)\n",
        "    # model.gradient_checkpointing_enable()\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "    sched = get_linear_schedule_with_warmup(\n",
        "        opt, num_warmup_steps=0, num_training_steps=len(train_loader)*6\n",
        "    )\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    hooks, activations = register_jd_hooks(model)\n",
        "    last_jd = None\n",
        "\n",
        "    for epoch in range(6):\n",
        "        jd_sums, jd_counts = defaultdict(float), defaultdict(int)\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            opt.zero_grad()\n",
        "            with torch.set_grad_enabled(True), autocast():\n",
        "                out = model(\n",
        "                    input_ids=batch[\"input_ids\"].to(device),\n",
        "                    attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                    labels=batch[\"labels\"].to(device),\n",
        "                )\n",
        "                # ---- JD BEFORE backward (graph needed) ----\n",
        "                batch_jd = compute_batch_jd(activations, num_probes=1, sample_pairs=4)\n",
        "\n",
        "            for idx, v in batch_jd.items():\n",
        "                jd_sums[idx]   += v\n",
        "                jd_counts[idx] += 1\n",
        "\n",
        "            scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "\n",
        "        epoch_jd = {idx: jd_sums[idx]/max(1, jd_counts[idx]) for idx in jd_sums}\n",
        "        print(f\"[Epoch {epoch+1}] approx Jacobian Deviation:\", {k: round(v, 6) for k, v in epoch_jd.items()})\n",
        "        last_jd = epoch_jd\n",
        "\n",
        "    metrics = evaluate_stsb(model, dev_loader, device)\n",
        "    print(f\"STS-B Pearson: {metrics['pearson']:.4f}, Spearman: {metrics['spearmanr']:.4f}\")\n",
        "    remove_hooks(hooks)\n",
        "    return model, last_jd\n",
        "\n",
        "def prune_and_finetuning(model, train_loader, dev_loader, device, jd_scores):\n",
        "    prune_idxs = prune_jd_layers(model, jd_scores, num_prune=num)\n",
        "    print(\"Pruned layers (lowest-JD pairs → pruned downstream):\", prune_idxs)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "    sched = get_linear_schedule_with_warmup(\n",
        "        opt, num_warmup_steps=0, num_training_steps=len(train_loader)*3\n",
        "    )\n",
        "    for epoch in range(5):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            opt.zero_grad()\n",
        "            out = model(\n",
        "                input_ids=batch[\"input_ids\"].to(device),\n",
        "                attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                labels=batch[\"labels\"].to(device),\n",
        "            )\n",
        "            out.loss.backward()\n",
        "            opt.step()\n",
        "            sched.step()\n",
        "\n",
        "        metrics = evaluate_stsb(model, dev_loader, device)\n",
        "        print(f\"[Prune Epoch {epoch+1}] Pearson: {metrics['pearson']:.4f}\")\n",
        "    return model\n",
        "\n",
        "def lora_only_finetuning(model, train_loader, dev_loader, device):\n",
        "    torch.cuda.empty_cache()\n",
        "    loras = apply_lora_to_all_layers(model)\n",
        "    for p in model.roberta.parameters(): p.requires_grad = False\n",
        "    for p in model.classifier.parameters(): p.requires_grad = True\n",
        "    for l in loras.values():\n",
        "        l.A.requires_grad = True\n",
        "        l.B.requires_grad = True\n",
        "\n",
        "    opt = torch.optim.AdamW(\n",
        "        list(model.classifier.parameters())\n",
        "        + [p for l in loras.values() for p in (l.A, l.B)],\n",
        "        lr=2e-5\n",
        "    )\n",
        "    sched = get_linear_schedule_with_warmup(\n",
        "        opt, num_warmup_steps=0, num_training_steps=len(train_loader)*6\n",
        "    )\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(6):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            opt.zero_grad()\n",
        "            with autocast():\n",
        "                out = model(\n",
        "                    input_ids=batch[\"input_ids\"].to(device),\n",
        "                    attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                    labels=batch[\"labels\"].to(device),\n",
        "                )\n",
        "            scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "\n",
        "        metrics = evaluate_stsb(model, dev_loader, device)\n",
        "        print(f\"[LoRA Epoch {epoch+1}] Pearson: {metrics['pearson']:.4f}\")\n",
        "\n",
        "# ─── 6) Main Entrypoint ────────────────────────────────────────────────────\n",
        "def main():\n",
        "    seed = 42\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "    train_ds = load_dataset(\"glue\", \"stsb\", split=\"train\").shuffle(seed)\n",
        "    dev_ds   = load_dataset(\"glue\", \"stsb\", split=\"validation\")\n",
        "\n",
        "    def preprocess(ex):\n",
        "        return tokenizer(\n",
        "            ex[\"sentence1\"], ex[\"sentence2\"],\n",
        "            truncation=True, padding=\"max_length\", max_length=128\n",
        "        )\n",
        "\n",
        "    train_ds = train_ds.map(preprocess, batched=True)\n",
        "    dev_ds   = dev_ds.map(preprocess, batched=True)\n",
        "\n",
        "    # Cast labels to flat float32\n",
        "    train_ds = train_ds.map(lambda x: {\"labels\": float(x[\"label\"])}, batched=False)\n",
        "    dev_ds   = dev_ds.map(lambda x: {\"labels\": float(x[\"label\"])}, batched=False)\n",
        "\n",
        "    train_ds = train_ds.remove_columns([\"sentence1\", \"sentence2\", \"label\", \"idx\"])\n",
        "    dev_ds   = dev_ds.remove_columns([\"sentence1\", \"sentence2\", \"label\", \"idx\"])\n",
        "\n",
        "    train_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "    dev_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "    collator     = DataCollatorWithPadding(tokenizer, padding=\"max_length\", max_length=128)\n",
        "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True,  collate_fn=collator)\n",
        "    dev_loader   = DataLoader(dev_ds,   batch_size=16, shuffle=False, collate_fn=collator)\n",
        "\n",
        "    model, jd_scores = full_finetuning(train_loader, dev_loader, device)\n",
        "    model = prune_and_finetuning(model, train_loader, dev_loader, device, jd_scores)\n",
        "    lora_only_finetuning(model, train_loader, dev_loader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "BH53sghcYqhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Jacobian using Autograde JVP\n",
        "\n",
        "# Jacobian → Energy Distance for STS-B (Autograd JVP version)\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import warnings\n",
        "\n",
        "# ── Robust NumPy 'copy' kwarg patch (NumPy 1.x and 2.x; idempotent; no recursion) ──\n",
        "try:\n",
        "    from numpy.core.multiarray import array as _np_array_c\n",
        "except Exception:\n",
        "    _np_array_c = None\n",
        "\n",
        "if _np_array_c is not None and not getattr(np, \"_array_copy_patched_relaxed\", False):\n",
        "    def _np_array_relaxed(obj, *args, **kwargs):\n",
        "        if \"copy\" in kwargs and kwargs[\"copy\"] is False:\n",
        "            kwargs = dict(kwargs); kwargs.pop(\"copy\", None)\n",
        "            return _np_array_c(obj, *args, **kwargs)\n",
        "        try:\n",
        "            return _np_array_c(obj, *args, **kwargs)\n",
        "        except ValueError:\n",
        "            if \"copy\" in kwargs:\n",
        "                kwargs = dict(kwargs); kwargs.pop(\"copy\", None)\n",
        "                return _np_array_c(obj, *args, **kwargs)\n",
        "            raise\n",
        "        except TypeError:\n",
        "            kwargs = dict(kwargs); kwargs.pop(\"copy\", None)\n",
        "            return _np_array_c(obj, *args, **kwargs)\n",
        "\n",
        "    np.array = _np_array_relaxed\n",
        "    np._array_copy_patched_relaxed = True\n",
        "# ───────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.utils.data import DataLoader\n",
        "import evaluate\n",
        "\n",
        "from transformers import (\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizerFast,\n",
        "    DataCollatorWithPadding,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from collections import defaultdict\n",
        "from contextlib import contextmanager\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# ─── 0) Make JVP work with SDPA by forcing math kernels (CUDA only) ─────────\n",
        "@contextmanager\n",
        "def force_math_sdp():\n",
        "    if torch.cuda.is_available():\n",
        "        with torch.backends.cuda.sdp_kernel(\n",
        "            enable_flash=False, enable_math=True, enable_mem_efficient=False\n",
        "        ):\n",
        "            yield\n",
        "    else:\n",
        "        yield\n",
        "\n",
        "# ─── 1) JVP hooks: capture per-layer inputs so we can re-run locally ───────\n",
        "def register_jvp_hooks(model):\n",
        "    \"\"\"\n",
        "    Store each RobertaLayer's input hidden_states (X) and non-input args\n",
        "    (attention_mask, etc.) so we can locally call the layer f and compute JVPs.\n",
        "    \"\"\"\n",
        "    layers = model.roberta.encoder.layer\n",
        "    activations = {i: {\"X\": None, \"args\": None} for i in range(len(layers))}\n",
        "    hooks = []\n",
        "\n",
        "    for i, layer in enumerate(layers):\n",
        "        def pre_hook(module, inputs, idx=i):\n",
        "            x = inputs[0]\n",
        "            extra = tuple(inputs[1:]) if len(inputs) > 1 else tuple()\n",
        "            activations[idx][\"X\"] = x.detach()                 # fresh capture each batch\n",
        "            activations[idx][\"args\"] = extra\n",
        "\n",
        "        hooks.append(layer.register_forward_pre_hook(pre_hook))\n",
        "\n",
        "    return hooks, activations\n",
        "\n",
        "def remove_hooks(hooks):\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "# ─── 2) Helpers for ED over vector samples ─────────────────────────────────\n",
        "@torch.no_grad()\n",
        "def _to_samples(x):\n",
        "    \"\"\"\n",
        "    Flatten (B, S, H) → (B*S, H) and cast to float32 for stable distance calcs.\n",
        "    If x is (N, H) already, returns it unchanged (float32).\n",
        "    \"\"\"\n",
        "    x = x.to(torch.float32)\n",
        "    if x.dim() == 3:   # (B, S, H)\n",
        "        b, s, h = x.shape\n",
        "        return x.reshape(b * s, h)\n",
        "    return x  # assume (N, H)\n",
        "\n",
        "@torch.no_grad()\n",
        "def _energy_distance(a, b, max_samples=2048):\n",
        "    \"\"\"\n",
        "    Biased energy distance:\n",
        "        ED = 2 E||A-B|| - E||A-A'|| - E||B-B'||\n",
        "    a, b: (Na, D) and (Nb, D) float32 tensors.\n",
        "    \"\"\"\n",
        "    if a.numel() == 0 or b.numel() == 0:\n",
        "        return 0.0\n",
        "\n",
        "    Na, Nb = a.shape[0], b.shape[0]\n",
        "    if Na > max_samples:\n",
        "        a = a[torch.randperm(Na, device=a.device)[:max_samples]]\n",
        "        Na = a.shape[0]\n",
        "    if Nb > max_samples:\n",
        "        b = b[torch.randperm(Nb, device=b.device)[:max_samples]]\n",
        "        Nb = b.shape[0]\n",
        "\n",
        "    dab = torch.cdist(a, b, p=2)                       # (Na, Nb)\n",
        "    daa = torch.cdist(a, a, p=2) if Na > 1 else torch.zeros((), device=a.device)\n",
        "    dbb = torch.cdist(b, b, p=2) if Nb > 1 else torch.zeros((), device=b.device)\n",
        "\n",
        "    term_ab = dab.mean()\n",
        "    term_aa = daa.mean() if Na > 1 else torch.tensor(0.0, device=a.device)\n",
        "    term_bb = dbb.mean() if Nb > 1 else torch.tensor(0.0, device=b.device)\n",
        "\n",
        "    ed = 2.0 * term_ab - term_aa - term_bb\n",
        "    ed = torch.nan_to_num(ed, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "    return float(ed.item())\n",
        "\n",
        "# ─── 3) Core scorer: ED between Jv and v (i.e., || distribution gap ||) ────\n",
        "def compute_batch_jed_autograd(\n",
        "    model,\n",
        "    activations,\n",
        "    k_probes: int = 2,\n",
        "    rademacher: bool = True,\n",
        "    max_tokens: int | None = 2048,\n",
        "    max_samples: int = 2048,\n",
        "):\n",
        "    \"\"\"\n",
        "    Jacobian→Energy Distance per layer:\n",
        "      For layer ℓ with input X, draw k probe vectors v ~ {±1} (or N(0,I)).\n",
        "      Compute Jv via autograd JVP, gather samples A = Jv and B = v over tokens\n",
        "      (and probes), and return ED(A, B). Small ED ⇒ linearization close to I.\n",
        "    Returns: {layer_idx: ed_scalar}\n",
        "    \"\"\"\n",
        "    from torch.autograd.functional import jvp\n",
        "\n",
        "    layers = model.roberta.encoder.layer\n",
        "    scores = {}\n",
        "\n",
        "    for idx, buf in activations.items():\n",
        "        X, args = buf[\"X\"], buf[\"args\"]\n",
        "        if X is None:\n",
        "            continue\n",
        "\n",
        "        # Optional token subsampling to keep JVP compute bounded\n",
        "        if X.dim() == 3 and max_tokens is not None:\n",
        "            b, s, h = X.shape\n",
        "            if b * s > max_tokens:\n",
        "                flat = X.reshape(b * s, h)\n",
        "                sel = torch.randperm(flat.size(0), device=flat.device)[:max_tokens]\n",
        "                X = flat[sel].unsqueeze(0)  # fake batch of 1 with max_tokens seq len\n",
        "                # best-effort attention mask recreation if present\n",
        "                if len(args) > 0 and isinstance(args[0], torch.Tensor):\n",
        "                    attn = torch.ones((1, X.size(1)), dtype=args[0].dtype, device=X.device)\n",
        "                    args = (attn,) + args[1:]\n",
        "\n",
        "        X = X.detach().requires_grad_(True)\n",
        "        layer = layers[idx]\n",
        "        was_training = layer.training\n",
        "        layer.eval()  # disable dropout for stable JVP\n",
        "\n",
        "        try:\n",
        "            def f(inp):\n",
        "                out = layer(inp, *args)\n",
        "                return out[0] if isinstance(out, tuple) else out\n",
        "\n",
        "            # Collect samples across probes\n",
        "            A_list = []  # Jv\n",
        "            B_list = []  # v\n",
        "\n",
        "            with force_math_sdp():\n",
        "                for _ in range(k_probes):\n",
        "                    if rademacher:\n",
        "                        v = torch.empty_like(X).bernoulli_(0.5).mul_(2).sub_(1)  # ±1\n",
        "                    else:\n",
        "                        v = torch.randn_like(X)\n",
        "\n",
        "                    _, Jv = jvp(f, (X,), (v,), create_graph=False, strict=True)\n",
        "                    # Flatten to (N, H) sample sets\n",
        "                    A_list.append(_to_samples(Jv))\n",
        "                    B_list.append(_to_samples(v))\n",
        "\n",
        "            A = torch.cat(A_list, dim=0) if len(A_list) else torch.empty(0, X.size(-1), device=X.device)\n",
        "            B = torch.cat(B_list, dim=0) if len(B_list) else torch.empty(0, X.size(-1), device=X.device)\n",
        "            scores[idx] = _energy_distance(A, B, max_samples=max_samples)\n",
        "\n",
        "        finally:\n",
        "            layer.train(was_training)\n",
        "\n",
        "        # Clear to avoid stale tensors\n",
        "        buf[\"X\"] = None\n",
        "        buf[\"args\"] = None\n",
        "\n",
        "    return scores\n",
        "\n",
        "# ─── 4) Pruning: remove layers with smallest ED(Jv, v) ─────────────────────\n",
        "class SkipFF(nn.Module):\n",
        "    def forward(self, hidden_states, input_tensor=None):\n",
        "        return input_tensor  # residual passthrough\n",
        "\n",
        "def prune_jed_layers(model, jed_scores, num_prune=4):\n",
        "    \"\"\"\n",
        "    Sort ascending by ED(Jv, v) (closest-to-identity layers first) and prune them.\n",
        "    We remove the FFN by replacing intermediate.dense with Identity and set\n",
        "    the output block to a residual passthrough.\n",
        "    \"\"\"\n",
        "    sorted_layers = sorted(jed_scores.items(), key=lambda x: x[1])\n",
        "    prune_idxs = [idx for idx, _ in sorted_layers[:num_prune]]\n",
        "    for idx in prune_idxs:\n",
        "        layer = model.roberta.encoder.layer[idx]\n",
        "        layer.intermediate.dense = nn.Identity()\n",
        "        layer.output = SkipFF()\n",
        "    return prune_idxs\n",
        "\n",
        "# ─── 5) LoRA Modules (unchanged) ───────────────────────────────────────────\n",
        "class LoRA(nn.Module):\n",
        "    def __init__(self, W0, r=2, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"W0\", W0.clone().detach())\n",
        "        L, M = W0.shape\n",
        "        self.B = nn.Parameter(torch.randn(L, r) * 0.01)\n",
        "        self.A = nn.Parameter(torch.zeros(r, M))\n",
        "        self.scaling = alpha / r\n",
        "    def forward(self):\n",
        "        return self.W0 + self.scaling * (self.B @ self.A)\n",
        "\n",
        "def apply_lora_to_all_layers(model, r=2, alpha=1.0):\n",
        "    loras = {}\n",
        "    for idx, layer in enumerate(model.roberta.encoder.layer):\n",
        "        if not hasattr(layer.output, \"dense\"):\n",
        "            continue\n",
        "        W0 = layer.output.dense.weight.data\n",
        "        lora = LoRA(W0, r, alpha).to(W0.device)\n",
        "        def fwd(x, layer=layer, lora=lora):\n",
        "            return F.linear(x, lora(), layer.output.dense.bias)\n",
        "        layer.output.dense.forward = fwd\n",
        "        loras[idx] = lora\n",
        "    return loras\n",
        "\n",
        "# ─── 6) STS-B Evaluation (Pearson/Spearman) ────────────────────────────────\n",
        "def evaluate_stsb(model, dataloader, device):\n",
        "    model.eval()\n",
        "    metric = evaluate.load(\"glue\", \"stsb\")\n",
        "    preds, refs = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            out = model(\n",
        "                input_ids=batch[\"input_ids\"].to(device),\n",
        "                attention_mask=batch[\"attention_mask\"].to(device),\n",
        "            )\n",
        "            p = out.logits.squeeze(-1).cpu().tolist()\n",
        "            preds.extend(p if isinstance(p, list) else [p])\n",
        "            r = batch[\"labels\"].cpu().tolist()\n",
        "            refs.extend([float(x[0]) if isinstance(x, (list, tuple, np.ndarray)) else float(x) for x in r])\n",
        "    return metric.compute(predictions=preds, references=refs)\n",
        "\n",
        "# ─── 7) Training Stages (use JVP-based ED for scoring) ─────────────────────\n",
        "def full_finetuning(train_loader, dev_loader, device):\n",
        "    print(\"=== Stage 1: Full Finetuning & JVP-ED Estimation ===\")\n",
        "    model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=1).to(device)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "    sched = get_linear_schedule_with_warmup(opt, 0, len(train_loader) * 6)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    hooks, activations = register_jvp_hooks(model)\n",
        "    last_jed = None\n",
        "\n",
        "    for epoch in range(6):\n",
        "        jed_sums, jed_counts = defaultdict(float), defaultdict(int)\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                out = model(\n",
        "                    input_ids=batch[\"input_ids\"].to(device),\n",
        "                    attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                    labels=batch[\"labels\"].to(device),\n",
        "                )\n",
        "\n",
        "            # JVP-based Energy Distance per layer (before backward)\n",
        "            batch_jed = compute_batch_jed_autograd(\n",
        "                model, activations,\n",
        "                k_probes=2, rademacher=True,\n",
        "                max_tokens=2048, max_samples=2048\n",
        "            )\n",
        "            for idx, v in batch_jed.items():\n",
        "                jed_sums[idx]   += v\n",
        "                jed_counts[idx] += 1\n",
        "\n",
        "            scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "\n",
        "        epoch_jed = {idx: jed_sums[idx] / max(1, jed_counts[idx]) for idx in jed_sums}\n",
        "        print(f\"[Epoch {epoch+1}] JVP-ED:\", {k: round(v, 6) for k, v in epoch_jed.items()})\n",
        "        last_jed = epoch_jed\n",
        "\n",
        "    metrics = evaluate_stsb(model, dev_loader, device)\n",
        "    print(f\"STS-B Pearson: {metrics['pearson']:.4f}, Spearman: {metrics['spearmanr']:.4f}\")\n",
        "    remove_hooks(hooks)\n",
        "    return model, last_jed\n",
        "\n",
        "def prune_and_finetuning(model, train_loader, dev_loader, device, jed_scores):\n",
        "    print(\"=== Stage 2: Prune (Lowest JVP-ED) & Finetuning ===\")\n",
        "    prune_idxs = prune_jed_layers(model, jed_scores, num_prune=num)\n",
        "    print(\"Pruned layers (lowest JVP-ED):\", prune_idxs)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "    sched = get_linear_schedule_with_warmup(opt, 0, len(train_loader) * 3)\n",
        "    for epoch in range(5):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            out = model(\n",
        "                input_ids=batch[\"input_ids\"].to(device),\n",
        "                attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                labels=batch[\"labels\"].to(device),\n",
        "            )\n",
        "            out.loss.backward()\n",
        "            opt.step()\n",
        "            sched.step()\n",
        "\n",
        "        metrics = evaluate_stsb(model, dev_loader, device)\n",
        "        print(f\"[Prune Epoch {epoch+1}] Pearson: {metrics['pearson']:.4f}\")\n",
        "    return model\n",
        "\n",
        "def lora_only_finetuning(model, train_loader, dev_loader, device):\n",
        "    print(\"=== Stage 3: LoRA Finetuning ===\")\n",
        "    torch.cuda.empty_cache()\n",
        "    loras = apply_lora_to_all_layers(model)\n",
        "    for p in model.roberta.parameters():   p.requires_grad = False\n",
        "    for p in model.classifier.parameters(): p.requires_grad = True\n",
        "    for l in loras.values():\n",
        "        l.A.requires_grad = True\n",
        "        l.B.requires_grad = True\n",
        "\n",
        "    opt = torch.optim.AdamW(\n",
        "        list(model.classifier.parameters()) +\n",
        "        [p for l in loras.values() for p in (l.A, l.B)],\n",
        "        lr=2e-5\n",
        "    )\n",
        "    sched = get_linear_schedule_with_warmup(opt, 0, len(train_loader) * 6)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(6):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                out = model(\n",
        "                    input_ids=batch[\"input_ids\"].to(device),\n",
        "                    attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                    labels=batch[\"labels\"].to(device),\n",
        "                )\n",
        "            scaler.scale(out.loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            sched.step()\n",
        "\n",
        "        metrics = evaluate_stsb(model, dev_loader, device)\n",
        "        print(f\"[LoRA Epoch {epoch+1}] Pearson: {metrics['pearson']:.4f}\")\n",
        "\n",
        "# ─── 8) Main Entrypoint ────────────────────────────────────────────────────\n",
        "def main():\n",
        "    seed = 42\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "    train_ds = load_dataset(\"glue\", \"stsb\", split=\"train\").shuffle(seed)\n",
        "    dev_ds   = load_dataset(\"glue\", \"stsb\", split=\"validation\")\n",
        "\n",
        "    def preprocess(ex):\n",
        "        return tokenizer(\n",
        "            ex[\"sentence1\"], ex[\"sentence2\"],\n",
        "            truncation=True, padding=\"max_length\", max_length=128\n",
        "        )\n",
        "\n",
        "    train_ds = train_ds.map(preprocess, batched=True)\n",
        "    dev_ds   = dev_ds.map(preprocess, batched=True)\n",
        "\n",
        "    # Cast labels to flat float32\n",
        "    train_ds = train_ds.map(lambda x: {\"labels\": float(x[\"label\"])}, batched=False)\n",
        "    dev_ds   = dev_ds.map(lambda x: {\"labels\": float(x[\"label\"])}, batched=False)\n",
        "\n",
        "    # Keep only model columns\n",
        "    train_ds = train_ds.remove_columns([\"sentence1\", \"sentence2\", \"label\", \"idx\"])\n",
        "    dev_ds   = dev_ds.remove_columns([\"sentence1\", \"sentence2\", \"label\", \"idx\"])\n",
        "\n",
        "    train_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "    dev_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "    collator     = DataCollatorWithPadding(tokenizer, padding=\"max_length\", max_length=128)\n",
        "    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True,  collate_fn=collator)\n",
        "    dev_loader   = DataLoader(dev_ds,   batch_size=16, shuffle=False, collate_fn=collator)\n",
        "\n",
        "    model, jed_scores = full_finetuning(train_loader, dev_loader, device)\n",
        "    model = prune_and_finetuning(model, train_loader, dev_loader, device, jed_scores)\n",
        "    lora_only_finetuning(model, train_loader, dev_loader, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V0vtHx7W6LDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cOdjD8R36Mxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D_8JiD_gSKXt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}